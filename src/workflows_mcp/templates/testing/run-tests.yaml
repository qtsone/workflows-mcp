name: run-tests
description: Language-agnostic test execution wrapper that delegates to language-specific test runners
version: "1.0"
author: Workflows MCP Team
tags: [testing, tdd, quality, language-agnostic, wrapper]

inputs:
  language:
    type: str
    description: Programming language for the project (python, javascript, go, rust, etc.)
    required: true

  test_path:
    type: str
    description: Path to test directory or specific test file
    default: "tests/"
    required: false

  source_path:
    type: str
    description: Path to source code for coverage measurement
    default: "src/"
    required: false

  coverage_threshold:
    type: int
    description: Minimum coverage percentage required (0-100)
    default: 80
    required: false

  pytest_args:
    type: str
    description: Additional test framework arguments (language-specific)
    default: "-v"
    required: false

  working_dir:
    type: str
    description: Working directory for test execution
    default: "."
    required: false

  venv_path:
    type: str
    description: Virtual environment path (language-specific, e.g., Python venv)
    default: ""
    required: false

blocks:
  # Delegate to language-specific test runner
  # If workflow doesn't exist, Workflow will fail with clear error listing available workflows
  - id: run_tests
    type: Workflow
    inputs:
      workflow: "{{inputs.language}}-run-tests"
      inputs:
        test_path: "{{inputs.test_path}}"
        source_path: "{{inputs.source_path}}"
        coverage_threshold: {{inputs.coverage_threshold}}
        pytest_args: "{{inputs.pytest_args}}"
        working_dir: "{{inputs.working_dir}}"
        venv_path: "{{inputs.venv_path}}"

# Pass through outputs from language-specific test runner
outputs:
  # Test execution status
  exit_code: "{{blocks.run_tests.outputs.exit_code}}"
  success: "{{blocks.run_tests.succeeded}}"

  # Test counts
  tests_passed: "{{blocks.run_tests.outputs.tests_passed}}"
  tests_failed: "{{blocks.run_tests.outputs.tests_failed}}"
  tests_skipped: "{{blocks.run_tests.outputs.tests_skipped}}"

  # Coverage information
  coverage_percent: "{{blocks.run_tests.outputs.coverage_percent}}"
  coverage_threshold_met: "{{blocks.run_tests.outputs.coverage_threshold_met}}"

  # Full output for debugging
  stdout: "{{blocks.run_tests.outputs.stdout}}"
  stderr: "{{blocks.run_tests.outputs.stderr}}"

  # Summary and metadata
  summary: "{{blocks.run_tests.outputs.summary}}"
  execution_time_ms: "{{blocks.run_tests.metadata.execution_time_ms}}"
  command_executed: "{{blocks.run_tests.outputs.command_executed}}"

  # Wrapper metadata
  language_used: "{{inputs.language}}"
  workflow_delegated_to: "{{inputs.language}}-run-tests"
