name: run-tests
description: Language-agnostic test execution wrapper that delegates to language-specific test runners
version: "1.0"
author: Workflows MCP Team
tags: [testing, tdd, quality, language-agnostic, wrapper]
inputs:
  language:
    type: str
    description: Programming language for the project (python, javascript, go, rust, etc.)
    required: true
  test_path:
    type: str
    description: Path to test directory or specific test file
    default: "tests/"
    required: false
  source_path:
    type: str
    description: Path to source code for coverage measurement
    default: "src/"
    required: false
  coverage_threshold:
    type: num
    description: Minimum coverage percentage required (0-100)
    default: 80
    required: false
  pytest_args:
    type: str
    description: Additional test framework arguments (language-specific)
    default: "-v"
    required: false
  working_dir:
    type: str
    description: Working directory for test execution
    default: "."
    required: false
  venv_path:
    type: str
    description: Virtual environment path (language-specific, e.g., Python venv)
    default: ""
    required: false
blocks:
  # Delegate to language-specific test runner
  # If workflow doesn't exist, Workflow will fail with clear error listing available workflows
  - id: run_tests
    type: Workflow
    inputs:
      workflow: "{{inputs.language}}-run-tests"
      inputs:
        test_path: "{{inputs.test_path}}"
        source_path: "{{inputs.source_path}}"
        coverage_threshold: "{{inputs.coverage_threshold}}"
        pytest_args: "{{inputs.pytest_args}}"
        working_dir: "{{inputs.working_dir}}"
        venv_path: "{{inputs.venv_path}}"
# Pass through outputs from language-specific test runner
outputs:
  # Test execution status
  exit_code:
    value: "{{blocks.run_tests.outputs.exit_code}}"
  success:
    value: "{{blocks.run_tests.succeeded}}"
  # Test counts
  tests_passed:
    value: "{{blocks.run_tests.outputs.tests_passed}}"
  tests_failed:
    value: "{{blocks.run_tests.outputs.tests_failed}}"
  tests_skipped:
    value: "{{blocks.run_tests.outputs.tests_skipped}}"
  # Coverage information
  coverage_percent:
    value: "{{blocks.run_tests.outputs.coverage_percent}}"
  coverage_threshold_met:
    value: "{{blocks.run_tests.outputs.coverage_threshold_met}}"
  # Full output for debugging
  stdout:
    value: "{{blocks.run_tests.outputs.stdout}}"
  stderr:
    value: "{{blocks.run_tests.outputs.stderr}}"
  # Summary and metadata
  summary:
    value: "{{blocks.run_tests.outputs.summary}}"
  execution_time_ms:
    value: "{{blocks.run_tests.metadata.execution_time_ms}}"
  command_executed:
    value: "{{blocks.run_tests.outputs.command_executed}}"
  # Wrapper metadata
  language_used:
    value: "{{inputs.language}}"
  workflow_delegated_to:
    value: "{{inputs.language}}-run-tests"
