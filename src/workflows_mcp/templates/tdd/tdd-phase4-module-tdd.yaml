# TDD Phase 4: Module TDD Implementation (Red-Green-Refactor)
#
# Implements the core Test-Driven Development cycle for a single module:
# 1. RED: Write tests first - verify they fail correctly
# 2. GREEN: Implement minimum code to make tests pass
# 3. REFACTOR: Improve code quality while keeping tests passing
#
# This workflow enforces TDD discipline through interactive checkpoints:
# - Tests MUST be written before implementation
# - Tests MUST fail initially (no false positives)
# - Tests MUST pass after implementation
# - Tests MUST still pass after refactoring
# - State tracking ensures progress persistence
#
# Philosophy: Test-first development with incremental verification
#
# Usage:
#   execute_workflow("tdd-phase4-module-tdd", {
#     "module_name": "user_service",
#     "project_path": "/path/to/project"
#   })
#
# The workflow will pause multiple times for LLM interaction:
# 1. Confirm start (verify module selection)
# 2. Request test code (LLM writes tests)
# 3. Confirm RED phase (tests fail as expected)
# 4. Request implementation (LLM writes minimum code)
# 5. Confirm GREEN phase (tests now pass)
# 6. Request refactoring (LLM improves code)
# 7. Confirm refactor success (tests still pass)
# 8. Ask to continue (proceed to next module?)

name: tdd-phase4-module-tdd
description: Core TDD red-green-refactor cycle for single module implementation with test-first discipline
version: "1.0"
author: MCP Workflows Team
tags: [tdd, implementation, red-green-refactor, test-first, python, interactive]

inputs:
  project_path:
    type: str
    description: Path to project root directory
    default: "."
    required: false

  state_file:
    type: str
    description: Path to TDD state file (JSON)
    default: ".tdd-state.json"
    required: false

  module_name:
    type: str
    description: Name of module to implement (required)
    required: true

  test_path:
    type: str
    description: Path to test directory (relative to project_path)
    default: "tests"
    required: false

  source_path:
    type: str
    description: Path to source directory (relative to project_path)
    default: "src"
    required: false

  coverage_threshold:
    type: int
    description: Minimum code coverage percentage for module
    default: 80
    required: false

  language:
    type: str
    description: Programming language for the project (python, javascript, go, etc.)
    default: "python"
    required: false

blocks:
  - id: read_state
    type: ReadJSONState
    inputs:
      path: "${inputs.project_path}/${inputs.state_file}"

  - id: confirm_start
    type: Prompt
    inputs:
      prompt: |
        Start TDD implementation for module: ${inputs.module_name}

        Current TDD state:
        - Phase: ${blocks.read_state.state.current_phase}
        - Total modules: ${blocks.read_state.state.modules}
        - Completed: ${blocks.read_state.state.completed_modules}

        Ready to begin RED-GREEN-REFACTOR cycle?
        
        Respond with 'yes' or 'no'
      operation: "start_tdd_module_${inputs.module_name}"
      details:
        module_name: "${inputs.module_name}"
        project_path: "${inputs.project_path}"
        test_path: "${inputs.test_path}"
        source_path: "${inputs.source_path}"
        state_loaded: "${blocks.read_state.outputs.found}"
    depends_on:
      - read_state

  - id: request_test_code
    type: Prompt
    inputs:
      prompt: |
        === RED PHASE: Write Tests First ===

        Module: ${inputs.module_name}
        Test file: ${inputs.project_path}/${inputs.test_path}/test_${inputs.module_name}.py

        Write ALL unit tests for this module following these principles:
        1. Use AAA pattern (Arrange-Act-Assert)
        2. Test edge cases and error conditions
        3. Tests should FAIL initially (no implementation exists yet)
        4. Use pytest fixtures and parametrize where appropriate
        5. Aim for comprehensive coverage (${inputs.coverage_threshold}%+ target)

        Provide the COMPLETE test file content below:
        
        Respond with 'yes' or 'no'
    depends_on:
      - confirm_start
    condition: "${blocks.confirm_start.outputs.response == 'yes'}"

  - id: write_test_file
    type: CreateFile
    inputs:
      path: "${inputs.project_path}/${inputs.test_path}/test_${inputs.module_name}.py"
      content: "${blocks.request_test_code.response}"
      encoding: "utf-8"
    depends_on:
      - request_test_code

  - id: run_tests_verify_fail
    type: Workflow
    inputs:
      workflow: "run-tests"
      inputs:
        language: "${inputs.language}"
        test_path: "${inputs.test_path}/test_${inputs.module_name}.py"
        source_path: "${inputs.source_path}"
        coverage_threshold: ${inputs.coverage_threshold}
        pytest_args: "-v --tb=short"
        working_dir: "${inputs.project_path}"
    depends_on:
      - write_test_file

  - id: validate_red_phase
    type: Shell
    inputs:
      command: |
        EXIT_CODE="${blocks.run_tests_verify_fail.outputs.exit_code}"
        if [ "$EXIT_CODE" = "0" ]; then
          echo "❌ ERROR: Tests passed without implementation!"
          echo "This violates TDD principles - tests must fail initially."
          echo "Recommendation: Check that tests actually exercise the module."
          exit 1
        else
          echo "✅ RED PHASE VALID: Tests failed as expected (exit code: $EXIT_CODE)"
          echo "Tests are properly written to fail without implementation."
          exit 0
        fi
      timeout: 10
    depends_on:
      - run_tests_verify_fail

  - id: confirm_red_phase
    type: Prompt
    inputs:
      prompt: |
        === RED PHASE COMPLETE ===

        Tests for ${inputs.module_name} are failing correctly:
        - Exit code: ${blocks.run_tests_verify_fail.outputs.exit_code}
        - Test output available for review
        - This confirms tests will catch missing implementation

        Proceed to GREEN phase (implement code)?
        
        Respond with 'yes' or 'no'
        
        Respond with 'yes' or 'no'
      operation: "confirm_red_phase_${inputs.module_name}"
      details:
        exit_code: "${blocks.run_tests_verify_fail.outputs.exit_code}"
        test_file: "${inputs.project_path}/${inputs.test_path}/test_${inputs.module_name}.py"
    depends_on:
      - validate_red_phase

  - id: request_implementation
    type: Prompt
    inputs:
      prompt: |
        === GREEN PHASE: Implement Minimum Code ===

        Module: ${inputs.module_name}
        Source file: ${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py

        Implement the MINIMUM code needed to make ALL tests pass:
        1. Focus on making tests pass, not on perfection
        2. Use simplest solution that works
        3. Don't optimize prematurely (that's for REFACTOR phase)
        4. Ensure all test cases are handled

        Test requirements from RED phase:
        ${blocks.run_tests_verify_fail.outputs.stderr}

        Provide the COMPLETE implementation file content below:
        
        Respond with 'yes' or 'no'
    depends_on:
      - confirm_red_phase
    condition: "${blocks.confirm_red_phase.outputs.response == 'yes'}"

  - id: write_implementation
    type: CreateFile
    inputs:
      path: "${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
      content: "${blocks.request_implementation.response}"
      encoding: "utf-8"
    depends_on:
      - request_implementation

  - id: run_tests_verify_pass
    type: Workflow
    inputs:
      workflow: "run-tests"
      inputs:
        language: "${inputs.language}"
        test_path: "${inputs.test_path}/test_${inputs.module_name}.py"
        source_path: "${inputs.source_path}/${inputs.module_name}.py"
        coverage_threshold: ${inputs.coverage_threshold}
        pytest_args: "-v --cov-report=term-missing"
        working_dir: "${inputs.project_path}"
    depends_on:
      - write_implementation

  - id: validate_green_phase
    type: Shell
    inputs:
      command: |
        EXIT_CODE="${blocks.run_tests_verify_pass.outputs.exit_code}"
        if [ "$EXIT_CODE" != "0" ]; then
          echo "❌ ERROR: Tests still failing after implementation!"
          echo "Exit code: $EXIT_CODE"
          echo "Recommendation: Review implementation and test output."
          exit 1
        else
          echo "✅ GREEN PHASE VALID: All tests passing"
          exit 0
        fi
      timeout: 10
    depends_on:
      - run_tests_verify_pass

  - id: confirm_green_phase
    type: Prompt
    inputs:
      prompt: |
        === GREEN PHASE COMPLETE ===

        Tests for ${inputs.module_name} are now passing:
        - Exit code: ${blocks.run_tests_verify_pass.outputs.exit_code}
        - Tests passed: ${blocks.run_tests_verify_pass.outputs.tests_passed}
        - Tests failed: ${blocks.run_tests_verify_pass.outputs.tests_failed}
        - Coverage: ${blocks.run_tests_verify_pass.outputs.coverage_percent}%
        - Implementation complete and verified

        Proceed to REFACTOR phase (improve code quality)?
        
        Respond with 'yes' or 'no'
        
        Respond with 'yes' or 'no'
      operation: "confirm_green_phase_${inputs.module_name}"
      details:
        exit_code: "${blocks.run_tests_verify_pass.outputs.exit_code}"
        tests_passed: "${blocks.run_tests_verify_pass.outputs.tests_passed}"
        tests_failed: "${blocks.run_tests_verify_pass.outputs.tests_failed}"
        coverage_percent: "${blocks.run_tests_verify_pass.outputs.coverage_percent}"
        implementation_file: "${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
    depends_on:
      - validate_green_phase

  - id: request_refactor
    type: Prompt
    inputs:
      prompt: |
        === REFACTOR PHASE: Improve Code Quality ===

        Module: ${inputs.module_name}
        Source file: ${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py

        Review the current implementation at: @${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py

        Refactor this code to improve quality while keeping tests passing:
        1. Improve naming and clarity
        2. Extract duplicated code
        3. Optimize algorithms if appropriate
        4. Add docstrings and type hints
        5. Follow Python best practices (PEP 8, PEP 484)
        6. IMPORTANT: Do NOT change behavior - tests must still pass

        Provide the REFACTORED implementation file content below:
    depends_on:
      - confirm_green_phase
    condition: "${blocks.confirm_green_phase.outputs.response == 'yes'}"

  - id: write_refactored
    type: CreateFile
    inputs:
      path: "${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
      content: "${blocks.request_refactor.response}"
      encoding: "utf-8"
    depends_on:
      - request_refactor

  - id: run_tests_after_refactor
    type: Workflow
    inputs:
      workflow: "run-tests"
      inputs:
        language: "${inputs.language}"
        test_path: "${inputs.test_path}/test_${inputs.module_name}.py"
        source_path: "${inputs.source_path}/${inputs.module_name}.py"
        coverage_threshold: ${inputs.coverage_threshold}
        pytest_args: "-v --cov-report=term-missing"
        working_dir: "${inputs.project_path}"
    depends_on:
      - write_refactored

  - id: validate_refactor_phase
    type: Shell
    inputs:
      command: |
        EXIT_CODE="${blocks.run_tests_after_refactor.outputs.exit_code}"
        if [ "$EXIT_CODE" != "0" ]; then
          echo "❌ ERROR: Tests failing after refactoring!"
          echo "Refactoring changed behavior - this violates TDD principles."
          echo "Recommendation: Revert to GREEN phase implementation or fix refactored code."
          exit 1
        else
          echo "✅ REFACTOR PHASE VALID: Tests still passing after refactoring"
          exit 0
        fi
      timeout: 10
    depends_on:
      - run_tests_after_refactor

  - id: confirm_refactor_success
    type: Prompt
    inputs:
      prompt: |
        === REFACTOR PHASE COMPLETE ===

        Module ${inputs.module_name} successfully refactored:
        - Exit code: ${blocks.run_tests_after_refactor.outputs.exit_code}
        - Tests passed: ${blocks.run_tests_after_refactor.outputs.tests_passed}
        - Tests failed: ${blocks.run_tests_after_refactor.outputs.tests_failed}
        - Coverage: ${blocks.run_tests_after_refactor.outputs.coverage_percent}%
        - Tests still passing after refactoring
        - Code quality improved

        Mark module as complete in TDD state?
        
        Respond with 'yes' or 'no'
        
        Respond with 'yes' or 'no'
      operation: "confirm_refactor_complete_${inputs.module_name}"
      details:
        exit_code: "${blocks.run_tests_after_refactor.outputs.exit_code}"
        tests_passed: "${blocks.run_tests_after_refactor.outputs.tests_passed}"
        tests_failed: "${blocks.run_tests_after_refactor.outputs.tests_failed}"
        coverage_percent: "${blocks.run_tests_after_refactor.outputs.coverage_percent}"
        refactored_file: "${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
    depends_on:
      - validate_refactor_phase

  - id: update_state
    type: Shell
    inputs:
      command: |
        # Get metrics directly from test execution outputs
        PASSED="${blocks.run_tests_after_refactor.outputs.tests_passed}"
        COVERAGE="${blocks.run_tests_after_refactor.outputs.coverage_percent}"

        # Create Python script to update state
        cat > /tmp/update_tdd_state.py << 'PYTHON_SCRIPT'
        import json
        import sys
        from pathlib import Path

        # Get arguments
        state_file = Path(sys.argv[1])
        module_name = sys.argv[2]
        tests_passed = int(sys.argv[3]) if sys.argv[3] else 0
        coverage = float(sys.argv[4]) if sys.argv[4] else 0.0

        # Read existing state
        if state_file.exists():
            state = json.loads(state_file.read_text())
        else:
            state = {}

        # Ensure required keys exist
        if 'completed_modules' not in state:
            state['completed_modules'] = []
        if 'modules_status' not in state:
            state['modules_status'] = {}

        # Add module to completed list if not already there
        if module_name not in state['completed_modules']:
            state['completed_modules'].append(module_name)

        # Update module status
        state['modules_status'][module_name] = {
            'tests_count': tests_passed,
            'coverage': coverage,
            'refactored': True,
            'phase': 'complete'
        }

        # Update current phase
        state['current_phase'] = 'phase4_implementation'

        # Write back
        state_file.parent.mkdir(parents=True, exist_ok=True)
        state_file.write_text(json.dumps(state, indent=2) + '\n')

        print(f'✅ Updated state: {module_name} marked complete')
        print(f'   Tests: {tests_passed}, Coverage: {coverage}%')
        PYTHON_SCRIPT

        # Run the Python script (use python3 for utility script execution)
        python3 /tmp/update_tdd_state.py \
          "${inputs.project_path}/${inputs.state_file}" \
          "${inputs.module_name}" \
          "$PASSED" \
          "$COVERAGE"
      timeout: 30
    depends_on:
      - confirm_refactor_success
    condition: "${blocks.confirm_refactor_success.outputs.response == 'yes'}"

  - id: ask_continue
    type: Prompt
    inputs:
      prompt: |
        Module '${inputs.module_name}' is complete and verified.

        TDD cycle summary:
        - ✅ RED: Tests written and failed correctly
        - ✅ GREEN: Implementation made tests pass
        - ✅ REFACTOR: Code improved, tests still passing

        What would you like to do next?
        
        Respond with 'yes' or 'no'
    depends_on:
      - update_state

  - id: completion_summary
    type: Shell
    inputs:
      command: |
        echo "╔════════════════════════════════════════════════════════════════╗"
        echo "║         TDD RED-GREEN-REFACTOR CYCLE COMPLETE                 ║"
        echo "╚════════════════════════════════════════════════════════════════╝"
        echo ""
        echo "Module: ${inputs.module_name}"
        echo ""
        echo "Phase Results:"
        echo "  🔴 RED:     Tests written and failed correctly"
        echo "  🟢 GREEN:   Implementation made tests pass"
        echo "  🔵 REFACTOR: Code improved while tests passing"
        echo ""
        echo "Final Metrics:"
        echo "  Tests passed: ${blocks.run_tests_after_refactor.outputs.tests_passed}"
        echo "  Tests failed: ${blocks.run_tests_after_refactor.outputs.tests_failed}"
        echo "  Coverage: ${blocks.run_tests_after_refactor.outputs.coverage_percent}%"
        echo ""
        echo "Files Updated:"
        echo "  - ${inputs.project_path}/${inputs.test_path}/test_${inputs.module_name}.py"
        echo "  - ${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
        echo "  - ${inputs.project_path}/${inputs.state_file}"
        echo ""
        echo "Next Action: ${blocks.ask_continue.response}"
      timeout: 10
    depends_on:
      - ask_continue

outputs:
  module_name: "${inputs.module_name}"
  tests_written: "${blocks.write_test_file.succeeded}"
  tests_passing: "${blocks.run_tests_after_refactor.succeeded}"
  tests_passed: "${blocks.run_tests_after_refactor.outputs.tests_passed}"
  tests_failed: "${blocks.run_tests_after_refactor.outputs.tests_failed}"
  coverage_percent: "${blocks.run_tests_after_refactor.outputs.coverage_percent}"
  refactored: "${blocks.confirm_refactor_success.outputs.response == 'yes'}"
  continue_choice: "${blocks.ask_continue.response}"
  continue_to_next_module: "${blocks.ask_continue.response}.lower().startswith('yes')"
  state_updated: "${blocks.update_state.succeeded}"

  # Phase completion flags
  red_phase_complete: "${blocks.validate_red_phase.succeeded}"
  green_phase_complete: "${blocks.validate_green_phase.succeeded}"
  refactor_phase_complete: "${blocks.validate_refactor_phase.succeeded}"

  # File paths
  test_file: "${inputs.project_path}/${inputs.test_path}/test_${inputs.module_name}.py"
  source_file: "${inputs.project_path}/${inputs.source_path}/${inputs.module_name}.py"
  state_file: "${inputs.project_path}/${inputs.state_file}"

  # Summary
  summary: "${blocks.completion_summary.outputs.stdout}"
