name: python-run-tests
description: Execute pytest with coverage, parse results, and return structured outputs for TDD workflows
version: "1.0"
author: Workflows MCP Team
tags: [python, testing, pytest, coverage, tdd, quality]
inputs:
  test_path:
    type: str
    description: Path to test directory or specific test file
    default: "tests/"
    required: false
  source_path:
    type: str
    description: Path to source code for coverage measurement
    default: "src/"
    required: false
  coverage_threshold:
    type: num
    description: Minimum coverage percentage required (0-100)
    default: 80
    required: false
  pytest_args:
    type: str
    description: Additional pytest arguments
    default: "-v"
    required: false
  working_dir:
    type: str
    description: Working directory for test execution
    default: "."
    required: false
  venv_path:
    type: str
    description: Virtual environment path (will use {{inputs.venv_path}}/bin/pytest if available)
    default: ""
    required: false
blocks:
  # Determine pytest executable (prefer uv for proper venv management)
  - id: build_pytest_command
    type: Shell
    inputs:
      command: |
        # Check if uv is available (modern Python tool management)
        if command -v uv &> /dev/null && [ -f "pyproject.toml" ]; then
          PYTEST="uv run pytest"
        elif [ -n "{{inputs.venv_path}}" ] && [ -f "{{inputs.venv_path}}/bin/pytest" ]; then
          PYTEST="{{inputs.venv_path}}/bin/pytest"
        else
          PYTEST="pytest"
        fi
        echo "$PYTEST {{inputs.test_path}} --cov={{inputs.source_path}} --cov-report=json --cov-report=term-missing {{inputs.pytest_args}}"
      timeout: 5
  # Execute pytest with coverage
  # Coverage is saved to coverage.json for parsing
  - id: run_pytest
    type: Shell
    inputs:
      command: "{{blocks.build_pytest_command.outputs.stdout}}"
      working_dir: "{{inputs.working_dir}}"
      timeout: 600
    depends_on:
      - build_pytest_command
  # Parse coverage percentage from coverage.json
  # Returns 0 if coverage.json doesn't exist or can't be parsed
  - id: extract_coverage
    type: Shell
    inputs:
      command: |
        cd "{{inputs.working_dir}}"
        if [ -f coverage.json ]; then
          # Use uv run python if available, fallback to python3
          if command -v uv &> /dev/null && [ -f "pyproject.toml" ]; then
            PYTHON="uv run python"
          else
            PYTHON="python3"
          fi
          $PYTHON -c "
        import json
        import sys
        try:
            data = json.load(open('coverage.json'))
            print(f\"{data['totals']['percent_covered']:.2f}\")
        except Exception as e:
            print('0.00', file=sys.stderr)
            print('0.00')
        "
        else
          echo "0.00"
        fi
      working_dir: "{{inputs.working_dir}}"
      timeout: 10
    depends_on:
      - run_pytest
  # Parse test counts from pytest output
  # Expected format: "X passed" or "X passed, Y failed, Z skipped"
  - id: parse_test_results
    type: Shell
    inputs:
      command: |
        OUTPUT="{{blocks.run_pytest.outputs.stdout}}"

        # Extract passed count
        PASSED=$(echo "$OUTPUT" | grep -oE '[0-9]+ passed' | grep -oE '[0-9]+' || echo "0")

        # Extract failed count
        FAILED=$(echo "$OUTPUT" | grep -oE '[0-9]+ failed' | grep -oE '[0-9]+' || echo "0")

        # Extract skipped count
        SKIPPED=$(echo "$OUTPUT" | grep -oE '[0-9]+ skipped' | grep -oE '[0-9]+' || echo "0")

        echo "PASSED=$PASSED"
        echo "FAILED=$FAILED"
        echo "SKIPPED=$SKIPPED"
      timeout: 10
    depends_on:
      - run_pytest
  # Extract individual test counts as separate values for workflow logic
  - id: extract_passed_count
    type: Shell
    inputs:
      command: "echo '{{blocks.parse_test_results.outputs.stdout}}' | grep '^PASSED=' | cut -d= -f2"
      timeout: 5
    depends_on:
      - parse_test_results
  - id: extract_failed_count
    type: Shell
    inputs:
      command: "echo '{{blocks.parse_test_results.outputs.stdout}}' | grep '^FAILED=' | cut -d= -f2"
      timeout: 5
    depends_on:
      - parse_test_results
  - id: extract_skipped_count
    type: Shell
    inputs:
      command: "echo '{{blocks.parse_test_results.outputs.stdout}}' | grep '^SKIPPED=' | cut -d= -f2"
      timeout: 5
    depends_on:
      - parse_test_results
  # Check if coverage threshold was met
  - id: check_coverage_threshold
    type: Shell
    inputs:
      command: |
        COVERAGE="{{blocks.extract_coverage.outputs.stdout}}"
        THRESHOLD="{{inputs.coverage_threshold}}"

        # Use awk for floating point comparison
        RESULT=$(awk -v cov="$COVERAGE" -v thresh="$THRESHOLD" 'BEGIN { print (cov >= thresh) ? "true" : "false" }')
        echo "$RESULT"
      timeout: 5
    depends_on:
      - extract_coverage
  # Generate comprehensive test summary
  - id: generate_summary
    type: Shell
    inputs:
      command: |
        printf "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
        printf "Test Execution Results\n"
        printf "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\n"
        printf "Tests Path:    {{inputs.test_path}}\n"
        printf "Source Path:   {{inputs.source_path}}\n"
        printf "\n"
        printf "Test Results:\n"
        printf "  ✓ Passed:    {{blocks.extract_passed_count.outputs.stdout}}\n"
        printf "  ✗ Failed:    {{blocks.extract_failed_count.outputs.stdout}}\n"
        printf "  ⊘ Skipped:   {{blocks.extract_skipped_count.outputs.stdout}}\n"
        printf "\n"
        printf "Coverage:\n"
        printf "  Measured:    {{blocks.extract_coverage.outputs.stdout}}%%\n"
        printf "  Threshold:   {{inputs.coverage_threshold}}%%\n"
        printf "  Status:      {{blocks.check_coverage_threshold.outputs.stdout}}\n"
        printf "\n"
        printf "Exit Code:     {{blocks.run_pytest.outputs.exit_code}}\n"
        printf "Execution:     {{blocks.run_pytest.metadata.execution_time_ms}}ms\n"
        printf "━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━"
    depends_on:
      - extract_passed_count
      - extract_failed_count
      - extract_skipped_count
      - check_coverage_threshold
outputs:
  # Test execution status
  exit_code:
    value: "{{blocks.run_pytest.outputs.exit_code}}"
  success:
    value: "{{blocks.run_pytest.outputs.exit_code}} == 0"
  # Test counts (as strings for now, can be converted to int in workflow logic)
  tests_passed:
    value: "{{blocks.extract_passed_count.outputs.stdout}}"
  tests_failed:
    value: "{{blocks.extract_failed_count.outputs.stdout}}"
  tests_skipped:
    value: "{{blocks.extract_skipped_count.outputs.stdout}}"
  # Coverage information
  coverage_percent:
    value: "{{blocks.extract_coverage.outputs.stdout}}"
  coverage_threshold_met:
    value: "{{blocks.check_coverage_threshold.outputs.stdout}}"
  # Full output for debugging
  stdout:
    value: "{{blocks.run_pytest.outputs.stdout}}"
  stderr:
    value: "{{blocks.run_pytest.outputs.stderr}}"
  # Summary and metadata
  summary:
    value: "{{blocks.generate_summary.outputs.stdout}}"
  execution_time_ms:
    value: "{{blocks.run_pytest.metadata.execution_time_ms}}"
  command_executed:
    value: "{{blocks.build_pytest_command.outputs.stdout}}"
