name: agent-pr-review
description: |
  AI Autonomous agent for in-depth PR/MR review using CORTEX cognitive architecture.

  This agent performs comprehensive code review by:
  1. Gathering context (cloning repo if URL provided, or using local repo)
  2. CORTEX cognitive cycle (CATEGORIZE → GATHER → REASON → ACT)
  3. Synthesis of findings with evidence-based reasoning
  4. Optional posting of review comments to GitHub/GitLab

  Architecture:
  - Uses cortex-cell for recursive cognitive processing
  - Memory-first retrieval with facts (semantic) and evidence (episodic) memory
  - "quality" category drives pipeline config: gather=focused, reason=evaluate, act=report
  - Parallel child cell spawning for complex sub-investigations

  The workflow normalizes input early - regardless of whether you provide a URL
  or local repo path, all downstream processing uses a consistent local path.

tags: ["agent", "review", "cortex", "cognitive"]

inputs:
  url:
    type: str
    description: |
      Full PR/MR URL for remote review.
      Mutually exclusive with repo_path. One must be provided.
      Examples:
        - https://github.com/owner/repo/pull/123
        - https://gitlab.com/group/project/-/merge_requests/456
    required: false

  repo_path:
    type: str
    description: |
      Path to local git repository for local review.
      Mutually exclusive with url. One must be provided.
    required: false

  base_branch:
    type: str
    description: |
      Branch to compare against (target branch).
      For remote PRs, this is typically extracted from PR metadata.
    required: false
    default: main

  head_branch:
    type: str
    description: |
      Branch being reviewed (source branch).
      Defaults to current branch if not specified.
    required: false

  focus:
    type: str
    description: |
      Area to prioritize in review.
      Options: security, performance, architecture, tests, general
    required: false

  threshold:
    type: str
    description: |
      Minimum severity to report.
      Values: critical, high, medium, low, info
    required: false
    default: low

  comment:
    type: bool
    description: |
      Whether to post review comments directly to remote PR/MR.
      Only works when url is provided and platform is supported.
    required: false
    default: false

  max_depth:
    type: num
    description: Maximum recursion depth for investigation (prevents infinite loops).
    required: false
    default: 3

blocks:
  # ==========================================================================
  # STATE MANAGEMENT: Initialize state for the PR review
  # ==========================================================================
  - id: state_management
    type: Workflow
    description: |
      Manage state for the PR review.
    inputs:
      workflow: agent-state-management
      inputs:
        task: "PR review"
        caller: "pr-review"
        data: "{{ inputs }}"
        ai: true
        audit: true

  # ==========================================================================
  # MEMORY INITIALIZATION: Clear memory for fresh review
  # ==========================================================================
  - id: init_memory
    type: Workflow
    description: |
      Initialize/clear memory for this PR review execution.
      Creates fresh memory file to avoid cross-review contamination.
    depends_on:
      - state_management
    condition: "{{ blocks.state_management.outputs.state | trim != '' }}"
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ blocks.state_management.outputs.state }}"
        op: memory
        memory_op: set
        memory_key: "_initialized"
        memory_value: |
          {
            "trace_id": "{{ blocks.state_management.outputs.trace_id }}",
            "started_at": "{{ now() }}",
            "version": 2
          }
        caller: "pr-review:main"

  # ==========================================================================
  # PHASE 1: Context Gathering
  # Normalizes input (URL vs local path) and outputs consistent repo_path
  # ==========================================================================
  - id: context_gathering
    type: Workflow
    description: |
      Gather and validate context for the PR review.
      Outputs normalized repo_path regardless of input source.
    depends_on:
      - state_management
      - block: init_memory
        required: false
    inputs:
      workflow: context-gathering
      inputs:
        url: "{{ inputs.url }}"
        repo_path: "{{ inputs.repo_path }}"
        base_branch: "{{ inputs.base_branch }}"
        head_branch: "{{ inputs.head_branch }}"
        workspace: "{{ tmp }}/pr-review-repo"
        state: "{{ blocks.state_management.outputs.state }}"
        parent_task_id: "{{ get(blocks.state_management.outputs.task, 'task_id', '') }}"

  # ==========================================================================
  # PHASE 2: CORTEX Cognitive Investigation
  # Uses cortex-cell for recursive cognitive processing with 4-phase cycle:
  # CATEGORIZE → GATHER → REASON → ACT
  # ==========================================================================
  - id: build_review_prompt
    type: Shell
    description: |
      Build a comprehensive PR review prompt for cortex-cell.
      Includes changed files, diff excerpt, and focus area.
    depends_on:
      - context_gathering
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        # Load context
        changed_files_json = os.environ.get('CHANGED_FILES', '{}')
        pr_metadata_json = os.environ.get('PR_METADATA', '{}')
        diff = os.environ.get('DIFF', '')
        focus = os.environ.get('FOCUS', 'general')
        threshold = os.environ.get('THRESHOLD', 'low')

        def safe_json_loads(val, default={}):
            """Handle potentially double-encoded JSON strings."""
            if not val:
                return default
            try:
                result = json.loads(val)
                # If result is still a string, it was double-encoded - decode again
                if isinstance(result, str):
                    result = json.loads(result)
                return result
            except (json.JSONDecodeError, TypeError):
                return default

        changed_files = safe_json_loads(changed_files_json, {})
        pr_metadata = safe_json_loads(pr_metadata_json, {})

        # Extract file list
        targets = []
        categories = changed_files.get('categories', [])
        for cat in categories:
            files = cat.get('files', [])
            targets.extend(files)

        # Deduplicate
        targets = list(dict.fromkeys(targets))

        # Build focus-specific instructions
        focus_instructions = {
            'security': 'Focus on security vulnerabilities: SQL injection, XSS, auth issues, secrets exposure, input validation.',
            'performance': 'Focus on performance issues: inefficient algorithms, N+1 queries, memory leaks, unnecessary operations.',
            'architecture': 'Focus on architectural concerns: coupling, cohesion, design patterns, modularity, maintainability.',
            'tests': 'Focus on test quality: coverage, edge cases, test isolation, assertions, mocking practices.',
            'general': 'Perform a comprehensive review covering code quality, best practices, potential bugs, and maintainability.'
        }.get(focus, 'Perform a comprehensive review covering code quality, best practices, potential bugs, and maintainability.')

        # Truncate diff if too long (keep first 6000 chars for context)
        diff_excerpt = diff[:6000] if len(diff) > 6000 else diff
        diff_truncated = len(diff) > 6000

        # Build the prompt
        prompt_parts = [
            "# PR Review Task",
            "",
            f"Review this pull request for **{focus}** issues.",
            "",
            focus_instructions,
            "",
            f"**Severity threshold:** Only report issues at **{threshold}** level or higher.",
            "",
        ]

        if pr_metadata:
            prompt_parts.extend([
                "## PR Metadata",
                f"- **Title:** {pr_metadata.get('title', 'N/A')}",
                f"- **Author:** {pr_metadata.get('author', 'N/A')}",
                f"- **Description:** {pr_metadata.get('description', 'N/A')[:500]}",
                "",
            ])

        prompt_parts.extend([
            "## Changed Files",
            f"Total: {len(targets)} files",
            "",
        ])

        for cat in categories:
            cat_name = cat.get('category', 'other')
            cat_files = cat.get('files', [])
            if cat_files:
                prompt_parts.append(f"**{cat_name}:** {', '.join(cat_files[:10])}" + ("..." if len(cat_files) > 10 else ""))

        prompt_parts.extend([
            "",
            "## Diff Excerpt",
            "```diff",
            diff_excerpt,
            "```",
        ])

        if diff_truncated:
            prompt_parts.append(f"*(Diff truncated. Full diff is {len(diff)} characters)*")

        prompt_parts.extend([
            "",
            "## Instructions",
            "",
            "1. Read the changed files to understand the modifications",
            "2. Analyze for issues based on the focus area",
            "3. For each finding, provide:",
            "   - Specific file path and line number",
            "   - Severity: critical, high, medium, low, or info",
            "   - Clear description of the issue",
            "   - Suggested fix or improvement",
            "4. Be evidence-based - only report issues you can verify from the code",
        ])

        prompt = '\n'.join(prompt_parts)

        result = {
            'prompt': prompt,
            'targets': targets,
            'target_count': len(targets)
        }

        print(json.dumps(result))
        EOF
      env:
        CHANGED_FILES: "{{ blocks.context_gathering.outputs.changed_files | tojson }}"
        PR_METADATA: "{{ blocks.context_gathering.outputs.pr_metadata | tojson }}"
        DIFF: "{{ blocks.context_gathering.outputs.diff }}"
        FOCUS: "{{ inputs.focus | default('general') }}"
        THRESHOLD: "{{ inputs.threshold }}"

  - id: investigation
    type: Workflow
    description: |
      CORTEX cognitive investigation using cortex-cell.
      The "quality" category will be auto-detected, providing:
      - gather: {depth: focused, spawn: complex_only}
      - reason: {goal: evaluate, spawn: uncertain_only}
      - act: {trigger: findings_require, mode: report}
    depends_on:
      - context_gathering
      - build_review_prompt
    inputs:
      workflow: cortex-cell
      inputs:
        prompt: "{{ (blocks.build_review_prompt.outputs.stdout | fromjson).prompt }}"
        context:
          repo_path: "{{ blocks.context_gathering.outputs.repo_path }}"
          diff: "{{ blocks.context_gathering.outputs.diff }}"
          pr_metadata: "{{ blocks.context_gathering.outputs.pr_metadata }}"
          changed_files: "{{ blocks.context_gathering.outputs.changed_files }}"
          base_branch: "{{ blocks.context_gathering.outputs.base_branch }}"
          head_branch: "{{ blocks.context_gathering.outputs.head_branch }}"
          focus: "{{ inputs.focus | default('general') }}"
          threshold: "{{ inputs.threshold }}"
        depth: 0
        max_depth: "{{ inputs.max_depth }}"
        iterations: 0
        max_iterations: 3
        confidence_threshold: 0.7
        state: "{{ blocks.state_management.outputs.state }}"
        parent_id: "{{ get(blocks.state_management.outputs.task, 'task_id', '') }}"
        profile: "default"
        permissions:
          read: true
          write: false
          execute: false

  # ==========================================================================
  # PHASE 2b: Transform CORTEX synthesis to generate-report format
  # Bridges the format difference between cortex-cell output and generate-report input
  # ==========================================================================
  - id: transform_synthesis
    type: Shell
    description: |
      Transform CORTEX synthesis format to the format expected by generate-report.
      Maps: findings→all_findings, summary→executive_summary, recommendations→action_items
    depends_on:
      - investigation
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        # Load CORTEX synthesis
        synthesis_json = os.environ.get('CORTEX_SYNTHESIS', '{}')
        threshold = os.environ.get('THRESHOLD', 'low').lower()

        def safe_json_loads(val, default={}):
            """Handle potentially double-encoded JSON strings."""
            if not val:
                return default
            try:
                result = json.loads(val)
                # If result is still a string, it was double-encoded - decode again
                if isinstance(result, str):
                    result = json.loads(result)
                return result
            except (json.JSONDecodeError, TypeError):
                return default

        cortex_synthesis = safe_json_loads(synthesis_json, {})

        # Severity order for filtering and risk assessment
        SEVERITY_ORDER = ['critical', 'high', 'medium', 'low', 'info']

        # Get threshold index
        try:
            threshold_idx = SEVERITY_ORDER.index(threshold)
        except ValueError:
            threshold_idx = 3  # Default to 'low'

        # Extract CORTEX fields
        cortex_findings = cortex_synthesis.get('findings', [])
        summary = cortex_synthesis.get('summary', 'No synthesis available.')
        risk_level = cortex_synthesis.get('risk_level', 'info')
        recommendations = cortex_synthesis.get('recommendations', [])
        actions_needed = cortex_synthesis.get('actions_needed', False)

        # Transform findings from CORTEX format to generate-report format
        all_findings = []
        severity_counts = {s: 0 for s in SEVERITY_ORDER}

        for f in cortex_findings:
            severity = f.get('severity', 'info').lower()
            if severity not in SEVERITY_ORDER:
                severity = 'info'

            # Filter by threshold
            severity_idx = SEVERITY_ORDER.index(severity)
            if severity_idx > threshold_idx:
                continue

            severity_counts[severity] += 1

            # Parse evidence chain for file_path and line_number
            evidence_chain = f.get('evidence_chain', [])
            file_path = ''
            line_number = None

            for ev in evidence_chain:
                if isinstance(ev, str) and ':' in ev:
                    parts = ev.split(':')
                    file_path = parts[0]
                    if len(parts) > 1 and parts[1].isdigit():
                        line_number = int(parts[1])
                    break

            # Transform finding
            transformed = {
                'title': f.get('claim', 'Finding'),
                'description': f.get('claim', ''),
                'severity': severity,
                'type': 'quality',  # Default type for CORTEX findings
                'file_path': file_path,
                'line_number': line_number,
                'suggestion': '',  # CORTEX doesn't provide separate suggestions
                'evidence_type': 'direct' if f.get('confidence', 0) >= 0.9 else 'inferred',
                'confidence': f.get('confidence', 0.7),
                'evidence_chain': evidence_chain,
                'verified': f.get('confidence', 0) >= 0.7
            }
            all_findings.append(transformed)

        # Calculate filtered out count
        total_cortex_findings = len(cortex_findings)
        filtered_out_count = total_cortex_findings - len(all_findings)

        # Generate top issues (up to 5, highest severity first)
        sorted_findings = sorted(all_findings, key=lambda x: SEVERITY_ORDER.index(x['severity']))
        top_issues = []
        for f in sorted_findings[:5]:
            top_issues.append({
                'title': f['title'],
                'severity': f['severity'],
                'description': f['description'],
                'action_required': f['severity'] in ['critical', 'high']
            })

        # Determine approval (inverse of actions_needed, unless critical/high issues exist)
        has_blocking = severity_counts['critical'] > 0 or severity_counts['high'] > 0
        approve = not actions_needed and not has_blocking

        # Build statistics
        statistics = {
            'findings_count': len(all_findings),
            'files_analyzed': len(set(f['file_path'] for f in all_findings if f['file_path'])),
            'severity_counts': {k: v for k, v in severity_counts.items() if v > 0},
            'threshold_applied': threshold,
            'investigations_count': 1,  # CORTEX handles internally
            'depth_distribution': {'0': 1}  # Placeholder
        }

        # Build transformed synthesis in generate-report format
        transformed_synthesis = {
            'executive_summary': summary,
            'risk_level': risk_level,
            'approve': approve,
            'approval_rationale': f"{'Approved: No blocking issues found.' if approve else 'Changes requested: ' + str(severity_counts['critical']) + ' critical and ' + str(severity_counts['high']) + ' high severity issues found.'}",
            'top_issues': top_issues,
            'action_items': recommendations,
            'all_findings': all_findings,
            'filtered_out_count': filtered_out_count,
            'statistics': statistics,
            'acknowledgments': []
        }

        print(json.dumps(transformed_synthesis))
        EOF
      env:
        CORTEX_SYNTHESIS: "{{ blocks.investigation.outputs.synthesis | tojson }}"
        THRESHOLD: "{{ inputs.threshold }}"

  # ==========================================================================
  # PHASE 3: Generate Report
  # Format synthesis results into markdown report
  # ==========================================================================
  - id: generate_report
    type: Workflow
    description: |
      Generate markdown report from transformed CORTEX synthesis.
      Formats findings, risk assessment, and approval decision.
    depends_on:
      - transform_synthesis
      - context_gathering
    inputs:
      workflow: generate-report
      inputs:
        synthesis: "{{ blocks.transform_synthesis.outputs.stdout | fromjson }}"
        pr_metadata: "{{ blocks.context_gathering.outputs.pr_metadata | default({}) }}"
        focus: "{{ inputs.focus | default('general') }}"
        threshold: "{{ inputs.threshold }}"

  # ==========================================================================
  # PHASE 4: Action
  # Post review comments if enabled, return final outputs
  # ==========================================================================
  - id: action
    type: Workflow
    description: |
      Post review comments to the PR/MR if conditions are met.
      Uses platform info from context_gathering to determine API to use.
    depends_on:
      - transform_synthesis
      - generate_report
    inputs:
      workflow: post-review-comments
      inputs:
        # Use normalized pr_url from context_gathering (empty for local repos)
        url: "{{ blocks.context_gathering.outputs.pr_url }}"
        # Use detected platform from context_gathering
        platform: "{{ blocks.context_gathering.outputs.platform }}"
        comment: "{{ inputs.comment }}"
        report: "{{ blocks.generate_report.outputs.report }}"
        approve: "{{ (blocks.transform_synthesis.outputs.stdout | fromjson).approve | default(false) }}"
        state: "{{ blocks.state_management.outputs.state }}"
        parent_task_id: "{{ get(blocks.state_management.outputs.task, 'task_id', '') }}"

outputs:
  report:
    value: "{{ blocks.action.outputs.final_report }}"
    type: str
    description: |
      Full review findings in markdown format, including all issues found,
      code suggestions, and rationale.

  approve:
    value: "{{ blocks.action.outputs.final_approval }}"
    type: bool
    description: |
      Whether the PR should be approved.
      If false, the report contains details on why.

  # Additional outputs for debugging/integration
  platform:
    value: "{{ blocks.context_gathering.outputs.platform }}"
    type: str
    description: "Detected platform: 'github', 'gitlab', or 'local'."

  repo_path:
    value: "{{ blocks.context_gathering.outputs.repo_path }}"
    type: str
    description: "Local path where the repository is located."

  risk_level:
    value: "{{ (blocks.transform_synthesis.outputs.stdout | fromjson).risk_level | default('unknown') }}"
    type: str
    description: "Overall risk level assessment: critical, high, medium, low, info."

  task_tree:
    value: "{{ blocks.action.outputs.task_tree | default('') }}"
    type: str
    description: "ASCII tree visualization of task hierarchy for debugging."

  task_summary:
    value: "{{ blocks.action.outputs.task_summary | default({}) }}"
    type: dict
    description: "Task statistics (total_tasks, done, failed, total_duration, etc.)."

  synthesis:
    value: "{{ blocks.transform_synthesis.outputs.stdout | fromjson }}"
    type: dict
    description: "Full synthesis results from CORTEX cognitive investigation."

  investigation_stats:
    value: "{{ (blocks.transform_synthesis.outputs.stdout | fromjson).statistics | default({}) }}"
    type: dict
    description: "Statistics from the CORTEX cognitive investigation."
