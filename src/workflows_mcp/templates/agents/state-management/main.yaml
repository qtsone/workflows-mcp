name: agent-state-management
description: |
  Reusable hierarchical state management with audit trail.

  Supports recursive workflows through a task tree model:
  - Root tasks: Created when no state/parent_id provided
  - Sub-tasks: Created by specifying parent_id
  - Updates: Target specific task_id within the state

  Storage: SQLite database at ~/.workflows/tasks/<trace_id>.db
  (configurable via WORKFLOWS_STATE_DIR)

  Uses WAL mode for concurrent access with automatic retry on busy.

tags: [agent, state-management, task-tracking, audit, hierarchical, sqlite]

inputs:
  state:
    type: str
    description: |
      Path to existing SQLite database.
      - Empty: Creates new state in ~/.workflows/tasks/<trace_id>.db
      - Existing path (*.db): Uses that database
    required: false
    default: ""

  task_id:
    type: str
    description: |
      Task to operate on within the state:
      - Empty + no parent_id: Operates on root task
      - Empty + parent_id: Creates NEW sub-task under parent
      - Specified: Updates EXISTING task with that ID
    required: false
    default: ""

  parent_id:
    type: str
    description: |
      Parent task ID when creating a new sub-task.
      - Empty: This is a root task or updating existing task
      - Specified: Creates new sub-task under this parent
    required: false
    default: ""

  kind:
    type: str
    description: Task type classification
    required: false
    default: ""

  name:
    type: str
    description: Task name (typically the workflow name)
    required: false
    default: ""

  metadata:
    type: dict
    description: Metadata for additional task info
    required: false
    default: {}

  inputs_data:
    type: dict
    description: Input data passed to this task
    required: false
    default: {}

  outputs_data:
    type: dict
    description: Output data produced by this task
    required: false
    default: {}

  depth:
    type: num
    description: Task depth in the hierarchy (0 = root, increments for children)
    required: false
    default: 0

  caller:
    type: str
    description: Who/what is making this call (for audit trail)
    required: true

  status:
    type: str
    description: Task status (pending, running, done, failed)
    required: false
    default: ""

  audit:
    type: bool
    description: Whether to add an audit entry for this operation
    required: false
    default: true

  ai:
    type: bool
    description: Whether to use AI for generating audit descriptions
    required: false
    default: false

  op:
    type: str
    description: |
      Operation type to perform:
      - task: Task operations (create, update) - default
      - iteration: Iteration tracking (init, advance, checkpoint, complete, reset)
      - memory: Key-value memory (set, get, append, merge, delete, keys)
      - query: Relational query on task tree (children, parent, ancestors, descendants)
      - evidence: Episodic memory (store, query, get, delete gathered content)
      - facts: Semantic memory with FTS (store, query, get, conflicts, update_status)
      - actions: Action deduplication (log, check, complete, fail, query)
    required: false
    default: "task"

  query:
    type: dict
    description: |
      Query specification (when op=query). Uses task_id as reference point.
      Structure:
        relation: children | parent | ancestors | descendants (required)
        where: { category: "...", status: "..." }  # Optional filters
        select: ["key1", "key2"]  # Optional: keys to extract from data JSON
        limit: 100  # Optional: max results (default 100)

      Examples:
        # Get child cells with synthesis data
        { relation: children, where: { category: cortex-cell }, select: [synthesis] }

        # Get parent task
        { relation: parent, select: [accumulated_knowledge] }

        # Get all ancestors
        { relation: ancestors, select: [synthesis] }
    required: false
    default: {}

  iteration_op:
    type: str
    description: |
      Iteration operation (when op=iteration):
      - init: Initialize iteration counter
      - advance: Increment counter
      - checkpoint: Save checkpoint data
      - complete: Mark iteration done
      - reset: Reset counter to zero
    required: false
    default: ""

  iteration_total:
    type: num
    description: Expected total iterations (for progress calculation).
    required: false
    default: 0

  iteration_cap:
    type: num
    description: Maximum iterations allowed (safety limit).
    required: false
    default: 100

  checkpoint_data:
    type: str
    description: JSON data to save with iteration checkpoint.
    required: false
    default: ""

  memory_op:
    type: str
    description: |
      Memory operation (when op=memory):
      - set: Store value at key
      - get: Retrieve value at key
      - append: Append to list at key
      - merge: Deep merge dict into key
      - delete: Remove key
      - keys: List keys matching pattern
    required: false
    default: ""

  memory_key:
    type: str
    description: Key path using dot-notation (e.g., "context.framework").
    required: false
    default: ""

  memory_value:
    type: str
    description: Value for set/append/merge operations (as JSON string).
    required: false
    default: ""

  memory_default:
    type: str
    description: Default value for get operation if key missing (as JSON string).
    required: false
    default: ""

  memory_type:
    type: str
    description: |
      Memory namespace for composite key support.
      Combined with memory_key forms composite primary key (type, key) in SQLite.
      Examples:
        - type: "findings" + key: "categorize" → row at (findings, categorize)
        - type: "evidence" + key: "gather" → row at (evidence, gather)
    required: false
    default: ""

  evidence_op:
    type: str
    description: |
      Evidence operation (when op=evidence):
      - store: Store evidence with metadata
      - query: Query evidence by type/cell
      - get: Get single evidence by key
      - delete: Remove evidence by key
    required: false
    default: ""

  evidence_key:
    type: str
    description: Evidence key (file path or search identifier).
    required: false
    default: ""

  evidence_value:
    type: str
    description: Evidence content (JSON string).
    required: false
    default: ""

  evidence_type:
    type: str
    description: Type of evidence (content, outline, search).
    required: false
    default: "content"

  evidence_query_type:
    type: str
    description: Filter by evidence_type for query op.
    required: false
    default: ""

  evidence_limit:
    type: num
    description: Max results for evidence query.
    required: false
    default: 100

  # Facts-specific inputs (used when op=facts)
  facts_op:
    type: str
    description: |
      Facts operation (when op=facts):
      - store: Store a new fact
      - query: Query facts (FTS or filter)
      - get: Get single fact by ID
      - conflicts: Detect conflicting facts
      - update_status: Update fact status
    required: false
    default: ""

  facts_claim:
    type: str
    description: The fact statement.
    required: false
    default: ""

  facts_evidence_type:
    type: str
    description: Epistemic status (direct, inferred, assumed).
    required: false
    default: "inferred"

  facts_confidence:
    type: num
    description: Confidence score (0.0-1.0).
    required: false
    default: 0.7

  facts_grounding:
    type: str
    description: Evidence references as JSON array.
    required: false
    default: "[]"

  facts_scope:
    type: str
    description: Optional scope limitation (e.g., "src/auth/").
    required: false
    default: ""

  facts_source_cell:
    type: str
    description: Cell ID that created this fact.
    required: false
    default: ""

  facts_query:
    type: str
    description: FTS query string (for query op).
    required: false
    default: ""

  facts_status_filter:
    type: str
    description: Filter by status (default "active").
    required: false
    default: "active"

  facts_limit:
    type: num
    description: Max results for facts query.
    required: false
    default: 50

  facts_id:
    type: str
    description: Fact ID for get/update operations.
    required: false
    default: ""

  facts_new_status:
    type: str
    description: New status for update_status operation.
    required: false
    default: ""

  facts_new_facts:
    type: str
    description: JSON array of new facts to check for conflicts.
    required: false
    default: "[]"

  # Actions-specific inputs (used when op=actions)
  actions_op:
    type: str
    description: |
      Actions operation (when op=actions):
      - log: Log a new action (returns existing if duplicate)
      - check: Check if action already exists/completed
      - complete: Mark action as completed with result
      - fail: Mark action as failed with error
      - query: Query actions by cell/capability
    required: false
    default: ""

  actions_cell_id:
    type: str
    description: Cell ID requesting the action.
    required: false
    default: ""

  actions_capability:
    type: str
    description: Capability workflow name.
    required: false
    default: ""

  actions_inputs_json:
    type: str
    description: Action inputs (JSON string).
    required: false
    default: "{}"

  actions_result_json:
    type: str
    description: Action result (JSON string) for complete/fail operation.
    required: false
    default: ""

  actions_action_id:
    type: str
    description: Action ID for complete/fail/query operations.
    required: false
    default: ""

  actions_status_filter:
    type: str
    description: Filter by status for query op.
    required: false
    default: ""

  actions_limit:
    type: num
    description: Max results for actions query.
    required: false
    default: 100

blocks:
  # ==========================================================================
  # STEP 1: Generate database path and initialize SQLite schema
  # ==========================================================================
  - id: paths
    type: Shell
    description: Generate trace_id and initialize SQLite database with schema.
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, secrets, sqlite3
        from pathlib import Path
        from datetime import datetime, timezone

        state_input = os.environ.get('STATE_INPUT', '')
        state_dir = os.environ.get('WORKFLOWS_STATE_DIR', str(Path.home() / '.workflows' / 'tasks'))
        Path(state_dir).mkdir(parents=True, exist_ok=True)

        # Determine paths
        if state_input:
            if state_input.endswith('.json'):
                raise ValueError(
                    f"JSON state files are no longer supported. "
                    f"Please delete old traces and use .db extension: {state_input}"
                )
            elif state_input.endswith('.db'):
                db_path = state_input
                trace_id = Path(state_input).stem
            else:
                # Assume it's a trace_id or path without extension
                trace_id = Path(state_input).stem
                db_path = str(Path(state_dir) / f'{trace_id}.db')
        else:
            # New state
            trace_id = f'exec-{secrets.token_hex(6)}'
            db_path = str(Path(state_dir) / f'{trace_id}.db')

        db_exists = os.path.isfile(db_path)
        now = datetime.now(timezone.utc).isoformat()

        # Initialize database if new
        if not db_exists:
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA busy_timeout=30000")
            conn.execute("PRAGMA synchronous=NORMAL")

            # Create schema
            conn.executescript('''
                CREATE TABLE IF NOT EXISTS metadata (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL
                );

                -- Tasks table (v1 schema - K8s-inspired)
                CREATE TABLE IF NOT EXISTS tasks (
                    task_id TEXT PRIMARY KEY,
                    parent_id TEXT REFERENCES tasks(task_id),
                    -- Identity
                    kind TEXT NOT NULL CHECK(kind IN ('cell', 'phase', 'capability')),
                    name TEXT NOT NULL,
                    metadata JSON DEFAULT '{}',
                    -- Contract
                    inputs JSON DEFAULT '{}',
                    outputs JSON DEFAULT '{}',
                    -- Lifecycle
                    status TEXT DEFAULT 'pending'
                        CHECK(status IN ('pending', 'running', 'done', 'failed')),
                    depth INTEGER DEFAULT 0,
                    -- Timestamps
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                );
                CREATE INDEX IF NOT EXISTS idx_tasks_parent ON tasks(parent_id);
                CREATE INDEX IF NOT EXISTS idx_tasks_kind ON tasks(kind);
                CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);
                CREATE INDEX IF NOT EXISTS idx_tasks_depth ON tasks(depth);

                CREATE TABLE IF NOT EXISTS memory (
                    type TEXT NOT NULL DEFAULT '',
                    key TEXT NOT NULL,
                    value JSON,
                    updated_at TEXT NOT NULL,
                    PRIMARY KEY (type, key)
                );
                CREATE INDEX IF NOT EXISTS idx_memory_type_key ON memory(type, key);

                CREATE TABLE IF NOT EXISTS audit (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    task_id TEXT,
                    caller TEXT NOT NULL,
                    action TEXT NOT NULL,
                    description TEXT,
                    changes JSON,
                    parent_id TEXT
                );
                CREATE INDEX IF NOT EXISTS idx_audit_task ON audit(task_id);
                CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON audit(timestamp);

                CREATE TABLE IF NOT EXISTS iterations (
                    task_id TEXT PRIMARY KEY,
                    current INTEGER DEFAULT 0,
                    total INTEGER DEFAULT 0,
                    cap INTEGER DEFAULT 100,
                    started_at TEXT,
                    completed_at TEXT,
                    checkpoints JSON DEFAULT '[]'
                );

                -- CORTEX Memory Architecture (v2)
                -- Evidence table (episodic memory)
                CREATE TABLE IF NOT EXISTS evidence (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    evidence_type TEXT NOT NULL
                        CHECK(evidence_type IN ('content', 'outline', 'search')),
                    task_id TEXT NOT NULL,
                    byte_size INTEGER,
                    created_at TEXT DEFAULT (datetime('now'))
                );
                CREATE INDEX IF NOT EXISTS idx_evidence_type ON evidence(evidence_type);
                CREATE INDEX IF NOT EXISTS idx_evidence_task ON evidence(task_id);

                -- Facts table (semantic memory)
                CREATE TABLE IF NOT EXISTS facts (
                    id TEXT PRIMARY KEY,
                    claim TEXT NOT NULL,
                    evidence_type TEXT NOT NULL
                        CHECK(evidence_type IN ('direct', 'inferred', 'assumed')),
                    confidence REAL NOT NULL
                        CHECK(confidence >= 0 AND confidence <= 1),
                    grounding TEXT NOT NULL,
                    scope TEXT,
                    source_cell TEXT NOT NULL,
                    source_phase TEXT DEFAULT 'reason',
                    status TEXT DEFAULT 'active'
                        CHECK(status IN ('active', 'verified', 'contested', 'superseded', 'refuted')),
                    superseded_by TEXT,
                    contested_with TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    updated_at TEXT DEFAULT (datetime('now'))
                );
                CREATE INDEX IF NOT EXISTS idx_facts_status ON facts(status);
                CREATE INDEX IF NOT EXISTS idx_facts_type ON facts(evidence_type);
                CREATE INDEX IF NOT EXISTS idx_facts_cell ON facts(source_cell);
                CREATE INDEX IF NOT EXISTS idx_facts_confidence ON facts(confidence DESC);

                -- FTS5 for fact search
                CREATE VIRTUAL TABLE IF NOT EXISTS facts_fts USING fts5(
                    id, claim, content=facts, content_rowid=rowid
                );

                -- FTS sync triggers
                CREATE TRIGGER IF NOT EXISTS facts_ai AFTER INSERT ON facts BEGIN
                    INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                END;
                CREATE TRIGGER IF NOT EXISTS facts_ad AFTER DELETE ON facts BEGIN
                    INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                END;
                CREATE TRIGGER IF NOT EXISTS facts_au AFTER UPDATE ON facts BEGIN
                    INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                    INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                END;

                -- Actions table (deduplication)
                CREATE TABLE IF NOT EXISTS actions (
                    id TEXT PRIMARY KEY,
                    cell_id TEXT NOT NULL,
                    capability TEXT NOT NULL,
                    inputs_hash TEXT NOT NULL,
                    inputs TEXT NOT NULL,
                    status TEXT DEFAULT 'pending'
                        CHECK(status IN ('pending', 'completed', 'failed', 'rolled_back')),
                    result TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    completed_at TEXT,
                    UNIQUE(capability, inputs_hash)
                );
                CREATE INDEX IF NOT EXISTS idx_actions_status ON actions(status);
                CREATE INDEX IF NOT EXISTS idx_actions_cell ON actions(cell_id);
                CREATE INDEX IF NOT EXISTS idx_actions_capability ON actions(capability);
            ''')

            # Initialize metadata
            conn.execute("INSERT INTO metadata VALUES ('created_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('updated_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('schema_version', '1')")
            conn.commit()
            conn.close()
        else:
            # Verify schema version (v1 required, no migrations)
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA busy_timeout=30000")

            try:
                row = conn.execute("SELECT value FROM metadata WHERE key='schema_version'").fetchone()
                schema_version = row[0] if row else None
            except:
                schema_version = None

            conn.close()

            if schema_version != '1':
                raise ValueError(
                    f"Incompatible database schema version: {schema_version}. "
                    f"Expected version '1'. Please delete the database at {db_path} "
                    f"and start fresh."
                )

        print(json.dumps({
            'trace_id': trace_id,
            'db_path': db_path,
            'db_exists': db_exists,
            # Backward compat fields (all point to same DB)
            'state_path': db_path,
            'memory_path': db_path,
            'audit_path': db_path
        }))
        EOF
      env:
        STATE_INPUT: "{{inputs.state}}"

  # ==========================================================================
  # ROUTING: Handle iteration operations
  # ==========================================================================
  - id: handle_iteration
    type: Workflow
    description: Route to iteration sub-workflow when op=iteration.
    depends_on: [paths]
    condition: "{{inputs.op == 'iteration'}}"
    inputs:
      workflow: agent-iteration
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        task_id: "{{inputs.task_id}}"
        op: "{{inputs.iteration_op}}"
        total: "{{inputs.iteration_total}}"
        cap: "{{inputs.iteration_cap}}"
        checkpoint_data: "{{inputs.checkpoint_data}}"

  # ==========================================================================
  # ROUTING: Handle memory operations
  # ==========================================================================
  - id: handle_memory
    type: Workflow
    description: Route to memory sub-workflow when op=memory.
    depends_on: [paths]
    condition: "{{inputs.op == 'memory'}}"
    inputs:
      workflow: agent-memory
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.memory_op}}"
        type: "{{inputs.memory_type}}"
        key: "{{inputs.memory_key}}"
        value: "{{inputs.memory_value}}"
        default: "{{inputs.memory_default}}"

  # ==========================================================================
  # ROUTING: Handle query operations (v1 schema)
  # ==========================================================================
  - id: handle_query
    type: Shell
    description: Execute relational query on task tree when op=query.
    depends_on: [paths]
    condition: "{{ inputs.op == 'query' }}"
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3

        db_path = os.environ['DB_PATH']
        task_id = os.environ.get('TASK_ID', '')
        query_json = os.environ.get('QUERY', '{}')

        try:
            query = json.loads(query_json) if query_json else {}
        except json.JSONDecodeError:
            print(json.dumps({'error': 'Invalid query JSON', 'results': [], 'count': 0}))
            exit(0)

        relation = query.get('relation', '')
        where = query.get('where', {})
        select_keys = query.get('select', [])
        limit = query.get('limit', 100)

        if not relation:
            print(json.dumps({'error': 'relation is required in query', 'results': [], 'count': 0}))
            exit(0)

        if not task_id and relation != 'all':
            print(json.dumps({'error': 'task_id required for relational queries', 'results': [], 'count': 0}))
            exit(0)

        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA busy_timeout=30000")

        results = []

        # v1 schema columns: task_id, parent_id, kind, name, metadata, inputs, outputs, status, depth
        base_cols = "task_id, parent_id, kind, name, metadata, inputs, outputs, status, depth"

        if relation == 'children':
            # Direct children of task_id
            sql = f"SELECT {base_cols} FROM tasks WHERE parent_id = ?"
            params = [task_id]

        elif relation == 'parent':
            # Get parent of task_id
            row = conn.execute("SELECT parent_id FROM tasks WHERE task_id = ?", (task_id,)).fetchone()
            if row and row['parent_id']:
                sql = f"SELECT {base_cols} FROM tasks WHERE task_id = ?"
                params = [row['parent_id']]
            else:
                print(json.dumps({'results': [], 'count': 0, 'relation': 'parent'}))
                conn.close()
                exit(0)

        elif relation == 'ancestors':
            # Recursive CTE to get all ancestors
            sql = f"""
                WITH RECURSIVE ancestors AS (
                    SELECT {base_cols}, 1 as traversal_depth
                    FROM tasks WHERE task_id = (SELECT parent_id FROM tasks WHERE task_id = ?)
                    UNION ALL
                    SELECT t.task_id, t.parent_id, t.kind, t.name, t.metadata, t.inputs, t.outputs,
                           t.status, t.depth, a.traversal_depth + 1
                    FROM tasks t
                    JOIN ancestors a ON t.task_id = a.parent_id
                )
                SELECT {base_cols}, traversal_depth FROM ancestors
                ORDER BY traversal_depth
            """
            params = [task_id]

        elif relation == 'descendants':
            # Recursive CTE to get all descendants
            sql = f"""
                WITH RECURSIVE descendants AS (
                    SELECT {base_cols}, 1 as traversal_depth
                    FROM tasks WHERE parent_id = ?
                    UNION ALL
                    SELECT t.task_id, t.parent_id, t.kind, t.name, t.metadata, t.inputs, t.outputs,
                           t.status, t.depth, d.traversal_depth + 1
                    FROM tasks t
                    JOIN descendants d ON t.parent_id = d.task_id
                )
                SELECT {base_cols}, traversal_depth FROM descendants
                ORDER BY traversal_depth
            """
            params = [task_id]

        else:
            print(json.dumps({'error': f'Unknown relation: {relation}', 'results': [], 'count': 0}))
            conn.close()
            exit(0)

        # Execute base query
        rows = conn.execute(sql, params).fetchall()

        # Apply where filters and build results
        for row in rows:
            # Check where conditions (v1: use kind instead of category)
            match = True
            if where:
                if 'kind' in where and row['kind'] != where['kind']:
                    match = False
                if 'name' in where and row['name'] != where['name']:
                    match = False
                if 'status' in where and row['status'] != where['status']:
                    match = False

            if not match:
                continue

            # Parse JSON columns
            try:
                metadata = json.loads(row['metadata']) if row['metadata'] else {}
            except json.JSONDecodeError:
                metadata = {}

            try:
                inputs = json.loads(row['inputs']) if row['inputs'] else {}
            except json.JSONDecodeError:
                inputs = {}

            try:
                outputs = json.loads(row['outputs']) if row['outputs'] else {}
            except json.JSONDecodeError:
                outputs = {}

            # Build result object (v1 schema)
            result = {
                'task_id': row['task_id'],
                'parent_id': row['parent_id'],
                'kind': row['kind'],
                'name': row['name'],
                'depth': row['depth'],
                'status': row['status']
            }

            # Handle select_keys extraction from inputs/outputs/metadata
            if select_keys:
                # Extract only specified keys from inputs, outputs, or metadata
                for key in select_keys:
                    # Check inputs first, then outputs, then metadata
                    if key in inputs:
                        result[key] = inputs[key]
                    elif key in outputs:
                        result[key] = outputs[key]
                    elif key in metadata:
                        result[key] = metadata[key]
                    elif key == 'metadata':
                        result['metadata'] = metadata
                    elif key == 'inputs':
                        result['inputs'] = inputs
                    elif key == 'outputs':
                        result['outputs'] = outputs
            else:
                # Return full metadata, inputs, outputs
                result['metadata'] = metadata
                result['inputs'] = inputs
                result['outputs'] = outputs

            results.append(result)

            if len(results) >= limit:
                break

        conn.close()

        print(json.dumps({
            'results': results,
            'count': len(results),
            'relation': relation,
            'reference_task_id': task_id
        }))
        EOF
      env:
        DB_PATH: "{{ (blocks.paths.outputs.stdout | fromjson).db_path }}"
        TASK_ID: "{{ inputs.task_id }}"
        QUERY: "{{ inputs.query | tojson }}"

  # ==========================================================================
  # ROUTING: Handle evidence operations (episodic memory)
  # ==========================================================================
  - id: handle_evidence
    type: Workflow
    description: Route to evidence sub-workflow when op=evidence.
    depends_on: [paths]
    condition: "{{inputs.op == 'evidence'}}"
    inputs:
      workflow: agent-evidence
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.evidence_op}}"
        key: "{{inputs.evidence_key}}"
        value: "{{inputs.evidence_value}}"
        evidence_type: "{{inputs.evidence_type}}"
        task_id: "{{inputs.task_id}}"
        query_type: "{{inputs.evidence_query_type}}"
        limit: "{{inputs.evidence_limit}}"

  # ==========================================================================
  # ROUTING: Handle facts operations (semantic memory with FTS)
  # ==========================================================================
  - id: handle_facts
    type: Workflow
    description: Route to facts sub-workflow when op=facts.
    depends_on: [paths]
    condition: "{{inputs.op == 'facts'}}"
    inputs:
      workflow: agent-facts
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.facts_op}}"
        claim: "{{inputs.facts_claim}}"
        evidence_type: "{{inputs.facts_evidence_type}}"
        confidence: "{{inputs.facts_confidence}}"
        grounding: "{{inputs.facts_grounding}}"
        scope: "{{inputs.facts_scope}}"
        source_cell: "{{inputs.facts_source_cell}}"
        query: "{{inputs.facts_query}}"
        status_filter: "{{inputs.facts_status_filter}}"
        limit: "{{inputs.facts_limit}}"
        fact_id: "{{inputs.facts_id}}"
        new_status: "{{inputs.facts_new_status}}"
        new_facts: "{{inputs.facts_new_facts}}"

  # ==========================================================================
  # ROUTING: Handle actions operations (deduplication)
  # ==========================================================================
  - id: handle_actions
    type: Workflow
    description: Route to actions sub-workflow when op=actions.
    depends_on: [paths]
    condition: "{{inputs.op == 'actions'}}"
    inputs:
      workflow: agent-actions
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.actions_op}}"
        cell_id: "{{inputs.actions_cell_id}}"
        capability: "{{inputs.actions_capability}}"
        inputs_json: "{{inputs.actions_inputs_json}}"
        result_json: "{{inputs.actions_result_json}}"
        action_id: "{{inputs.actions_action_id}}"
        status_filter: "{{inputs.actions_status_filter}}"
        limit: "{{inputs.actions_limit}}"

  # ==========================================================================
  # TASK OPS - STEP 2: Generate new task ID if needed
  # ==========================================================================
  - id: new_task_id
    type: Shell
    description: Generate a new task ID for root or sub-task creation.
    depends_on: [paths]
    condition: "{{inputs.op == 'task' and inputs.task_id == '' and (inputs.parent_id != '' or not (blocks.paths.outputs.stdout | fromjson).db_exists)}}"
    inputs:
      command: |
        python3 -c "import secrets; print(f'task-{secrets.token_hex(4)}')"

  # ==========================================================================
  # TASK OPS - STEP 3: Build/update task in SQLite
  # ==========================================================================
  - id: build_task
    type: Shell
    description: Create or update task in SQLite database (v1 schema).
    depends_on:
      - paths
      - block: new_task_id
        required: false
    condition: "{{inputs.op == 'task'}}"
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3, secrets
        from datetime import datetime, timezone

        # Load environment
        db_path = os.environ['DB_PATH']
        task_id_input = os.environ.get('TASK_ID_INPUT', '')
        parent_id_input = os.environ.get('PARENT_ID_INPUT', '')
        kind_input = os.environ.get('KIND_INPUT', '')
        name_input = os.environ.get('NAME_INPUT', '')
        metadata_input = json.loads(os.environ.get('METADATA_INPUT', '{}') or '{}')
        inputs_data = json.loads(os.environ.get('INPUTS_DATA', '{}') or '{}')
        outputs_data = json.loads(os.environ.get('OUTPUTS_DATA', '{}') or '{}')
        depth_input = int(os.environ.get('DEPTH_INPUT', '0') or '0')
        status_input = os.environ.get('STATUS_INPUT', '')
        caller = os.environ.get('CALLER', 'unknown')
        new_task_id = os.environ.get('NEW_TASK_ID', '').strip()

        now = datetime.now(timezone.utc).isoformat()

        # Connect to SQLite
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA busy_timeout=30000")

        # Get or create root_task_id from metadata
        root_row = conn.execute(
            "SELECT value FROM metadata WHERE key='root_task_id'"
        ).fetchone()
        root_task_id = root_row['value'] if root_row else None

        # Determine operation and target task
        changes = {}
        existing_metadata = {}
        existing_inputs = {}
        existing_outputs = {}

        if task_id_input:
            # Update existing task
            row = conn.execute(
                "SELECT * FROM tasks WHERE task_id=?", (task_id_input,)
            ).fetchone()
            if row:
                target_task_id = task_id_input
                operation = 'update'
                existing_metadata = json.loads(row['metadata'] or '{}')
                existing_inputs = json.loads(row['inputs'] or '{}')
                existing_outputs = json.loads(row['outputs'] or '{}')
            else:
                raise ValueError(f"Task {task_id_input} not found")
        elif parent_id_input:
            # Create sub-task
            target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
            operation = 'create_subtask'
            # Verify parent exists
            parent = conn.execute(
                "SELECT task_id FROM tasks WHERE task_id=?", (parent_id_input,)
            ).fetchone()
            if not parent:
                raise ValueError(f"Parent task {parent_id_input} not found")
        else:
            # Root task
            if root_task_id:
                # Check if root task exists in tasks table
                row = conn.execute(
                    "SELECT * FROM tasks WHERE task_id=?", (root_task_id,)
                ).fetchone()
                if row:
                    target_task_id = root_task_id
                    operation = 'update'
                    existing_metadata = json.loads(row['metadata'] or '{}')
                    existing_inputs = json.loads(row['inputs'] or '{}')
                    existing_outputs = json.loads(row['outputs'] or '{}')
                else:
                    # Root task ID in metadata but task not in table - create it
                    target_task_id = root_task_id
                    operation = 'create_root'
            else:
                # Create new root
                target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
                operation = 'create_root'
                conn.execute(
                    "INSERT OR REPLACE INTO metadata (key, value) VALUES ('root_task_id', ?)",
                    (target_task_id,)
                )
                root_task_id = target_task_id

        # Merge JSON fields (deep merge for metadata, simple merge for inputs/outputs)
        def deep_merge(base, overlay):
            """Deep merge overlay into base."""
            result = base.copy()
            for key, value in overlay.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = deep_merge(result[key], value)
                else:
                    result[key] = value
            return result

        merged_metadata = deep_merge(existing_metadata, metadata_input) if metadata_input else existing_metadata
        merged_inputs = {**existing_inputs, **inputs_data} if inputs_data else existing_inputs
        merged_outputs = {**existing_outputs, **outputs_data} if outputs_data else existing_outputs

        # Perform operation
        if operation in ('create_root', 'create_subtask'):
            if not kind_input:
                raise ValueError("kind is required when creating a task")
            if not name_input:
                raise ValueError("name is required when creating a task")

            conn.execute('''
                INSERT INTO tasks (task_id, parent_id, kind, name, metadata, inputs, outputs, depth, status, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                target_task_id,
                parent_id_input or None,
                kind_input,
                name_input,
                json.dumps(merged_metadata),
                json.dumps(merged_inputs),
                json.dumps(merged_outputs),
                depth_input,
                status_input or 'pending',
                now,
                now
            ))
            changes = {'created': True}
        else:
            # Update existing task - build dynamic UPDATE query
            updates = []
            params = []

            # Get current values for change tracking
            current_row = conn.execute(
                "SELECT kind, name, status FROM tasks WHERE task_id=?",
                (target_task_id,)
            ).fetchone()

            if kind_input and kind_input != (current_row['kind'] if current_row else ''):
                updates.append("kind=?")
                params.append(kind_input)
                changes['kind'] = [current_row['kind'] if current_row else '', kind_input]

            if name_input and name_input != (current_row['name'] if current_row else ''):
                updates.append("name=?")
                params.append(name_input)
                changes['name'] = [current_row['name'] if current_row else '', name_input]

            if status_input and status_input != (current_row['status'] if current_row else ''):
                updates.append("status=?")
                params.append(status_input)
                changes['status'] = [current_row['status'] if current_row else '', status_input]

            if metadata_input:
                updates.append("metadata=?")
                params.append(json.dumps(merged_metadata))
                changes['metadata'] = 'merged'

            if inputs_data:
                updates.append("inputs=?")
                params.append(json.dumps(merged_inputs))
                changes['inputs'] = 'merged'

            if outputs_data:
                updates.append("outputs=?")
                params.append(json.dumps(merged_outputs))
                changes['outputs'] = 'merged'

            updates.append("updated_at=?")
            params.append(now)
            params.append(target_task_id)

            if updates:
                conn.execute(f'''
                    UPDATE tasks SET {', '.join(updates)} WHERE task_id=?
                ''', params)

        # Update metadata timestamp
        conn.execute(
            "UPDATE metadata SET value=? WHERE key='updated_at'", (now,)
        )

        # Get children via SQL query (computed from parent_id relationships)
        children_rows = conn.execute(
            "SELECT task_id, status FROM tasks WHERE parent_id=?", (target_task_id,)
        ).fetchall()

        children = [r['task_id'] for r in children_rows]
        child_statuses = [r['status'] for r in children_rows]

        # Compute children stats
        children_done = sum(1 for s in child_statuses if s == 'done')
        children_failed = sum(1 for s in child_statuses if s == 'failed')
        children_running = sum(1 for s in child_statuses if s == 'running')
        children_pending_count = len(children) - children_done - children_failed - children_running

        # Aggregate status helpers
        all_children_done = len(children) > 0 and children_done == len(children)
        any_child_failed = children_failed > 0
        progress_pct = round((children_done / len(children) * 100), 1) if children else 100.0

        children_summary = {
            'total': len(children),
            'done': children_done,
            'failed': children_failed,
            'running': children_running,
            'pending': children_pending_count
        }

        # List of child IDs that aren't complete (for iteration)
        pending_children = [
            task_id for task_id, status in zip(children, child_statuses)
            if status not in ('done', 'failed')
        ]

        # Commit all changes
        conn.commit()

        # Get final task state
        row = conn.execute(
            "SELECT * FROM tasks WHERE task_id=?", (target_task_id,)
        ).fetchone()
        conn.close()

        # Build audit entry for audit block
        if operation.startswith('create'):
            audit_desc = f"Created {kind_input} task: {name_input}"
            action = operation
        elif changes:
            audit_desc = f"Updated: {list(changes.keys())}"
            action = 'task_completed' if status_input == 'done' else 'task_updated'
        else:
            audit_desc = 'Task accessed'
            action = 'task_accessed'

        audit_entry = {
            'timestamp': now,
            'task_id': target_task_id,
            'caller': caller,
            'action': action,
            'description': audit_desc
        }
        if changes:
            audit_entry['changes'] = changes
        if parent_id_input:
            audit_entry['parent_id'] = parent_id_input

        # Output (v1 schema)
        print(json.dumps({
            'task_id': target_task_id,
            'root_task_id': root_task_id or target_task_id,
            'kind': row['kind'] if row else kind_input,
            'name': row['name'] if row else name_input,
            'status': row['status'] if row else (status_input or 'pending'),
            'depth': row['depth'] if row else depth_input,
            'parent_id': row['parent_id'] if row else parent_id_input,
            'metadata': json.loads(row['metadata'] or '{}') if row else merged_metadata,
            'inputs': json.loads(row['inputs'] or '{}') if row else merged_inputs,
            'outputs': json.loads(row['outputs'] or '{}') if row else merged_outputs,
            'children': children,
            'pending_children': pending_children,
            'all_children_done': all_children_done,
            'any_child_failed': any_child_failed,
            'progress_pct': progress_pct,
            'children_summary': children_summary,
            # Internal field for audit block
            '_audit_entry': audit_entry,
            '_operation': operation
        }))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        TASK_ID_INPUT: "{{inputs.task_id}}"
        PARENT_ID_INPUT: "{{inputs.parent_id}}"
        KIND_INPUT: "{{inputs.kind}}"
        NAME_INPUT: "{{inputs.name}}"
        METADATA_INPUT: "{{inputs.metadata | tojson}}"
        INPUTS_DATA: "{{inputs.inputs_data | tojson}}"
        OUTPUTS_DATA: "{{inputs.outputs_data | tojson}}"
        DEPTH_INPUT: "{{inputs.depth}}"
        STATUS_INPUT: "{{inputs.status}}"
        CALLER: "{{inputs.caller}}"
        NEW_TASK_ID: "{{blocks.new_task_id.outputs.stdout if blocks.new_task_id.succeeded else ''}}"

  # ==========================================================================
  # TASK OPS - STEP 4: Write audit entry to SQLite
  # ==========================================================================
  - id: write_audit
    type: Shell
    description: Insert audit entry into SQLite audit table.
    condition: "{{inputs.op == 'task' and inputs.audit}}"
    depends_on: [build_task, paths]
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3

        db_path = os.environ['DB_PATH']
        entry = json.loads(os.environ['AUDIT_ENTRY'])

        conn = sqlite3.connect(db_path)
        conn.execute("PRAGMA busy_timeout=30000")

        # Insert audit entry
        conn.execute('''
            INSERT INTO audit (timestamp, task_id, caller, action, description, changes, parent_id)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            entry.get('timestamp'),
            entry.get('task_id'),
            entry.get('caller'),
            entry.get('action'),
            entry.get('description'),
            json.dumps(entry.get('changes')) if entry.get('changes') else None,
            entry.get('parent_id')
        ))
        conn.commit()

        # Get total audit entries
        count = conn.execute("SELECT COUNT(*) FROM audit").fetchone()[0]
        conn.close()

        print(json.dumps({'db_path': db_path, 'total_entries': count}))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        AUDIT_ENTRY: "{{(blocks.build_task.outputs.stdout | fromjson)._audit_entry | tojson}}"

outputs:
  # ==========================================================================
  # Common outputs (always available)
  # ==========================================================================
  state:
    description: Path to the SQLite database (entrypoint for all operations)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"

  trace_id:
    description: Execution trace ID (database file identifier)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).trace_id}}"

  # ==========================================================================
  # Task outputs (populated when op=task) - v1 schema
  # ==========================================================================
  task:
    description: |
      Task operation results (when op=task). Contains:
      - task_id: ID of created/updated task
      - root_task_id: ID of root task in state
      - kind: Task type (cell, phase, capability)
      - name: Task name (workflow name)
      - metadata: K8s-style metadata {labels: {}, annotations: {}}
      - inputs: Input data passed to this task
      - outputs: Output data produced by this task
      - status: Current task status (pending, running, done, failed)
      - depth: Task depth in hierarchy (0 = root)
      - parent_id: Parent task ID (null for root)
      - children: List of child task IDs
      - pending_children: Children not done/failed
      - all_children_done: True if all children done
      - any_child_failed: True if any child failed
      - progress_pct: Percentage of children completed
      - children_summary: Counts by status
    type: dict
    value: "{{(blocks.build_task.outputs.stdout | fromjson) if blocks.build_task.succeeded else {}}}"

  # ==========================================================================
  # Iteration outputs (populated when op=iteration)
  # ==========================================================================
  iteration:
    description: |
      Iteration operation results (when op=iteration). Contains:
      - current: Current iteration index (0-based)
      - total: Expected total iterations
      - cap: Maximum allowed (safety limit)
      - progress_pct: Percentage complete (0-100)
      - progress_str: Human-readable (e.g., "5/10 (50%)")
      - rate: Iterations per second
      - eta_seconds: Estimated seconds remaining
      - is_capped: True if safety limit reached
      - checkpoint: Last checkpoint data
    type: dict
    value: "{{blocks.handle_iteration.outputs if blocks.handle_iteration.succeeded else {}}}"

  # ==========================================================================
  # Memory outputs (populated when op=memory)
  # ==========================================================================
  memory:
    description: |
      Memory operation results (when op=memory). Contains:
      - value: Retrieved/resulting value
      - exists: Whether key existed before operation
      - previous: Previous value (set/delete ops)
      - keys: List of matching keys (keys op)
    type: dict
    value: "{{blocks.handle_memory.outputs if blocks.handle_memory.succeeded else {}}}"

  # ==========================================================================
  # Query outputs (populated when op=query) - v1 schema
  # ==========================================================================
  query:
    description: |
      Query operation results (when op=query). Contains:
      - results: List of matching tasks with:
        - task_id, parent_id, kind, name, depth, status
        - metadata, inputs, outputs (or selected keys from them)
      - count: Number of results returned
      - relation: The relation queried (children, parent, ancestors, descendants)
      - reference_task_id: The task_id used as reference point
      - error: Error message if query failed
    type: dict
    value: "{{ (blocks.handle_query.outputs.stdout | fromjson) if blocks.handle_query.succeeded else {} }}"

  # ==========================================================================
  # Evidence outputs (populated when op=evidence)
  # ==========================================================================
  evidence:
    description: |
      Evidence operation results (when op=evidence). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - items: Query results (for query op)
      - count: Number of items returned
      - total_bytes: Total bytes of evidence (for query op)
    type: dict
    value: "{{blocks.handle_evidence.outputs if blocks.handle_evidence.succeeded else {}}}"

  # ==========================================================================
  # Facts outputs (populated when op=facts)
  # ==========================================================================
  facts:
    description: |
      Facts operation results (when op=facts). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - fact_id: Fact ID (for store/get/update ops)
      - items: Query results (for query op)
      - count: Number of items returned
      - conflicts: Detected conflicts (for conflicts op)
      - has_conflicts: Whether conflicts were found
    type: dict
    value: "{{blocks.handle_facts.outputs if blocks.handle_facts.succeeded else {}}}"

  # ==========================================================================
  # Actions outputs (populated when op=actions)
  # ==========================================================================
  actions:
    description: |
      Actions operation results (when op=actions). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - action_id: Action ID (for log/complete/fail ops)
      - already_exists: Whether action already existed (for log op)
      - status: Action status
      - exists: Whether action exists (for check op)
      - items: Query results (for query op)
      - count: Number of items returned
    type: dict
    value: "{{blocks.handle_actions.outputs if blocks.handle_actions.succeeded else {}}}"
