name: agent-state-management
description: |
  Reusable hierarchical state management with audit trail.

  Supports recursive workflows through a task tree model:
  - Root tasks: Created when no state/parent_id provided
  - Sub-tasks: Created by specifying parent_id
  - Updates: Target specific task_id within the state

  Storage: SQLite database at ~/.workflows/tasks/<trace_id>.db
  (configurable via WORKFLOWS_STATE_DIR)

  Uses WAL mode for concurrent access with automatic retry on busy.

tags: [agent, state-management, task-tracking, audit, hierarchical, sqlite]

inputs:
  state:
    type: str
    description: |
      Path to existing SQLite database.
      - Empty: Creates new state in ~/.workflows/tasks/<trace_id>.db
      - Existing path (*.db): Uses that database
    required: false
    default: ""

  task_id:
    type: str
    description: |
      Task to operate on within the state:
      - Empty + no parent_id: Operates on root task
      - Empty + parent_id: Creates NEW sub-task under parent
      - Specified: Updates EXISTING task with that ID
    required: false
    default: ""

  parent_id:
    type: str
    description: |
      Parent task ID when creating a new sub-task.
      - Empty: This is a root task or updating existing task
      - Specified: Creates new sub-task under this parent
    required: false
    default: ""

  task:
    type: str
    description: Human-readable task description.
    required: false
    default: ""

  category:
    type: str
    description: Category/type of task (e.g., "investigation", "pr-review"). Optional.
    required: false
    default: ""

  caller:
    type: str
    description: Who/what is making this call (for audit trail).
    required: true

  data:
    type: dict
    description: Task-specific data to merge into the task's data field.
    required: false
    default: {}

  status:
    type: str
    description: Task status (pending, in-progress, done, failed, blocked).
    required: false
    default: ""

  audit:
    type: bool
    description: Whether to add an audit entry for this operation.
    required: false
    default: true

  ai:
    type: bool
    description: Whether to use AI for generating audit descriptions.
    required: false
    default: false

  # Operation routing
  op:
    type: str
    description: |
      Operation type to perform:
      - task: Task operations (create, update) - default
      - iteration: Iteration tracking (init, advance, checkpoint, complete, reset)
      - memory: Key-value memory (set, get, append, merge, delete, keys)
      - query: Relational query on task tree (children, parent, ancestors, descendants)
      - evidence: Episodic memory (store, query, get, delete gathered content)
      - facts: Semantic memory with FTS (store, query, get, conflicts, update_status)
      - actions: Action deduplication (log, check, complete, fail, query)
    required: false
    default: "task"

  # Query input (used when op=query)
  query:
    type: dict
    description: |
      Query specification (when op=query). Uses task_id as reference point.
      Structure:
        relation: children | parent | ancestors | descendants (required)
        where: { category: "...", status: "..." }  # Optional filters
        select: ["key1", "key2"]  # Optional: keys to extract from data JSON
        limit: 100  # Optional: max results (default 100)

      Examples:
        # Get child cells with synthesis data
        { relation: children, where: { category: cortex-cell }, select: [synthesis] }

        # Get parent task
        { relation: parent, select: [accumulated_knowledge] }

        # Get all ancestors
        { relation: ancestors, select: [synthesis] }
    required: false
    default: {}

  # Iteration-specific inputs (used when op=iteration)
  iteration_op:
    type: str
    description: |
      Iteration operation (when op=iteration):
      - init: Initialize iteration counter
      - advance: Increment counter
      - checkpoint: Save checkpoint data
      - complete: Mark iteration done
      - reset: Reset counter to zero
    required: false
    default: ""

  iteration_total:
    type: num
    description: Expected total iterations (for progress calculation).
    required: false
    default: 0

  iteration_cap:
    type: num
    description: Maximum iterations allowed (safety limit).
    required: false
    default: 100

  checkpoint_data:
    type: str
    description: JSON data to save with iteration checkpoint.
    required: false
    default: ""

  # Memory-specific inputs (used when op=memory)
  memory_op:
    type: str
    description: |
      Memory operation (when op=memory):
      - set: Store value at key
      - get: Retrieve value at key
      - append: Append to list at key
      - merge: Deep merge dict into key
      - delete: Remove key
      - keys: List keys matching pattern
    required: false
    default: ""

  memory_key:
    type: str
    description: Key path using dot-notation (e.g., "context.framework").
    required: false
    default: ""

  memory_value:
    type: str
    description: Value for set/append/merge operations (as JSON string).
    required: false
    default: ""

  memory_default:
    type: str
    description: Default value for get operation if key missing (as JSON string).
    required: false
    default: ""

  memory_type:
    type: str
    description: |
      Memory namespace for composite key support.
      Combined with memory_key forms composite primary key (type, key) in SQLite.
      Examples:
        - type: "findings" + key: "categorize" → row at (findings, categorize)
        - type: "evidence" + key: "gather" → row at (evidence, gather)
    required: false
    default: ""

  # Evidence-specific inputs (used when op=evidence)
  evidence_op:
    type: str
    description: |
      Evidence operation (when op=evidence):
      - store: Store evidence with metadata
      - query: Query evidence by type/cell
      - get: Get single evidence by key
      - delete: Remove evidence by key
    required: false
    default: ""

  evidence_key:
    type: str
    description: Evidence key (file path or search identifier).
    required: false
    default: ""

  evidence_value:
    type: str
    description: Evidence content (JSON string).
    required: false
    default: ""

  evidence_type:
    type: str
    description: Type of evidence (content, outline, search).
    required: false
    default: "content"

  evidence_source_cell:
    type: str
    description: Cell ID that gathered this evidence.
    required: false
    default: ""

  evidence_source_operation:
    type: str
    description: Capability name that produced this evidence.
    required: false
    default: ""

  evidence_query_type:
    type: str
    description: Filter by evidence_type for query op.
    required: false
    default: ""

  evidence_query_cell:
    type: str
    description: Filter by source_cell for query op.
    required: false
    default: ""

  evidence_limit:
    type: num
    description: Max results for evidence query.
    required: false
    default: 100

  # Facts-specific inputs (used when op=facts)
  facts_op:
    type: str
    description: |
      Facts operation (when op=facts):
      - store: Store a new fact
      - query: Query facts (FTS or filter)
      - get: Get single fact by ID
      - conflicts: Detect conflicting facts
      - update_status: Update fact status
    required: false
    default: ""

  facts_claim:
    type: str
    description: The fact statement.
    required: false
    default: ""

  facts_evidence_type:
    type: str
    description: Epistemic status (direct, inferred, assumed).
    required: false
    default: "inferred"

  facts_confidence:
    type: num
    description: Confidence score (0.0-1.0).
    required: false
    default: 0.7

  facts_grounding:
    type: str
    description: Evidence references as JSON array.
    required: false
    default: "[]"

  facts_scope:
    type: str
    description: Optional scope limitation (e.g., "src/auth/").
    required: false
    default: ""

  facts_source_cell:
    type: str
    description: Cell ID that created this fact.
    required: false
    default: ""

  facts_query:
    type: str
    description: FTS query string (for query op).
    required: false
    default: ""

  facts_status_filter:
    type: str
    description: Filter by status (default "active").
    required: false
    default: "active"

  facts_limit:
    type: num
    description: Max results for facts query.
    required: false
    default: 50

  facts_id:
    type: str
    description: Fact ID for get/update operations.
    required: false
    default: ""

  facts_new_status:
    type: str
    description: New status for update_status operation.
    required: false
    default: ""

  facts_new_facts:
    type: str
    description: JSON array of new facts to check for conflicts.
    required: false
    default: "[]"

  # Actions-specific inputs (used when op=actions)
  actions_op:
    type: str
    description: |
      Actions operation (when op=actions):
      - log: Log a new action (returns existing if duplicate)
      - check: Check if action already exists/completed
      - complete: Mark action as completed with result
      - fail: Mark action as failed with error
      - query: Query actions by cell/capability
    required: false
    default: ""

  actions_cell_id:
    type: str
    description: Cell ID requesting the action.
    required: false
    default: ""

  actions_capability:
    type: str
    description: Capability workflow name.
    required: false
    default: ""

  actions_inputs_json:
    type: str
    description: Action inputs (JSON string).
    required: false
    default: "{}"

  actions_result_json:
    type: str
    description: Action result (JSON string) for complete/fail operation.
    required: false
    default: ""

  actions_action_id:
    type: str
    description: Action ID for complete/fail/query operations.
    required: false
    default: ""

  actions_status_filter:
    type: str
    description: Filter by status for query op.
    required: false
    default: ""

  actions_limit:
    type: num
    description: Max results for actions query.
    required: false
    default: 100

blocks:
  # ==========================================================================
  # STEP 1: Generate database path and initialize SQLite schema
  # ==========================================================================
  - id: paths
    type: Shell
    description: Generate trace_id and initialize SQLite database with schema.
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, secrets, sqlite3
        from pathlib import Path
        from datetime import datetime, timezone

        state_input = os.environ.get('STATE_INPUT', '')
        state_dir = os.environ.get('WORKFLOWS_STATE_DIR', str(Path.home() / '.workflows' / 'tasks'))
        Path(state_dir).mkdir(parents=True, exist_ok=True)

        # Determine paths
        if state_input:
            if state_input.endswith('.json'):
                raise ValueError(
                    f"JSON state files are no longer supported. "
                    f"Please delete old traces and use .db extension: {state_input}"
                )
            elif state_input.endswith('.db'):
                db_path = state_input
                trace_id = Path(state_input).stem
            else:
                # Assume it's a trace_id or path without extension
                trace_id = Path(state_input).stem
                db_path = str(Path(state_dir) / f'{trace_id}.db')
        else:
            # New state
            trace_id = f'exec-{secrets.token_hex(6)}'
            db_path = str(Path(state_dir) / f'{trace_id}.db')

        db_exists = os.path.isfile(db_path)
        now = datetime.now(timezone.utc).isoformat()

        # Initialize database if new
        if not db_exists:
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA busy_timeout=30000")
            conn.execute("PRAGMA synchronous=NORMAL")

            # Create schema
            conn.executescript('''
                CREATE TABLE IF NOT EXISTS metadata (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL
                );

                CREATE TABLE IF NOT EXISTS tasks (
                    task_id TEXT PRIMARY KEY,
                    parent_id TEXT REFERENCES tasks(task_id),
                    task TEXT NOT NULL,
                    category TEXT DEFAULT '',
                    status TEXT DEFAULT 'pending'
                        CHECK(status IN ('pending', 'in-progress', 'done', 'failed', 'blocked')),
                    data JSON DEFAULT '{}',
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                );
                CREATE INDEX IF NOT EXISTS idx_tasks_parent ON tasks(parent_id);
                CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);

                CREATE TABLE IF NOT EXISTS memory (
                    type TEXT NOT NULL DEFAULT '',
                    key TEXT NOT NULL,
                    value JSON,
                    updated_at TEXT NOT NULL,
                    PRIMARY KEY (type, key)
                );
                CREATE INDEX IF NOT EXISTS idx_memory_type_key ON memory(type, key);

                CREATE TABLE IF NOT EXISTS audit (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    task_id TEXT,
                    caller TEXT NOT NULL,
                    action TEXT NOT NULL,
                    description TEXT,
                    changes JSON,
                    parent_id TEXT
                );
                CREATE INDEX IF NOT EXISTS idx_audit_task ON audit(task_id);
                CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON audit(timestamp);

                CREATE TABLE IF NOT EXISTS iterations (
                    task_id TEXT PRIMARY KEY,
                    current INTEGER DEFAULT 0,
                    total INTEGER DEFAULT 0,
                    cap INTEGER DEFAULT 100,
                    started_at TEXT,
                    completed_at TEXT,
                    checkpoints JSON DEFAULT '[]'
                );

                -- CORTEX Memory Architecture (v2)
                -- Evidence table (episodic memory)
                CREATE TABLE IF NOT EXISTS evidence (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL,
                    evidence_type TEXT NOT NULL
                        CHECK(evidence_type IN ('content', 'outline', 'search')),
                    source_cell TEXT NOT NULL,
                    source_operation TEXT,
                    byte_size INTEGER,
                    created_at TEXT DEFAULT (datetime('now'))
                );
                CREATE INDEX IF NOT EXISTS idx_evidence_type ON evidence(evidence_type);
                CREATE INDEX IF NOT EXISTS idx_evidence_cell ON evidence(source_cell);

                -- Facts table (semantic memory)
                CREATE TABLE IF NOT EXISTS facts (
                    id TEXT PRIMARY KEY,
                    claim TEXT NOT NULL,
                    evidence_type TEXT NOT NULL
                        CHECK(evidence_type IN ('direct', 'inferred', 'assumed')),
                    confidence REAL NOT NULL
                        CHECK(confidence >= 0 AND confidence <= 1),
                    grounding TEXT NOT NULL,
                    scope TEXT,
                    source_cell TEXT NOT NULL,
                    source_phase TEXT DEFAULT 'reason',
                    status TEXT DEFAULT 'active'
                        CHECK(status IN ('active', 'verified', 'contested', 'superseded', 'refuted')),
                    superseded_by TEXT,
                    contested_with TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    updated_at TEXT DEFAULT (datetime('now'))
                );
                CREATE INDEX IF NOT EXISTS idx_facts_status ON facts(status);
                CREATE INDEX IF NOT EXISTS idx_facts_type ON facts(evidence_type);
                CREATE INDEX IF NOT EXISTS idx_facts_cell ON facts(source_cell);
                CREATE INDEX IF NOT EXISTS idx_facts_confidence ON facts(confidence DESC);

                -- FTS5 for fact search
                CREATE VIRTUAL TABLE IF NOT EXISTS facts_fts USING fts5(
                    id, claim, content=facts, content_rowid=rowid
                );

                -- FTS sync triggers
                CREATE TRIGGER IF NOT EXISTS facts_ai AFTER INSERT ON facts BEGIN
                    INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                END;
                CREATE TRIGGER IF NOT EXISTS facts_ad AFTER DELETE ON facts BEGIN
                    INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                END;
                CREATE TRIGGER IF NOT EXISTS facts_au AFTER UPDATE ON facts BEGIN
                    INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                    INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                END;

                -- Actions table (deduplication)
                CREATE TABLE IF NOT EXISTS actions (
                    id TEXT PRIMARY KEY,
                    cell_id TEXT NOT NULL,
                    capability TEXT NOT NULL,
                    inputs_hash TEXT NOT NULL,
                    inputs TEXT NOT NULL,
                    status TEXT DEFAULT 'pending'
                        CHECK(status IN ('pending', 'completed', 'failed', 'rolled_back')),
                    result TEXT,
                    created_at TEXT DEFAULT (datetime('now')),
                    completed_at TEXT,
                    UNIQUE(capability, inputs_hash)
                );
                CREATE INDEX IF NOT EXISTS idx_actions_status ON actions(status);
                CREATE INDEX IF NOT EXISTS idx_actions_cell ON actions(cell_id);
                CREATE INDEX IF NOT EXISTS idx_actions_capability ON actions(capability);
            ''')

            # Initialize metadata
            conn.execute("INSERT INTO metadata VALUES ('created_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('updated_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('schema_version', '2')")
            conn.commit()
            conn.close()
        else:
            # Migrate existing database if needed
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA busy_timeout=30000")

            # Check schema version
            try:
                row = conn.execute("SELECT value FROM metadata WHERE key='schema_version'").fetchone()
                current_version = int(row[0]) if row else 1
            except:
                current_version = 1

            if current_version < 2:
                # Migrate to v2: Add CORTEX memory tables
                conn.executescript('''
                    -- Evidence table (episodic memory)
                    CREATE TABLE IF NOT EXISTS evidence (
                        key TEXT PRIMARY KEY,
                        value TEXT NOT NULL,
                        evidence_type TEXT NOT NULL
                            CHECK(evidence_type IN ('content', 'outline', 'search')),
                        source_cell TEXT NOT NULL,
                        source_operation TEXT,
                        byte_size INTEGER,
                        created_at TEXT DEFAULT (datetime('now'))
                    );
                    CREATE INDEX IF NOT EXISTS idx_evidence_type ON evidence(evidence_type);
                    CREATE INDEX IF NOT EXISTS idx_evidence_cell ON evidence(source_cell);

                    -- Facts table (semantic memory)
                    CREATE TABLE IF NOT EXISTS facts (
                        id TEXT PRIMARY KEY,
                        claim TEXT NOT NULL,
                        evidence_type TEXT NOT NULL
                            CHECK(evidence_type IN ('direct', 'inferred', 'assumed')),
                        confidence REAL NOT NULL
                            CHECK(confidence >= 0 AND confidence <= 1),
                        grounding TEXT NOT NULL,
                        scope TEXT,
                        source_cell TEXT NOT NULL,
                        source_phase TEXT DEFAULT 'reason',
                        status TEXT DEFAULT 'active'
                            CHECK(status IN ('active', 'verified', 'contested', 'superseded', 'refuted')),
                        superseded_by TEXT,
                        contested_with TEXT,
                        created_at TEXT DEFAULT (datetime('now')),
                        updated_at TEXT DEFAULT (datetime('now'))
                    );
                    CREATE INDEX IF NOT EXISTS idx_facts_status ON facts(status);
                    CREATE INDEX IF NOT EXISTS idx_facts_type ON facts(evidence_type);
                    CREATE INDEX IF NOT EXISTS idx_facts_cell ON facts(source_cell);
                    CREATE INDEX IF NOT EXISTS idx_facts_confidence ON facts(confidence DESC);

                    -- FTS5 for fact search
                    CREATE VIRTUAL TABLE IF NOT EXISTS facts_fts USING fts5(
                        id, claim, content=facts, content_rowid=rowid
                    );

                    -- FTS sync triggers
                    CREATE TRIGGER IF NOT EXISTS facts_ai AFTER INSERT ON facts BEGIN
                        INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                    END;
                    CREATE TRIGGER IF NOT EXISTS facts_ad AFTER DELETE ON facts BEGIN
                        INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                    END;
                    CREATE TRIGGER IF NOT EXISTS facts_au AFTER UPDATE ON facts BEGIN
                        INSERT INTO facts_fts(facts_fts, rowid, id, claim) VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
                        INSERT INTO facts_fts(rowid, id, claim) VALUES (NEW.rowid, NEW.id, NEW.claim);
                    END;

                    -- Actions table (deduplication)
                    CREATE TABLE IF NOT EXISTS actions (
                        id TEXT PRIMARY KEY,
                        cell_id TEXT NOT NULL,
                        capability TEXT NOT NULL,
                        inputs_hash TEXT NOT NULL,
                        inputs TEXT NOT NULL,
                        status TEXT DEFAULT 'pending'
                            CHECK(status IN ('pending', 'completed', 'failed', 'rolled_back')),
                        result TEXT,
                        created_at TEXT DEFAULT (datetime('now')),
                        completed_at TEXT,
                        UNIQUE(capability, inputs_hash)
                    );
                    CREATE INDEX IF NOT EXISTS idx_actions_status ON actions(status);
                    CREATE INDEX IF NOT EXISTS idx_actions_cell ON actions(cell_id);
                    CREATE INDEX IF NOT EXISTS idx_actions_capability ON actions(capability);
                ''')
                conn.execute("INSERT OR REPLACE INTO metadata VALUES ('schema_version', '2')")
                conn.commit()

            conn.close()

        print(json.dumps({
            'trace_id': trace_id,
            'db_path': db_path,
            'db_exists': db_exists,
            # Backward compat fields (all point to same DB)
            'state_path': db_path,
            'memory_path': db_path,
            'audit_path': db_path
        }))
        EOF
      env:
        STATE_INPUT: "{{inputs.state}}"

  # ==========================================================================
  # ROUTING: Handle iteration operations
  # ==========================================================================
  - id: handle_iteration
    type: Workflow
    description: Route to iteration sub-workflow when op=iteration.
    depends_on: [paths]
    condition: "{{inputs.op == 'iteration'}}"
    inputs:
      workflow: agent-iteration
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        task_id: "{{inputs.task_id}}"
        op: "{{inputs.iteration_op}}"
        total: "{{inputs.iteration_total}}"
        cap: "{{inputs.iteration_cap}}"
        checkpoint_data: "{{inputs.checkpoint_data}}"

  # ==========================================================================
  # ROUTING: Handle memory operations
  # ==========================================================================
  - id: handle_memory
    type: Workflow
    description: Route to memory sub-workflow when op=memory.
    depends_on: [paths]
    condition: "{{inputs.op == 'memory'}}"
    inputs:
      workflow: agent-memory
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.memory_op}}"
        type: "{{inputs.memory_type}}"
        key: "{{inputs.memory_key}}"
        value: "{{inputs.memory_value}}"
        default: "{{inputs.memory_default}}"

  # ==========================================================================
  # ROUTING: Handle query operations
  # ==========================================================================
  - id: handle_query
    type: Shell
    description: Execute relational query on task tree when op=query.
    depends_on: [paths]
    condition: "{{ inputs.op == 'query' }}"
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3

        db_path = os.environ['DB_PATH']
        task_id = os.environ.get('TASK_ID', '')
        query_json = os.environ.get('QUERY', '{}')

        try:
            query = json.loads(query_json) if query_json else {}
        except json.JSONDecodeError:
            print(json.dumps({'error': 'Invalid query JSON', 'results': [], 'count': 0}))
            exit(0)

        relation = query.get('relation', '')
        where = query.get('where', {})
        select_keys = query.get('select', [])
        limit = query.get('limit', 100)

        if not relation:
            print(json.dumps({'error': 'relation is required in query', 'results': [], 'count': 0}))
            exit(0)

        if not task_id and relation != 'all':
            print(json.dumps({'error': 'task_id required for relational queries', 'results': [], 'count': 0}))
            exit(0)

        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA busy_timeout=30000")

        results = []

        if relation == 'children':
            # Direct children of task_id
            sql = "SELECT task_id, parent_id, category, status, data FROM tasks WHERE parent_id = ?"
            params = [task_id]

        elif relation == 'parent':
            # Get parent of task_id
            row = conn.execute("SELECT parent_id FROM tasks WHERE task_id = ?", (task_id,)).fetchone()
            if row and row['parent_id']:
                sql = "SELECT task_id, parent_id, category, status, data FROM tasks WHERE task_id = ?"
                params = [row['parent_id']]
            else:
                print(json.dumps({'results': [], 'count': 0, 'relation': 'parent'}))
                conn.close()
                exit(0)

        elif relation == 'ancestors':
            # Recursive CTE to get all ancestors
            sql = """
                WITH RECURSIVE ancestors AS (
                    SELECT task_id, parent_id, category, status, data, 1 as depth
                    FROM tasks WHERE task_id = (SELECT parent_id FROM tasks WHERE task_id = ?)
                    UNION ALL
                    SELECT t.task_id, t.parent_id, t.category, t.status, t.data, a.depth + 1
                    FROM tasks t
                    JOIN ancestors a ON t.task_id = a.parent_id
                )
                SELECT task_id, parent_id, category, status, data, depth FROM ancestors
                ORDER BY depth
            """
            params = [task_id]

        elif relation == 'descendants':
            # Recursive CTE to get all descendants
            sql = """
                WITH RECURSIVE descendants AS (
                    SELECT task_id, parent_id, category, status, data, 1 as depth
                    FROM tasks WHERE parent_id = ?
                    UNION ALL
                    SELECT t.task_id, t.parent_id, t.category, t.status, t.data, d.depth + 1
                    FROM tasks t
                    JOIN descendants d ON t.parent_id = d.task_id
                )
                SELECT task_id, parent_id, category, status, data, depth FROM descendants
                ORDER BY depth
            """
            params = [task_id]

        else:
            print(json.dumps({'error': f'Unknown relation: {relation}', 'results': [], 'count': 0}))
            conn.close()
            exit(0)

        # Execute base query
        rows = conn.execute(sql, params).fetchall()

        # Apply where filters and build results
        for row in rows:
            # Check where conditions
            match = True
            if where:
                if 'category' in where and row['category'] != where['category']:
                    match = False
                if 'status' in where and row['status'] != where['status']:
                    match = False

            if not match:
                continue

            # Build result object
            result = {
                'task_id': row['task_id'],
                'parent_id': row['parent_id'],
                'category': row['category'],
                'status': row['status']
            }

            # Handle data extraction
            if row['data']:
                try:
                    data = json.loads(row['data'])
                    if select_keys:
                        # Extract only specified keys
                        for key in select_keys:
                            if key in data:
                                result[key] = data[key]
                    else:
                        # Return full data
                        result['data'] = data
                except json.JSONDecodeError:
                    result['data'] = {}

            results.append(result)

            if len(results) >= limit:
                break

        conn.close()

        print(json.dumps({
            'results': results,
            'count': len(results),
            'relation': relation,
            'reference_task_id': task_id
        }))
        EOF
      env:
        DB_PATH: "{{ (blocks.paths.outputs.stdout | fromjson).db_path }}"
        TASK_ID: "{{ inputs.task_id }}"
        QUERY: "{{ inputs.query | tojson }}"

  # ==========================================================================
  # ROUTING: Handle evidence operations (episodic memory)
  # ==========================================================================
  - id: handle_evidence
    type: Workflow
    description: Route to evidence sub-workflow when op=evidence.
    depends_on: [paths]
    condition: "{{inputs.op == 'evidence'}}"
    inputs:
      workflow: agent-evidence
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.evidence_op}}"
        key: "{{inputs.evidence_key}}"
        value: "{{inputs.evidence_value}}"
        evidence_type: "{{inputs.evidence_type}}"
        source_cell: "{{inputs.evidence_source_cell}}"
        source_operation: "{{inputs.evidence_source_operation}}"
        query_type: "{{inputs.evidence_query_type}}"
        query_cell: "{{inputs.evidence_query_cell}}"
        limit: "{{inputs.evidence_limit}}"

  # ==========================================================================
  # ROUTING: Handle facts operations (semantic memory with FTS)
  # ==========================================================================
  - id: handle_facts
    type: Workflow
    description: Route to facts sub-workflow when op=facts.
    depends_on: [paths]
    condition: "{{inputs.op == 'facts'}}"
    inputs:
      workflow: agent-facts
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.facts_op}}"
        claim: "{{inputs.facts_claim}}"
        evidence_type: "{{inputs.facts_evidence_type}}"
        confidence: "{{inputs.facts_confidence}}"
        grounding: "{{inputs.facts_grounding}}"
        scope: "{{inputs.facts_scope}}"
        source_cell: "{{inputs.facts_source_cell}}"
        query: "{{inputs.facts_query}}"
        status_filter: "{{inputs.facts_status_filter}}"
        limit: "{{inputs.facts_limit}}"
        fact_id: "{{inputs.facts_id}}"
        new_status: "{{inputs.facts_new_status}}"
        new_facts: "{{inputs.facts_new_facts}}"

  # ==========================================================================
  # ROUTING: Handle actions operations (deduplication)
  # ==========================================================================
  - id: handle_actions
    type: Workflow
    description: Route to actions sub-workflow when op=actions.
    depends_on: [paths]
    condition: "{{inputs.op == 'actions'}}"
    inputs:
      workflow: agent-actions
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.actions_op}}"
        cell_id: "{{inputs.actions_cell_id}}"
        capability: "{{inputs.actions_capability}}"
        inputs_json: "{{inputs.actions_inputs_json}}"
        result_json: "{{inputs.actions_result_json}}"
        action_id: "{{inputs.actions_action_id}}"
        status_filter: "{{inputs.actions_status_filter}}"
        limit: "{{inputs.actions_limit}}"

  # ==========================================================================
  # TASK OPS - STEP 2: Generate new task ID if needed
  # ==========================================================================
  - id: new_task_id
    type: Shell
    description: Generate a new task ID for root or sub-task creation.
    depends_on: [paths]
    condition: "{{inputs.op == 'task' and inputs.task_id == '' and (inputs.parent_id != '' or not (blocks.paths.outputs.stdout | fromjson).db_exists)}}"
    inputs:
      command: |
        python3 -c "import secrets; print(f'task-{secrets.token_hex(4)}')"

  # ==========================================================================
  # TASK OPS - STEP 3: Build/update task in SQLite
  # ==========================================================================
  - id: build_task
    type: Shell
    description: Create or update task in SQLite database.
    depends_on:
      - paths
      - block: new_task_id
        required: false
    condition: "{{inputs.op == 'task'}}"
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3, secrets
        from datetime import datetime, timezone

        # Load environment
        db_path = os.environ['DB_PATH']
        task_input = os.environ.get('TASK_INPUT', '')
        task_id_input = os.environ.get('TASK_ID_INPUT', '')
        parent_id_input = os.environ.get('PARENT_ID_INPUT', '')
        category_input = os.environ.get('TASK_TYPE_INPUT', '')
        status_input = os.environ.get('STATUS_INPUT', '')
        data_input = json.loads(os.environ.get('DATA_INPUT', '{}') or '{}')
        caller = os.environ.get('CALLER', 'unknown')
        new_task_id = os.environ.get('NEW_TASK_ID', '').strip()

        now = datetime.now(timezone.utc).isoformat()

        # Connect to SQLite
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA busy_timeout=30000")

        # Get or create root_task_id from metadata
        root_row = conn.execute(
            "SELECT value FROM metadata WHERE key='root_task_id'"
        ).fetchone()
        root_task_id = root_row['value'] if root_row else None

        # Determine operation and target task
        changes = {}
        existing_data = {}

        if task_id_input:
            # Update existing task
            row = conn.execute(
                "SELECT * FROM tasks WHERE task_id=?", (task_id_input,)
            ).fetchone()
            if row:
                target_task_id = task_id_input
                operation = 'update'
                existing_data = json.loads(row['data'] or '{}')
            else:
                raise ValueError(f"Task {task_id_input} not found")
        elif parent_id_input:
            # Create sub-task
            target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
            operation = 'create_subtask'
            # Verify parent exists
            parent = conn.execute(
                "SELECT task_id FROM tasks WHERE task_id=?", (parent_id_input,)
            ).fetchone()
            if not parent:
                raise ValueError(f"Parent task {parent_id_input} not found")
        else:
            # Root task
            if root_task_id:
                # Check if root task exists in tasks table
                row = conn.execute(
                    "SELECT * FROM tasks WHERE task_id=?", (root_task_id,)
                ).fetchone()
                if row:
                    target_task_id = root_task_id
                    operation = 'update'
                    existing_data = json.loads(row['data'] or '{}')
                else:
                    # Root task ID in metadata but task not in table - create it
                    target_task_id = root_task_id
                    operation = 'create_root'
            else:
                # Create new root
                target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
                operation = 'create_root'
                conn.execute(
                    "INSERT OR REPLACE INTO metadata (key, value) VALUES ('root_task_id', ?)",
                    (target_task_id,)
                )
                root_task_id = target_task_id

        # Merge data
        merged_data = {**existing_data, **data_input} if data_input else existing_data

        # Perform operation
        if operation in ('create_root', 'create_subtask'):
            conn.execute('''
                INSERT INTO tasks (task_id, parent_id, task, category, status, data, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                target_task_id,
                parent_id_input or None,
                task_input or 'Task',
                category_input or '',
                status_input or 'in-progress',
                json.dumps(merged_data),
                now,
                now
            ))
            changes = {'created': True}
        else:
            # Update existing task - build dynamic UPDATE query
            updates = []
            params = []

            # Get current values for change tracking
            current_row = conn.execute(
                "SELECT task, category, status FROM tasks WHERE task_id=?",
                (target_task_id,)
            ).fetchone()

            if task_input and task_input != (current_row['task'] if current_row else ''):
                updates.append("task=?")
                params.append(task_input)
                changes['task'] = [current_row['task'] if current_row else '', task_input]

            if category_input and category_input != (current_row['category'] if current_row else ''):
                updates.append("category=?")
                params.append(category_input)
                changes['category'] = [current_row['category'] if current_row else '', category_input]

            if status_input and status_input != (current_row['status'] if current_row else ''):
                updates.append("status=?")
                params.append(status_input)
                changes['status'] = [current_row['status'] if current_row else '', status_input]

            if data_input:
                updates.append("data=?")
                params.append(json.dumps(merged_data))
                changes['data'] = 'merged'

            updates.append("updated_at=?")
            params.append(now)
            params.append(target_task_id)

            if updates:
                conn.execute(f'''
                    UPDATE tasks SET {', '.join(updates)} WHERE task_id=?
                ''', params)

        # Update metadata timestamp
        conn.execute(
            "UPDATE metadata SET value=? WHERE key='updated_at'", (now,)
        )

        # Get children via SQL query (computed from parent_id relationships)
        children_rows = conn.execute(
            "SELECT task_id, status FROM tasks WHERE parent_id=?", (target_task_id,)
        ).fetchall()

        children = [r['task_id'] for r in children_rows]
        child_statuses = [r['status'] for r in children_rows]

        # Compute children stats
        children_done = sum(1 for s in child_statuses if s == 'done')
        children_failed = sum(1 for s in child_statuses if s == 'failed')
        children_in_progress = sum(1 for s in child_statuses if s == 'in-progress')
        children_pending_count = len(children) - children_done - children_failed - children_in_progress

        # Aggregate status helpers
        all_children_done = len(children) > 0 and children_done == len(children)
        any_child_failed = children_failed > 0
        progress_pct = round((children_done / len(children) * 100), 1) if children else 100.0

        children_summary = {
            'total': len(children),
            'done': children_done,
            'failed': children_failed,
            'in_progress': children_in_progress,
            'pending': children_pending_count
        }

        # List of child IDs that aren't complete (for iteration)
        pending_children = [
            task_id for task_id, status in zip(children, child_statuses)
            if status not in ('done', 'failed')
        ]

        # Commit all changes
        conn.commit()

        # Get final task state
        row = conn.execute(
            "SELECT * FROM tasks WHERE task_id=?", (target_task_id,)
        ).fetchone()
        conn.close()

        # Build audit entry for audit block
        if operation.startswith('create'):
            audit_desc = f"Created {'root' if operation == 'create_root' else 'sub-'}task: {task_input or target_task_id}"
            action = operation
        elif changes:
            audit_desc = f"Updated: {list(changes.keys())}"
            action = 'task_completed' if status_input == 'done' else 'task_updated'
        else:
            audit_desc = 'Task accessed'
            action = 'task_accessed'

        audit_entry = {
            'timestamp': now,
            'task_id': target_task_id,
            'caller': caller,
            'action': action,
            'description': audit_desc
        }
        if changes:
            audit_entry['changes'] = changes
        if parent_id_input:
            audit_entry['parent_id'] = parent_id_input

        # Output (matches current interface for callers)
        print(json.dumps({
            'task_id': target_task_id,
            'root_task_id': root_task_id or target_task_id,
            'status': row['status'] if row else (status_input or 'in-progress'),
            'parent_id': row['parent_id'] if row else parent_id_input,
            'children': children,
            'pending_children': pending_children,
            'data': json.loads(row['data'] or '{}') if row else merged_data,
            'all_children_done': all_children_done,
            'any_child_failed': any_child_failed,
            'progress_pct': progress_pct,
            'children_summary': children_summary,
            # Internal field for audit block
            '_audit_entry': audit_entry,
            '_operation': operation
        }))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        TASK_INPUT: "{{inputs.task}}"
        TASK_ID_INPUT: "{{inputs.task_id}}"
        PARENT_ID_INPUT: "{{inputs.parent_id}}"
        TASK_TYPE_INPUT: "{{inputs.category}}"
        STATUS_INPUT: "{{inputs.status}}"
        DATA_INPUT: "{{inputs.data | tojson}}"
        CALLER: "{{inputs.caller}}"
        NEW_TASK_ID: "{{blocks.new_task_id.outputs.stdout if blocks.new_task_id.succeeded else ''}}"

  # ==========================================================================
  # TASK OPS - STEP 4: Write audit entry to SQLite
  # ==========================================================================
  - id: write_audit
    type: Shell
    description: Insert audit entry into SQLite audit table.
    condition: "{{inputs.op == 'task' and inputs.audit}}"
    depends_on: [build_task, paths]
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3

        db_path = os.environ['DB_PATH']
        entry = json.loads(os.environ['AUDIT_ENTRY'])

        conn = sqlite3.connect(db_path)
        conn.execute("PRAGMA busy_timeout=30000")

        # Insert audit entry
        conn.execute('''
            INSERT INTO audit (timestamp, task_id, caller, action, description, changes, parent_id)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            entry.get('timestamp'),
            entry.get('task_id'),
            entry.get('caller'),
            entry.get('action'),
            entry.get('description'),
            json.dumps(entry.get('changes')) if entry.get('changes') else None,
            entry.get('parent_id')
        ))
        conn.commit()

        # Get total audit entries
        count = conn.execute("SELECT COUNT(*) FROM audit").fetchone()[0]
        conn.close()

        print(json.dumps({'db_path': db_path, 'total_entries': count}))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        AUDIT_ENTRY: "{{(blocks.build_task.outputs.stdout | fromjson)._audit_entry | tojson}}"

outputs:
  # ==========================================================================
  # Common outputs (always available)
  # ==========================================================================
  state:
    description: Path to the SQLite database (entrypoint for all operations)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"

  trace_id:
    description: Execution trace ID (database file identifier)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).trace_id}}"

  # ==========================================================================
  # Task outputs (populated when op=task)
  # ==========================================================================
  task:
    description: |
      Task operation results (when op=task). Contains:
      - task_id: ID of created/updated task
      - root_task_id: ID of root task in state
      - status: Current task status
      - parent_id: Parent task ID (null for root)
      - children: List of child task IDs
      - pending_children: Children not done/failed
      - data: Task-specific data field
      - all_children_done: True if all children done
      - any_child_failed: True if any child failed
      - progress_pct: Percentage of children completed
      - children_summary: Counts by status
    type: dict
    value: "{{(blocks.build_task.outputs.stdout | fromjson) if blocks.build_task.succeeded else {}}}"

  # ==========================================================================
  # Iteration outputs (populated when op=iteration)
  # ==========================================================================
  iteration:
    description: |
      Iteration operation results (when op=iteration). Contains:
      - current: Current iteration index (0-based)
      - total: Expected total iterations
      - cap: Maximum allowed (safety limit)
      - progress_pct: Percentage complete (0-100)
      - progress_str: Human-readable (e.g., "5/10 (50%)")
      - rate: Iterations per second
      - eta_seconds: Estimated seconds remaining
      - is_capped: True if safety limit reached
      - checkpoint: Last checkpoint data
    type: dict
    value: "{{blocks.handle_iteration.outputs if blocks.handle_iteration.succeeded else {}}}"

  # ==========================================================================
  # Memory outputs (populated when op=memory)
  # ==========================================================================
  memory:
    description: |
      Memory operation results (when op=memory). Contains:
      - value: Retrieved/resulting value
      - exists: Whether key existed before operation
      - previous: Previous value (set/delete ops)
      - keys: List of matching keys (keys op)
    type: dict
    value: "{{blocks.handle_memory.outputs if blocks.handle_memory.succeeded else {}}}"

  # ==========================================================================
  # Query outputs (populated when op=query)
  # ==========================================================================
  query:
    description: |
      Query operation results (when op=query). Contains:
      - results: List of matching tasks with requested data
      - count: Number of results returned
      - relation: The relation queried (children, parent, ancestors, descendants)
      - reference_task_id: The task_id used as reference point
      - error: Error message if query failed
    type: dict
    value: "{{ (blocks.handle_query.outputs.stdout | fromjson) if blocks.handle_query.succeeded else {} }}"

  # ==========================================================================
  # Evidence outputs (populated when op=evidence)
  # ==========================================================================
  evidence:
    description: |
      Evidence operation results (when op=evidence). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - items: Query results (for query op)
      - count: Number of items returned
      - total_bytes: Total bytes of evidence (for query op)
    type: dict
    value: "{{blocks.handle_evidence.outputs if blocks.handle_evidence.succeeded else {}}}"

  # ==========================================================================
  # Facts outputs (populated when op=facts)
  # ==========================================================================
  facts:
    description: |
      Facts operation results (when op=facts). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - fact_id: Fact ID (for store/get/update ops)
      - items: Query results (for query op)
      - count: Number of items returned
      - conflicts: Detected conflicts (for conflicts op)
      - has_conflicts: Whether conflicts were found
    type: dict
    value: "{{blocks.handle_facts.outputs if blocks.handle_facts.succeeded else {}}}"

  # ==========================================================================
  # Actions outputs (populated when op=actions)
  # ==========================================================================
  actions:
    description: |
      Actions operation results (when op=actions). Contains:
      - result: Full operation result
      - success: Whether operation succeeded
      - action_id: Action ID (for log/complete/fail ops)
      - already_exists: Whether action already existed (for log op)
      - status: Action status
      - exists: Whether action exists (for check op)
      - items: Query results (for query op)
      - count: Number of items returned
    type: dict
    value: "{{blocks.handle_actions.outputs if blocks.handle_actions.succeeded else {}}}"
