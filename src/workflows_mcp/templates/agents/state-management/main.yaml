name: agent-state-management
description: |
  Reusable hierarchical state management with audit trail.

  Supports recursive workflows through a task tree model:
  - Root tasks: Created when no state/parent_id provided
  - Sub-tasks: Created by specifying parent_id
  - Updates: Target specific task_id within the state

  Storage: SQLite database at ~/.workflows/tasks/<trace_id>.db
  (configurable via WORKFLOWS_STATE_DIR)

  Uses WAL mode for concurrent access with automatic retry on busy.

tags: [agent, state-management, task-tracking, audit, hierarchical, sqlite]

inputs:
  state:
    type: str
    description: |
      Path to existing SQLite database or trace ID.
      - Empty: Creates new state in ~/.workflows/tasks/<trace_id>.db
      - Existing path (*.db): Uses that database
      - Trace ID: Uses ~/.workflows/tasks/<trace_id>.db
    required: false
    default: ""

  task_id:
    type: str
    description: |
      Task to operate on within the state:
      - Empty + no parent_id: Operates on root task
      - Empty + parent_id: Creates NEW sub-task under parent
      - Specified: Updates EXISTING task with that ID
    required: false
    default: ""

  parent_id:
    type: str
    description: |
      Parent task ID when creating a new sub-task.
      - Empty: This is a root task or updating existing task
      - Specified: Creates new sub-task under this parent
    required: false
    default: ""

  task:
    type: str
    description: Human-readable task description.
    required: false
    default: ""

  task_type:
    type: str
    description: Category/type of task (e.g., "investigation", "pr-review"). Optional.
    required: false
    default: ""

  caller:
    type: str
    description: Who/what is making this call (for audit trail).
    required: true

  data:
    type: dict
    description: Task-specific data to merge into the task's data field.
    required: false
    default: {}

  status:
    type: str
    description: Task status (pending, in-progress, done, failed, blocked).
    required: false
    default: ""

  audit:
    type: bool
    description: Whether to add an audit entry for this operation.
    required: false
    default: true

  ai:
    type: bool
    description: Whether to use AI for generating audit descriptions.
    required: false
    default: true

  # Operation routing
  op:
    type: str
    description: |
      Operation type to perform:
      - task: Task operations (create, update) - default
      - iteration: Iteration tracking (init, advance, checkpoint, complete, reset)
      - memory: Key-value memory (set, get, append, merge, delete, keys)
    required: false
    default: "task"

  # Iteration-specific inputs (used when op=iteration)
  iteration_op:
    type: str
    description: |
      Iteration operation (when op=iteration):
      - init: Initialize iteration counter
      - advance: Increment counter
      - checkpoint: Save checkpoint data
      - complete: Mark iteration done
      - reset: Reset counter to zero
    required: false
    default: ""

  iteration_total:
    type: num
    description: Expected total iterations (for progress calculation).
    required: false
    default: 0

  iteration_cap:
    type: num
    description: Maximum iterations allowed (safety limit).
    required: false
    default: 100

  checkpoint_data:
    type: str
    description: JSON data to save with iteration checkpoint.
    required: false
    default: ""

  # Memory-specific inputs (used when op=memory)
  memory_op:
    type: str
    description: |
      Memory operation (when op=memory):
      - set: Store value at key
      - get: Retrieve value at key
      - append: Append to list at key
      - merge: Deep merge dict into key
      - delete: Remove key
      - keys: List keys matching pattern
    required: false
    default: ""

  memory_key:
    type: str
    description: Key path using dot-notation (e.g., "context.framework").
    required: false
    default: ""

  memory_value:
    type: str
    description: Value for set/append/merge operations (as JSON string).
    required: false
    default: ""

  memory_default:
    type: str
    description: Default value for get operation if key missing (as JSON string).
    required: false
    default: ""

blocks:
  # ==========================================================================
  # STEP 1: Generate database path and initialize SQLite schema
  # ==========================================================================
  - id: paths
    type: Shell
    description: Generate trace_id and initialize SQLite database with schema.
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, secrets, sqlite3
        from pathlib import Path
        from datetime import datetime, timezone

        state_input = os.environ.get('STATE_INPUT', '')
        state_dir = os.environ.get('WORKFLOWS_STATE_DIR', str(Path.home() / '.workflows' / 'tasks'))
        Path(state_dir).mkdir(parents=True, exist_ok=True)

        # Determine paths
        if state_input:
            if state_input.endswith('.json'):
                raise ValueError(
                    f"JSON state files are no longer supported. "
                    f"Please delete old traces and use .db extension: {state_input}"
                )
            elif state_input.endswith('.db'):
                db_path = state_input
                trace_id = Path(state_input).stem
            else:
                # Assume it's a trace_id or path without extension
                trace_id = Path(state_input).stem
                db_path = str(Path(state_dir) / f'{trace_id}.db')
        else:
            # New state
            trace_id = f'exec-{secrets.token_hex(6)}'
            db_path = str(Path(state_dir) / f'{trace_id}.db')

        db_exists = os.path.isfile(db_path)
        now = datetime.now(timezone.utc).isoformat()

        # Initialize database if new
        if not db_exists:
            conn = sqlite3.connect(db_path)
            conn.execute("PRAGMA journal_mode=WAL")
            conn.execute("PRAGMA busy_timeout=30000")
            conn.execute("PRAGMA synchronous=NORMAL")

            # Create schema
            conn.executescript('''
                CREATE TABLE IF NOT EXISTS metadata (
                    key TEXT PRIMARY KEY,
                    value TEXT NOT NULL
                );

                CREATE TABLE IF NOT EXISTS tasks (
                    task_id TEXT PRIMARY KEY,
                    parent_id TEXT REFERENCES tasks(task_id),
                    task TEXT NOT NULL,
                    task_type TEXT DEFAULT '',
                    status TEXT DEFAULT 'pending'
                        CHECK(status IN ('pending', 'in-progress', 'done', 'failed', 'blocked')),
                    data JSON DEFAULT '{}',
                    created_at TEXT NOT NULL,
                    updated_at TEXT NOT NULL
                );
                CREATE INDEX IF NOT EXISTS idx_tasks_parent ON tasks(parent_id);
                CREATE INDEX IF NOT EXISTS idx_tasks_status ON tasks(status);

                CREATE TABLE IF NOT EXISTS memory (
                    key TEXT PRIMARY KEY,
                    value JSON,
                    updated_at TEXT NOT NULL
                );
                CREATE INDEX IF NOT EXISTS idx_memory_key ON memory(key);

                CREATE TABLE IF NOT EXISTS audit (
                    id INTEGER PRIMARY KEY AUTOINCREMENT,
                    timestamp TEXT NOT NULL,
                    task_id TEXT,
                    caller TEXT NOT NULL,
                    action TEXT NOT NULL,
                    description TEXT,
                    changes JSON,
                    parent_id TEXT
                );
                CREATE INDEX IF NOT EXISTS idx_audit_task ON audit(task_id);
                CREATE INDEX IF NOT EXISTS idx_audit_timestamp ON audit(timestamp);

                CREATE TABLE IF NOT EXISTS iterations (
                    task_id TEXT PRIMARY KEY,
                    current INTEGER DEFAULT 0,
                    total INTEGER DEFAULT 0,
                    cap INTEGER DEFAULT 100,
                    started_at TEXT,
                    completed_at TEXT,
                    checkpoints JSON DEFAULT '[]'
                );
            ''')

            # Initialize metadata
            conn.execute("INSERT INTO metadata VALUES ('created_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('updated_at', ?)", (now,))
            conn.execute("INSERT INTO metadata VALUES ('schema_version', '1')")
            conn.commit()
            conn.close()

        print(json.dumps({
            'trace_id': trace_id,
            'db_path': db_path,
            'db_exists': db_exists,
            # Backward compat fields (all point to same DB)
            'state_path': db_path,
            'memory_path': db_path,
            'audit_path': db_path
        }))
        EOF
      env:
        STATE_INPUT: "{{inputs.state}}"

  # ==========================================================================
  # ROUTING: Handle iteration operations
  # ==========================================================================
  - id: handle_iteration
    type: Workflow
    description: Route to iteration sub-workflow when op=iteration.
    depends_on: [paths]
    condition: "{{inputs.op == 'iteration'}}"
    inputs:
      workflow: agent-iteration
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        task_id: "{{inputs.task_id}}"
        op: "{{inputs.iteration_op}}"
        total: "{{inputs.iteration_total}}"
        cap: "{{inputs.iteration_cap}}"
        checkpoint_data: "{{inputs.checkpoint_data}}"

  # ==========================================================================
  # ROUTING: Handle memory operations
  # ==========================================================================
  - id: handle_memory
    type: Workflow
    description: Route to memory sub-workflow when op=memory.
    depends_on: [paths]
    condition: "{{inputs.op == 'memory'}}"
    inputs:
      workflow: agent-memory
      inputs:
        path: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        op: "{{inputs.memory_op}}"
        key: "{{inputs.memory_key}}"
        value: "{{inputs.memory_value}}"
        default: "{{inputs.memory_default}}"

  # ==========================================================================
  # TASK OPS - STEP 2: Generate new task ID if needed
  # ==========================================================================
  - id: new_task_id
    type: Shell
    description: Generate a new task ID for root or sub-task creation.
    depends_on: [paths]
    condition: "{{inputs.op == 'task' and inputs.task_id == '' and (inputs.parent_id != '' or not (blocks.paths.outputs.stdout | fromjson).db_exists)}}"
    inputs:
      command: |
        python3 -c "import secrets; print(f'task-{secrets.token_hex(4)}')"

  # ==========================================================================
  # TASK OPS - STEP 3: Build/update task in SQLite
  # ==========================================================================
  - id: build_task
    type: Shell
    description: Create or update task in SQLite database.
    depends_on:
      - paths
      - block: new_task_id
        required: false
    condition: "{{inputs.op == 'task'}}"
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3, secrets
        from datetime import datetime, timezone

        # Load environment
        db_path = os.environ['DB_PATH']
        task_input = os.environ.get('TASK_INPUT', '')
        task_id_input = os.environ.get('TASK_ID_INPUT', '')
        parent_id_input = os.environ.get('PARENT_ID_INPUT', '')
        task_type_input = os.environ.get('TASK_TYPE_INPUT', '')
        status_input = os.environ.get('STATUS_INPUT', '')
        data_input = json.loads(os.environ.get('DATA_INPUT', '{}') or '{}')
        caller = os.environ.get('CALLER', 'unknown')
        new_task_id = os.environ.get('NEW_TASK_ID', '').strip()

        now = datetime.now(timezone.utc).isoformat()

        # Connect to SQLite
        conn = sqlite3.connect(db_path)
        conn.row_factory = sqlite3.Row
        conn.execute("PRAGMA busy_timeout=30000")

        # Get or create root_task_id from metadata
        root_row = conn.execute(
            "SELECT value FROM metadata WHERE key='root_task_id'"
        ).fetchone()
        root_task_id = root_row['value'] if root_row else None

        # Determine operation and target task
        changes = {}
        existing_data = {}

        if task_id_input:
            # Update existing task
            row = conn.execute(
                "SELECT * FROM tasks WHERE task_id=?", (task_id_input,)
            ).fetchone()
            if row:
                target_task_id = task_id_input
                operation = 'update'
                existing_data = json.loads(row['data'] or '{}')
            else:
                raise ValueError(f"Task {task_id_input} not found")
        elif parent_id_input:
            # Create sub-task
            target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
            operation = 'create_subtask'
            # Verify parent exists
            parent = conn.execute(
                "SELECT task_id FROM tasks WHERE task_id=?", (parent_id_input,)
            ).fetchone()
            if not parent:
                raise ValueError(f"Parent task {parent_id_input} not found")
        else:
            # Root task
            if root_task_id:
                # Check if root task exists in tasks table
                row = conn.execute(
                    "SELECT * FROM tasks WHERE task_id=?", (root_task_id,)
                ).fetchone()
                if row:
                    target_task_id = root_task_id
                    operation = 'update'
                    existing_data = json.loads(row['data'] or '{}')
                else:
                    # Root task ID in metadata but task not in table - create it
                    target_task_id = root_task_id
                    operation = 'create_root'
            else:
                # Create new root
                target_task_id = new_task_id or f'task-{secrets.token_hex(4)}'
                operation = 'create_root'
                conn.execute(
                    "INSERT OR REPLACE INTO metadata (key, value) VALUES ('root_task_id', ?)",
                    (target_task_id,)
                )
                root_task_id = target_task_id

        # Merge data
        merged_data = {**existing_data, **data_input} if data_input else existing_data

        # Perform operation
        if operation in ('create_root', 'create_subtask'):
            conn.execute('''
                INSERT INTO tasks (task_id, parent_id, task, task_type, status, data, created_at, updated_at)
                VALUES (?, ?, ?, ?, ?, ?, ?, ?)
            ''', (
                target_task_id,
                parent_id_input or None,
                task_input or 'Task',
                task_type_input or '',
                status_input or 'in-progress',
                json.dumps(merged_data),
                now,
                now
            ))
            changes = {'created': True}
        else:
            # Update existing task - build dynamic UPDATE query
            updates = []
            params = []

            # Get current values for change tracking
            current_row = conn.execute(
                "SELECT task, task_type, status FROM tasks WHERE task_id=?",
                (target_task_id,)
            ).fetchone()

            if task_input and task_input != (current_row['task'] if current_row else ''):
                updates.append("task=?")
                params.append(task_input)
                changes['task'] = [current_row['task'] if current_row else '', task_input]

            if task_type_input and task_type_input != (current_row['task_type'] if current_row else ''):
                updates.append("task_type=?")
                params.append(task_type_input)
                changes['task_type'] = [current_row['task_type'] if current_row else '', task_type_input]

            if status_input and status_input != (current_row['status'] if current_row else ''):
                updates.append("status=?")
                params.append(status_input)
                changes['status'] = [current_row['status'] if current_row else '', status_input]

            if data_input:
                updates.append("data=?")
                params.append(json.dumps(merged_data))
                changes['data'] = 'merged'

            updates.append("updated_at=?")
            params.append(now)
            params.append(target_task_id)

            if updates:
                conn.execute(f'''
                    UPDATE tasks SET {', '.join(updates)} WHERE task_id=?
                ''', params)

        # Update metadata timestamp
        conn.execute(
            "UPDATE metadata SET value=? WHERE key='updated_at'", (now,)
        )

        # Get children via SQL query (computed from parent_id relationships)
        children_rows = conn.execute(
            "SELECT task_id, status FROM tasks WHERE parent_id=?", (target_task_id,)
        ).fetchall()

        children = [r['task_id'] for r in children_rows]
        child_statuses = [r['status'] for r in children_rows]

        # Compute children stats
        children_done = sum(1 for s in child_statuses if s == 'done')
        children_failed = sum(1 for s in child_statuses if s == 'failed')
        children_in_progress = sum(1 for s in child_statuses if s == 'in-progress')
        children_pending_count = len(children) - children_done - children_failed - children_in_progress

        # Aggregate status helpers
        all_children_done = len(children) > 0 and children_done == len(children)
        any_child_failed = children_failed > 0
        progress_pct = round((children_done / len(children) * 100), 1) if children else 100.0

        children_summary = {
            'total': len(children),
            'done': children_done,
            'failed': children_failed,
            'in_progress': children_in_progress,
            'pending': children_pending_count
        }

        # List of child IDs that aren't complete (for iteration)
        pending_children = [
            task_id for task_id, status in zip(children, child_statuses)
            if status not in ('done', 'failed')
        ]

        # Commit all changes
        conn.commit()

        # Get final task state
        row = conn.execute(
            "SELECT * FROM tasks WHERE task_id=?", (target_task_id,)
        ).fetchone()
        conn.close()

        # Build audit entry for audit block
        if operation.startswith('create'):
            audit_desc = f"Created {'root' if operation == 'create_root' else 'sub-'}task: {task_input or target_task_id}"
            action = operation
        elif changes:
            audit_desc = f"Updated: {list(changes.keys())}"
            action = 'task_completed' if status_input == 'done' else 'task_updated'
        else:
            audit_desc = 'Task accessed'
            action = 'task_accessed'

        audit_entry = {
            'timestamp': now,
            'task_id': target_task_id,
            'caller': caller,
            'action': action,
            'description': audit_desc
        }
        if changes:
            audit_entry['changes'] = changes
        if parent_id_input:
            audit_entry['parent_id'] = parent_id_input

        # Output (matches current interface for callers)
        print(json.dumps({
            'task_id': target_task_id,
            'root_task_id': root_task_id or target_task_id,
            'status': row['status'] if row else (status_input or 'in-progress'),
            'parent_id': row['parent_id'] if row else parent_id_input,
            'children': children,
            'pending_children': pending_children,
            'data': json.loads(row['data'] or '{}') if row else merged_data,
            'all_children_done': all_children_done,
            'any_child_failed': any_child_failed,
            'progress_pct': progress_pct,
            'children_summary': children_summary,
            # Internal field for audit block
            '_audit_entry': audit_entry,
            '_operation': operation
        }))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        TASK_INPUT: "{{inputs.task}}"
        TASK_ID_INPUT: "{{inputs.task_id}}"
        PARENT_ID_INPUT: "{{inputs.parent_id}}"
        TASK_TYPE_INPUT: "{{inputs.task_type}}"
        STATUS_INPUT: "{{inputs.status}}"
        DATA_INPUT: "{{inputs.data | tojson}}"
        CALLER: "{{inputs.caller}}"
        NEW_TASK_ID: "{{blocks.new_task_id.outputs.stdout if blocks.new_task_id.succeeded else ''}}"

  # ==========================================================================
  # TASK OPS - STEP 4: Write audit entry to SQLite
  # ==========================================================================
  - id: write_audit
    type: Shell
    description: Insert audit entry into SQLite audit table.
    condition: "{{inputs.op == 'task' and inputs.audit}}"
    depends_on: [build_task, paths]
    inputs:
      command: |
        python3 << 'EOF'
        import os, json, sqlite3

        db_path = os.environ['DB_PATH']
        entry = json.loads(os.environ['AUDIT_ENTRY'])

        conn = sqlite3.connect(db_path)
        conn.execute("PRAGMA busy_timeout=30000")

        # Insert audit entry
        conn.execute('''
            INSERT INTO audit (timestamp, task_id, caller, action, description, changes, parent_id)
            VALUES (?, ?, ?, ?, ?, ?, ?)
        ''', (
            entry.get('timestamp'),
            entry.get('task_id'),
            entry.get('caller'),
            entry.get('action'),
            entry.get('description'),
            json.dumps(entry.get('changes')) if entry.get('changes') else None,
            entry.get('parent_id')
        ))
        conn.commit()

        # Get total audit entries
        count = conn.execute("SELECT COUNT(*) FROM audit").fetchone()[0]
        conn.close()

        print(json.dumps({'db_path': db_path, 'total_entries': count}))
        EOF
      env:
        DB_PATH: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"
        AUDIT_ENTRY: "{{(blocks.build_task.outputs.stdout | fromjson)._audit_entry | tojson}}"

outputs:
  # ==========================================================================
  # Common outputs (always available)
  # ==========================================================================
  state:
    description: Path to the SQLite database (entrypoint for all operations)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).db_path}}"

  trace_id:
    description: Execution trace ID (database file identifier)
    type: str
    value: "{{(blocks.paths.outputs.stdout | fromjson).trace_id}}"

  # ==========================================================================
  # Task outputs (populated when op=task)
  # ==========================================================================
  task:
    description: |
      Task operation results (when op=task). Contains:
      - task_id: ID of created/updated task
      - root_task_id: ID of root task in state
      - status: Current task status
      - parent_id: Parent task ID (null for root)
      - children: List of child task IDs
      - pending_children: Children not done/failed
      - data: Task-specific data field
      - all_children_done: True if all children done
      - any_child_failed: True if any child failed
      - progress_pct: Percentage of children completed
      - children_summary: Counts by status
    type: dict
    value: "{{(blocks.build_task.outputs.stdout | fromjson) if blocks.build_task.succeeded else {}}}"

  # ==========================================================================
  # Iteration outputs (populated when op=iteration)
  # ==========================================================================
  iteration:
    description: |
      Iteration operation results (when op=iteration). Contains:
      - current: Current iteration index (0-based)
      - total: Expected total iterations
      - cap: Maximum allowed (safety limit)
      - progress_pct: Percentage complete (0-100)
      - progress_str: Human-readable (e.g., "5/10 (50%)")
      - rate: Iterations per second
      - eta_seconds: Estimated seconds remaining
      - is_capped: True if safety limit reached
      - checkpoint: Last checkpoint data
    type: dict
    value: "{{blocks.handle_iteration.outputs if blocks.handle_iteration.succeeded else {}}}"

  # ==========================================================================
  # Memory outputs (populated when op=memory)
  # ==========================================================================
  memory:
    description: |
      Memory operation results (when op=memory). Contains:
      - value: Retrieved/resulting value
      - exists: Whether key existed before operation
      - previous: Previous value (set/delete ops)
      - keys: List of matching keys (keys op)
    type: dict
    value: "{{blocks.handle_memory.outputs if blocks.handle_memory.succeeded else {}}}"
