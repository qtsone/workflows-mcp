name: investigation-synthesize
description: |
  Synthesize all findings from a recursive investigation.

  This workflow runs only at the root level (depth=0) after all sub-investigations
  complete. It retrieves all accumulated findings from memory, merges them,
  removes duplicates, assesses overall risk, and makes an approval recommendation.

  Key responsibilities:
  1. Retrieve all investigation findings from memory tree
  2. Deduplicate and merge findings from all depths
  3. Assess overall risk level based on combined findings
  4. Generate approval recommendation
  5. Create consolidated report structure

tags: [agent, investigation, synthesis, pr-review]

inputs:
  state:
    type: str
    description: Path to state file containing memory with findings.
    required: true

  parent_task_id:
    type: str
    description: Parent task ID for creating sub-task in state tree.
    required: false
    default: ""

  context:
    type: dict
    description: |
      Original investigation context:
      - repo_path: local repository path
      - diff: PR diff content
      - focus: review focus area
    required: false
    default: {}

  root_findings:
    type: list
    description: Direct findings from root investigation level.
    required: false
    default: []

  focus:
    type: str
    description: |
      Area to prioritize in synthesis.
      Options: security, performance, architecture, tests, general
    required: false
    default: "general"

  threshold:
    type: str
    description: |
      Minimum severity level to include in the report.
      Values: critical, high, medium, low, info
      Findings below this threshold will be filtered out.
    required: false
    default: "low"

blocks:
  # ==========================================================================
  # STEP 1: Register synthesis task
  # ==========================================================================
  - id: register_task
    type: Workflow
    description: Create task in state tree for synthesis phase.
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ inputs.state }}"
        parent_id: "{{ inputs.parent_task_id }}"
        task: "Phase: Synthesis"
        task_type: "synthesis"
        status: "in-progress"
        caller: "investigation-synthesize"

  # ==========================================================================
  # STEP 2: Retrieve all findings from memory
  # ==========================================================================
  - id: retrieve_findings
    type: Workflow
    description: Get all accumulated investigation findings from memory.
    depends_on: [register_task]
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ inputs.state }}"
        op: memory
        memory_op: get
        memory_key: "investigations"
        memory_default: "[]"
        caller: "investigation-synthesize"

  # ==========================================================================
  # STEP 3: Merge and deduplicate findings
  # ==========================================================================
  - id: merge_findings
    type: Shell
    description: Merge all findings from all investigation depths, removing duplicates.
    depends_on: [retrieve_findings]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        from collections import defaultdict

        # Load all investigation records from memory
        investigations_json = os.environ.get('INVESTIGATIONS', '[]')
        root_findings_json = os.environ.get('ROOT_FINDINGS', '[]')

        try:
            investigations = json.loads(investigations_json) if investigations_json else []
        except json.JSONDecodeError:
            investigations = []

        try:
            root_findings = json.loads(root_findings_json) if root_findings_json else []
        except json.JSONDecodeError:
            root_findings = []

        # Collect all findings from all investigations
        all_findings = []
        files_analyzed = set()
        total_confidence = 0
        depth_counts = defaultdict(int)

        for inv in investigations:
            depth = inv.get('depth', 0)
            depth_counts[depth] += 1

            # Add findings with source info
            for finding in inv.get('findings', []):
                finding_with_source = {
                    **finding,
                    'source_depth': depth,
                    'source_task_id': inv.get('task_id', 'unknown')
                }
                all_findings.append(finding_with_source)

            # Track analyzed files
            for target in inv.get('targets', []):
                if isinstance(target, str):
                    files_analyzed.add(target)

            total_confidence += inv.get('confidence', 0)

        # Also include root findings if not already in memory
        for finding in root_findings:
            if finding not in [f for f in all_findings if 'source_depth' not in f or f.get('source_depth') == 0]:
                finding_with_source = {
                    **finding,
                    'source_depth': 0,
                    'source_task_id': 'root'
                }
                all_findings.append(finding_with_source)

        # Deduplicate findings by (file_path, line_number, title)
        seen = set()
        unique_findings = []
        for finding in all_findings:
            key = (
                finding.get('file_path', ''),
                finding.get('line_number', 0),
                finding.get('title', finding.get('description', '')[:50])
            )
            if key not in seen:
                seen.add(key)
                unique_findings.append(finding)

        # Count by severity
        severity_counts = defaultdict(int)
        for f in unique_findings:
            severity_counts[f.get('severity', 'info')] += 1

        # Determine overall risk level
        if severity_counts['critical'] > 0:
            risk_level = 'critical'
        elif severity_counts['high'] > 0:
            risk_level = 'high'
        elif severity_counts['medium'] > 2:
            risk_level = 'high'
        elif severity_counts['medium'] > 0:
            risk_level = 'medium'
        elif severity_counts['low'] > 0:
            risk_level = 'low'
        else:
            risk_level = 'info'

        # Calculate average confidence
        avg_confidence = (total_confidence / len(investigations)) if investigations else 0.5

        # Determine approval recommendation
        approve = risk_level in ('low', 'info') and severity_counts['critical'] == 0 and severity_counts['high'] == 0

        result = {
            'findings': unique_findings,
            'findings_count': len(unique_findings),
            'duplicates_removed': len(all_findings) - len(unique_findings),
            'severity_counts': dict(severity_counts),
            'risk_level': risk_level,
            'approve': approve,
            'files_analyzed': list(files_analyzed),
            'files_count': len(files_analyzed),
            'investigations_count': len(investigations),
            'depth_distribution': dict(depth_counts),
            'average_confidence': round(avg_confidence, 2)
        }

        print(json.dumps(result))
        EOF
      env:
        INVESTIGATIONS: "{{ blocks.retrieve_findings.outputs.memory.value | default([]) | tojson }}"
        ROOT_FINDINGS: "{{ inputs.root_findings | tojson }}"

  # ==========================================================================
  # STEP 4: Filter findings by threshold
  # ==========================================================================
  - id: filter_by_threshold
    type: Shell
    description: Filter findings to only include those at or above the severity threshold.
    depends_on: [merge_findings]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        # Severity levels in order (highest to lowest)
        SEVERITY_ORDER = ['critical', 'high', 'medium', 'low', 'info']

        merged_json = os.environ.get('MERGED_FINDINGS', '{}')
        threshold = os.environ.get('THRESHOLD', 'low').lower()

        try:
            merged = json.loads(merged_json)
        except json.JSONDecodeError:
            merged = {'findings': [], 'severity_counts': {}}

        # Get threshold index (findings at this level or higher are included)
        try:
            threshold_idx = SEVERITY_ORDER.index(threshold)
        except ValueError:
            threshold_idx = 3  # Default to 'low'

        # Filter findings
        all_findings = merged.get('findings', [])
        filtered_findings = []
        filtered_out = []

        for finding in all_findings:
            severity = finding.get('severity', 'info').lower()
            try:
                severity_idx = SEVERITY_ORDER.index(severity)
            except ValueError:
                severity_idx = 4  # Default to 'info'

            if severity_idx <= threshold_idx:
                filtered_findings.append(finding)
            else:
                filtered_out.append(finding)

        # Recount severities after filtering
        from collections import defaultdict
        severity_counts = defaultdict(int)
        for f in filtered_findings:
            severity_counts[f.get('severity', 'info')] += 1

        result = {
            **merged,
            'findings': filtered_findings,
            'findings_count': len(filtered_findings),
            'filtered_out_count': len(filtered_out),
            'severity_counts': dict(severity_counts),
            'threshold_applied': threshold
        }

        print(json.dumps(result))
        EOF
      env:
        MERGED_FINDINGS: "{{ blocks.merge_findings.outputs.stdout }}"
        THRESHOLD: "{{ inputs.threshold }}"

  # ==========================================================================
  # STEP 5: LLM synthesis for final assessment
  # ==========================================================================
  - id: llm_synthesis
    type: LLMCall
    description: |
      Final LLM synthesis to create coherent report from all findings.
      Provides rationale for approval decision and prioritizes issues.
    depends_on: [filter_by_threshold]
    inputs:
      profile: default
      prompt: |
        # Investigation Synthesis

        You are synthesizing findings from a recursive code investigation.
        Multiple investigation depths have analyzed the code, and now you need
        to create a coherent final assessment.

        ## Investigation Statistics
        - **Total Investigations:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).investigations_count }}
        - **Files Analyzed:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count }}
        - **Unique Findings:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count }}
        - **Filtered Out (below threshold):** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count }}
        - **Severity Threshold:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).threshold_applied }}
        - **Depth Distribution:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).depth_distribution }}
        - **Average Confidence:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).average_confidence }}

        ## Severity Distribution (after threshold filtering)
        {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).severity_counts | tojson }}

        ## Preliminary Assessment
        - **Risk Level:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).risk_level }}
        - **Approve Recommendation:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).approve }}

        {% if inputs.focus and inputs.focus != 'general' %}
        ## Focus Area: {{ inputs.focus }}
        {% elif inputs.context.focus %}
        ## Focus Area: {{ inputs.context.focus }}
        {% endif %}

        ## All Findings (filtered by threshold: {{ inputs.threshold }})
        {% for finding in (blocks.filter_by_threshold.outputs.stdout | fromjson).findings %}
        ### {{ loop.index }}. [{{ finding.severity | upper }}] {{ finding.title }}
        - **Type:** {{ finding.type }}
        - **File:** {{ finding.file_path | default('N/A') }}{% if finding.line_number %} (line {{ finding.line_number }}){% endif %}
        - **Description:** {{ finding.description }}
        {% if finding.suggestion %}
        - **Suggestion:** {{ finding.suggestion }}
        {% endif %}
        - **Source Depth:** {{ finding.source_depth }}
        {% endfor %}

        ## Your Task

        Create a final synthesis that:
        1. **Executive Summary**: Brief overview of the investigation results
        2. **Top Issues**: Prioritize the most important findings (max 5)
        3. **Risk Assessment**: Confirm or adjust the risk level with rationale
        4. **Approval Decision**: Confirm or adjust with detailed reasoning
        5. **Action Items**: What must be addressed before approval (if any)

      response_schema:
        type: object
        properties:
          executive_summary:
            type: string
            description: "Brief overview of investigation results"
          top_issues:
            type: array
            items:
              type: object
              properties:
                title:
                  type: string
                severity:
                  type: string
                  enum: [critical, high, medium, low, info]
                description:
                  type: string
                action_required:
                  type: boolean
              required: [title, severity, description]
            description: "Top 5 prioritized issues"
          risk_level:
            type: string
            enum: [critical, high, medium, low, info]
            description: "Final assessed risk level"
          risk_rationale:
            type: string
            description: "Reasoning for risk level"
          approve:
            type: boolean
            description: "Whether to approve the PR"
          approval_rationale:
            type: string
            description: "Reasoning for approval decision"
          action_items:
            type: array
            items:
              type: string
            description: "Required actions before approval"
          acknowledgments:
            type: array
            items:
              type: string
            description: "Positive aspects worth noting"
        required: [executive_summary, top_issues, risk_level, approve, approval_rationale]

  # ==========================================================================
  # STEP 6: Store synthesis results in memory
  # ==========================================================================
  - id: store_synthesis
    type: Workflow
    description: Store final synthesis results in memory.
    depends_on: [llm_synthesis, filter_by_threshold]
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ inputs.state }}"
        op: memory
        memory_op: set
        memory_key: "synthesis"
        memory_value: |
          {
            "executive_summary": {{ blocks.llm_synthesis.outputs.response.executive_summary | tojson }},
            "risk_level": "{{ blocks.llm_synthesis.outputs.response.risk_level }}",
            "approve": {{ blocks.llm_synthesis.outputs.response.approve | tojson }},
            "top_issues": {{ blocks.llm_synthesis.outputs.response.top_issues | tojson }},
            "action_items": {{ blocks.llm_synthesis.outputs.response.action_items | default([]) | tojson }},
            "findings_count": {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count }},
            "filtered_out_count": {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count }},
            "files_analyzed": {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count }},
            "threshold": "{{ inputs.threshold }}"
          }
        caller: "investigation-synthesize"

  # ==========================================================================
  # STEP 7: Mark synthesis complete
  # ==========================================================================
  - id: track_done
    type: Workflow
    description: Mark synthesis task as complete.
    depends_on:
      - llm_synthesis
      - store_synthesis
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ inputs.state }}"
        task_id: "{{ blocks.register_task.outputs.task.task_id }}"
        status: "done"
        caller: "investigation-synthesize"
        data:
          risk_level: "{{ blocks.llm_synthesis.outputs.response.risk_level }}"
          approve: "{{ blocks.llm_synthesis.outputs.response.approve }}"
          findings_count: "{{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count }}"
          threshold: "{{ inputs.threshold }}"

outputs:
  executive_summary:
    value: "{{ blocks.llm_synthesis.outputs.response.executive_summary }}"
    type: str
    description: "Brief overview of investigation results."

  risk_level:
    value: "{{ blocks.llm_synthesis.outputs.response.risk_level }}"
    type: str
    description: "Final assessed risk level (critical/high/medium/low/info)."

  approve:
    value: "{{ blocks.llm_synthesis.outputs.response.approve }}"
    type: bool
    description: "Whether the PR should be approved."

  approval_rationale:
    value: "{{ blocks.llm_synthesis.outputs.response.approval_rationale }}"
    type: str
    description: "Reasoning for approval decision."

  top_issues:
    value: "{{ blocks.llm_synthesis.outputs.response.top_issues }}"
    type: list
    description: "Top prioritized issues from the investigation."

  action_items:
    value: "{{ blocks.llm_synthesis.outputs.response.action_items | default([]) }}"
    type: list
    description: "Required actions before approval."

  all_findings:
    value: "{{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings }}"
    type: list
    description: "Complete list of all unique findings (after threshold filtering)."

  filtered_out_count:
    value: "{{ (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count }}"
    type: num
    description: "Number of findings filtered out due to threshold."

  statistics:
    value: |
      {{
        {
          'findings_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count,
          'filtered_out_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count,
          'files_analyzed': (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count,
          'investigations_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).investigations_count,
          'severity_counts': (blocks.filter_by_threshold.outputs.stdout | fromjson).severity_counts,
          'depth_distribution': (blocks.filter_by_threshold.outputs.stdout | fromjson).depth_distribution,
          'threshold_applied': (blocks.filter_by_threshold.outputs.stdout | fromjson).threshold_applied
        }
      }}
    type: dict
    description: "Statistics about the investigation."
