name: investigation-synthesize
description: |
  Synthesize all findings from a recursive investigation.

  This workflow runs only at the root level (depth=0) after all sub-investigations
  complete. It retrieves all accumulated findings from memory, merges them,
  removes duplicates, assesses overall risk, and makes an approval recommendation.

  Key responsibilities:
  1. Retrieve all investigation findings from memory tree
  2. Deduplicate and merge findings from all depths
  3. Assess overall risk level based on combined findings
  4. Generate approval recommendation
  5. Create consolidated report structure

tags: [agent, investigation, synthesis, pr-review]

inputs:
  state:
    type: str
    description: Path to state file containing memory with findings.
    required: true

  parent_task_id:
    type: str
    description: Parent task ID for creating sub-task in state tree.
    required: false
    default: ""

  context:
    type: dict
    description: |
      Original investigation context:
      - repo_path: local repository path
      - diff: PR diff content
      - focus: review focus area
    required: false
    default: {}

  root_findings:
    type: list
    description: Direct findings from root investigation level.
    required: false
    default: []

  focus:
    type: str
    description: |
      Area to prioritize in synthesis.
      Options: security, performance, architecture, tests, general
    required: false
    default: "general"

  threshold:
    type: str
    description: |
      Minimum severity level to include in the report.
      Values: critical, high, medium, low, info
      Findings below this threshold will be filtered out.
    required: false
    default: "low"

blocks:
  # ==========================================================================
  # STEP 0: Generate Task ID
  # ==========================================================================
  - id: gen_task_id
    type: Shell
    description: Generate UUID for this task.
    inputs:
      command: python3 -c "import uuid; print(str(uuid.uuid4())[:8])"

  # ==========================================================================
  # STEP 1: Register synthesis task (direct Sql)
  # ==========================================================================
  - id: register_task
    type: Sql
    description: Create task in state tree for synthesis phase.
    depends_on: [gen_task_id]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        INSERT INTO tasks (id, parent_id, kind, name, metadata, inputs, status, depth)
        VALUES (?, ?, 'synthesis', 'Phase: Synthesis', '{}', '{}', 'running', 0)
        RETURNING id, parent_id, kind, name, status, depth, created_at
      params:
        - "task-{{ blocks.gen_task_id.outputs.stdout | trim }}"
        - "{{ inputs.parent_task_id if inputs.parent_task_id else none }}"

  # ==========================================================================
  # STEP 2: Retrieve all findings from memory (direct Sql)
  # ==========================================================================
  - id: retrieve_findings
    type: Sql
    description: Get all accumulated investigation findings from memory.
    depends_on: [register_task]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        SELECT key, value, metadata
        FROM memory
        WHERE namespace = 'investigations'
        ORDER BY key

  # ==========================================================================
  # STEP 3: Merge and deduplicate findings
  # ==========================================================================
  - id: merge_findings
    type: Shell
    description: Merge all findings from all investigation depths, removing duplicates.
    depends_on: [retrieve_findings]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        from collections import defaultdict

        # Load all investigation records from memory
        investigations_json = os.environ.get('INVESTIGATIONS', '[]')
        root_findings_json = os.environ.get('ROOT_FINDINGS', '[]')

        try:
            investigations = json.loads(investigations_json) if investigations_json else []
        except json.JSONDecodeError:
            investigations = []

        try:
            root_findings = json.loads(root_findings_json) if root_findings_json else []
        except json.JSONDecodeError:
            root_findings = []

        # ==========================================================================
        # BUILD FILE READ MAP: Track which files were read at which modes
        # This enables evidence validation for findings
        # ==========================================================================
        file_read_map = defaultdict(lambda: {'outline': [], 'full': [], 'summary': []})

        for inv in investigations:
            mode = inv.get('mode', 'outline')
            depth = inv.get('depth', 0)
            for target in inv.get('targets', []):
                if isinstance(target, str):
                    file_read_map[target][mode].append(depth)

        # Collect all findings from all investigations
        all_findings = []
        files_analyzed = set()
        total_confidence = 0
        depth_counts = defaultdict(int)

        for inv in investigations:
            depth = inv.get('depth', 0)
            depth_counts[depth] += 1

            # Add findings with source info
            for finding in inv.get('findings', []):
                finding_with_source = {
                    **finding,
                    'source_depth': depth,
                    'source_task_id': inv.get('task_id', 'unknown')
                }
                all_findings.append(finding_with_source)

            # Track analyzed files
            for target in inv.get('targets', []):
                if isinstance(target, str):
                    files_analyzed.add(target)

            total_confidence += inv.get('confidence', 0)

        # Also include root findings if not already in memory
        for finding in root_findings:
            if finding not in [f for f in all_findings if 'source_depth' not in f or f.get('source_depth') == 0]:
                finding_with_source = {
                    **finding,
                    'source_depth': 0,
                    'source_task_id': 'root'
                }
                all_findings.append(finding_with_source)

        # Deduplicate findings by (file_path, line_number, title)
        seen = set()
        unique_findings = []
        for finding in all_findings:
            key = (
                finding.get('file_path', ''),
                finding.get('line_number', 0),
                finding.get('title', finding.get('description', '')[:50])
            )
            if key not in seen:
                seen.add(key)
                unique_findings.append(finding)

        # ==========================================================================
        # EVIDENCE VALIDATION: Check findings against file read history
        # Findings requiring content-level evidence must have full-mode reads
        # ==========================================================================
        CONTENT_REQUIRED_EVIDENCE = ['content', 'cross_reference']
        validated_findings = []
        validation_warnings = []

        for finding in unique_findings:
            file_path = finding.get('file_path', '')
            evidence_type = finding.get('evidence_type', 'inferred')
            verified = finding.get('verified', True)  # Default to True for legacy findings

            # Check if finding claims content-level evidence
            needs_content_evidence = evidence_type in CONTENT_REQUIRED_EVIDENCE

            # Check if we have full-mode reads for this file
            file_reads = file_read_map.get(file_path, {'outline': [], 'full': [], 'summary': []})
            has_full_read = len(file_reads['full']) > 0

            if needs_content_evidence and not has_full_read and file_path:
                # Finding claims content evidence but file was never read in full mode
                finding['_validation_warning'] = f"Claims content evidence but {file_path} was only read in outline mode"
                finding['_evidence_valid'] = False
                validation_warnings.append({
                    'finding_title': finding.get('title', 'Unknown'),
                    'file_path': file_path,
                    'claimed_evidence': evidence_type,
                    'available_modes': list(k for k, v in file_reads.items() if v)
                })
            elif not verified and not has_full_read and file_path:
                # Finding marked as unverified and no full read available
                finding['_validation_warning'] = f"Unverified finding - {file_path} was not read in full mode"
                finding['_evidence_valid'] = False
            else:
                finding['_evidence_valid'] = True

            validated_findings.append(finding)

        # Count validated vs unvalidated
        evidence_valid_count = sum(1 for f in validated_findings if f.get('_evidence_valid', True))
        evidence_invalid_count = len(validated_findings) - evidence_valid_count

        unique_findings = validated_findings

        # Count by severity
        severity_counts = defaultdict(int)
        for f in unique_findings:
            severity_counts[f.get('severity', 'info')] += 1

        # Determine overall risk level
        if severity_counts['critical'] > 0:
            risk_level = 'critical'
        elif severity_counts['high'] > 0:
            risk_level = 'high'
        elif severity_counts['medium'] > 2:
            risk_level = 'high'
        elif severity_counts['medium'] > 0:
            risk_level = 'medium'
        elif severity_counts['low'] > 0:
            risk_level = 'low'
        else:
            risk_level = 'info'

        # Calculate average confidence
        avg_confidence = (total_confidence / len(investigations)) if investigations else 0.5

        # Determine approval recommendation
        approve = risk_level in ('low', 'info') and severity_counts['critical'] == 0 and severity_counts['high'] == 0

        result = {
            'findings': unique_findings,
            'findings_count': len(unique_findings),
            'duplicates_removed': len(all_findings) - len(unique_findings),
            'severity_counts': dict(severity_counts),
            'risk_level': risk_level,
            'approve': approve,
            'files_analyzed': list(files_analyzed),
            'files_count': len(files_analyzed),
            'investigations_count': len(investigations),
            'depth_distribution': dict(depth_counts),
            'average_confidence': round(avg_confidence, 2),
            # Evidence validation results
            'evidence_valid_count': evidence_valid_count,
            'evidence_invalid_count': evidence_invalid_count,
            'validation_warnings': validation_warnings,
            'file_read_map': {k: {m: v for m, v in modes.items() if v} for k, modes in file_read_map.items()}
        }

        print(json.dumps(result))
        EOF
      env:
        INVESTIGATIONS: "{{ blocks.retrieve_findings.outputs.rows | map(attribute='value') | list | tojson }}"
        ROOT_FINDINGS: "{{ inputs.root_findings | tojson }}"

  # ==========================================================================
  # STEP 4: Filter findings by threshold
  # ==========================================================================
  - id: filter_by_threshold
    type: Shell
    description: Filter findings to only include those at or above the severity threshold.
    depends_on: [merge_findings]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        # Severity levels in order (highest to lowest)
        SEVERITY_ORDER = ['critical', 'high', 'medium', 'low', 'info']

        merged_json = os.environ.get('MERGED_FINDINGS', '{}')
        threshold = os.environ.get('THRESHOLD', 'low').lower()

        try:
            merged = json.loads(merged_json)
        except json.JSONDecodeError:
            merged = {'findings': [], 'severity_counts': {}}

        # Get threshold index (findings at this level or higher are included)
        try:
            threshold_idx = SEVERITY_ORDER.index(threshold)
        except ValueError:
            threshold_idx = 3  # Default to 'low'

        # Filter findings
        all_findings = merged.get('findings', [])
        filtered_findings = []
        filtered_out = []

        for finding in all_findings:
            severity = finding.get('severity', 'info').lower()
            try:
                severity_idx = SEVERITY_ORDER.index(severity)
            except ValueError:
                severity_idx = 4  # Default to 'info'

            if severity_idx <= threshold_idx:
                filtered_findings.append(finding)
            else:
                filtered_out.append(finding)

        # Recount severities after filtering
        from collections import defaultdict
        severity_counts = defaultdict(int)
        for f in filtered_findings:
            severity_counts[f.get('severity', 'info')] += 1

        result = {
            **merged,
            'findings': filtered_findings,
            'findings_count': len(filtered_findings),
            'filtered_out_count': len(filtered_out),
            'severity_counts': dict(severity_counts),
            'threshold_applied': threshold
        }

        print(json.dumps(result))
        EOF
      env:
        MERGED_FINDINGS: "{{ blocks.merge_findings.outputs.stdout }}"
        THRESHOLD: "{{ inputs.threshold }}"

  # ==========================================================================
  # STEP 5: LLM synthesis for final assessment
  # ==========================================================================
  - id: llm_synthesis
    type: LLMCall
    description: |
      Final LLM synthesis to create coherent report from all findings.
      Provides rationale for approval decision and prioritizes issues.
    depends_on: [filter_by_threshold]
    inputs:
      profile: default
      timeout: 600
      prompt: |
        # Investigation Synthesis

        You are synthesizing findings from a recursive code investigation.
        Multiple investigation depths have analyzed the code, and now you need
        to create a coherent final assessment.

        ## Investigation Statistics
        - **Total Investigations:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).investigations_count }}
        - **Files Analyzed:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count }}
        - **Unique Findings:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count }}
        - **Filtered Out (below threshold):** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count }}
        - **Severity Threshold:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).threshold_applied }}
        - **Depth Distribution:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).depth_distribution }}
        - **Average Confidence:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).average_confidence }}

        ## Evidence Validation
        - **Findings with valid evidence:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).evidence_valid_count }}
        - **Findings with insufficient evidence:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).evidence_invalid_count }}
        {% if (blocks.filter_by_threshold.outputs.stdout | fromjson).validation_warnings | length > 0 %}
        ### Validation Warnings
        The following findings may be based on insufficient evidence:
        {% for warning in (blocks.filter_by_threshold.outputs.stdout | fromjson).validation_warnings %}
        - **{{ warning.finding_title }}** ({{ warning.file_path }}): Claims {{ warning.claimed_evidence }} evidence but only {{ warning.available_modes | join(', ') }} reads available
        {% endfor %}

        **IMPORTANT:** Findings with `_evidence_valid: false` should be treated with skepticism.
        They may claim issues that cannot be verified from the available evidence.
        Consider excluding or downgrading such findings in your final assessment.
        {% endif %}

        ## Severity Distribution (after threshold filtering)
        {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).severity_counts | tojson }}

        ## Preliminary Assessment
        - **Risk Level:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).risk_level }}
        - **Approve Recommendation:** {{ (blocks.filter_by_threshold.outputs.stdout | fromjson).approve }}

        {% if inputs.focus and inputs.focus != 'general' %}
        ## Focus Area: {{ inputs.focus }}
        {% elif inputs.context.focus %}
        ## Focus Area: {{ inputs.context.focus }}
        {% endif %}

        ## All Findings (filtered by threshold: {{ inputs.threshold }})
        {% for finding in (blocks.filter_by_threshold.outputs.stdout | fromjson).findings %}
        ### {{ loop.index }}. [{{ finding.severity | upper }}] {{ finding.title }}{% if finding._evidence_valid == false %} ⚠️ UNVERIFIED{% endif %}

        - **Type:** {{ finding.type }}
        - **File:** {{ finding.file_path | default('N/A') }}{% if finding.line_number %} (line {{ finding.line_number }}){% endif %}
        - **Description:** {{ finding.description }}
        - **Evidence:** {{ finding.evidence_type | default('legacy') }}{% if finding.verified == false %} (unverified){% endif %}
        {% if finding._validation_warning %}
        - **⚠️ Validation Warning:** {{ finding._validation_warning }}
        {% endif %}
        {% if finding.suggestion %}
        - **Suggestion:** {{ finding.suggestion }}
        {% endif %}
        - **Source Depth:** {{ finding.source_depth }}
        {% endfor %}

        ## Your Task

        Create a final synthesis that:
        1. **Executive Summary**: Brief overview of the investigation results
        2. **Top Issues**: Prioritize the most important **verified** findings (max 5)
           - Exclude or heavily discount findings marked as UNVERIFIED
           - Findings with `_evidence_valid: false` should NOT be in top issues
        3. **Risk Assessment**: Confirm or adjust the risk level with rationale
           - Base risk level on VERIFIED findings only
        4. **Approval Decision**: Confirm or adjust with detailed reasoning
           - Do not block approval based solely on unverified findings
        5. **Action Items**: What must be addressed before approval (if any)

      response_schema:
        type: object
        properties:
          executive_summary:
            type: string
            description: "Brief overview of investigation results"
          top_issues:
            type: array
            items:
              type: object
              properties:
                title:
                  type: string
                severity:
                  type: string
                  enum: [critical, high, medium, low, info]
                description:
                  type: string
                action_required:
                  type: boolean
              required: [title, severity, description]
            description: "Top 5 prioritized issues"
          risk_level:
            type: string
            enum: [critical, high, medium, low, info]
            description: "Final assessed risk level"
          risk_rationale:
            type: string
            description: "Reasoning for risk level"
          approve:
            type: boolean
            description: "Whether to approve the PR"
          approval_rationale:
            type: string
            description: "Reasoning for approval decision"
          action_items:
            type: array
            items:
              type: string
            description: "Required actions before approval"
          acknowledgments:
            type: array
            items:
              type: string
            description: "Positive aspects worth noting"
        required:
          [
            executive_summary,
            top_issues,
            risk_level,
            approve,
            approval_rationale,
          ]

  # ==========================================================================
  # STEP 6: Store synthesis results in memory (direct Sql)
  # ==========================================================================
  - id: store_synthesis
    type: Sql
    description: Store final synthesis results in memory.
    depends_on: [llm_synthesis, filter_by_threshold]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        INSERT OR REPLACE INTO memory (namespace, key, value, metadata, task_id)
        VALUES ('synthesis', 'result', ?, ?, ?)
      params:
        - "{{ {'executive_summary': blocks.llm_synthesis.outputs.response.executive_summary, 'risk_level': blocks.llm_synthesis.outputs.response.risk_level, 'approve': blocks.llm_synthesis.outputs.response.approve, 'top_issues': blocks.llm_synthesis.outputs.response.top_issues, 'action_items': blocks.llm_synthesis.outputs.response.action_items | default([]), 'findings_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count, 'filtered_out_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count, 'files_analyzed': (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count, 'threshold': inputs.threshold} | tojson }}"
        - "{{ {'threshold': inputs.threshold} | tojson }}"
        - "{{ blocks.register_task.outputs.rows[0].id }}"

  # ==========================================================================
  # STEP 7: Mark synthesis complete (direct Sql)
  # ==========================================================================
  - id: track_done
    type: Sql
    description: Mark synthesis task as complete.
    depends_on:
      - llm_synthesis
      - store_synthesis
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        UPDATE tasks
        SET status = 'done',
            outputs = ?,
            updated_at = datetime('now')
        WHERE id = ?
      params:
        - "{{ {'risk_level': blocks.llm_synthesis.outputs.response.risk_level, 'approve': blocks.llm_synthesis.outputs.response.approve, 'findings_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count, 'threshold': inputs.threshold} | tojson }}"
        - "{{ blocks.register_task.outputs.rows[0].id }}"

outputs:
  executive_summary:
    value: "{{ blocks.llm_synthesis.outputs.response.executive_summary }}"
    type: str
    description: "Brief overview of investigation results."

  risk_level:
    value: "{{ blocks.llm_synthesis.outputs.response.risk_level }}"
    type: str
    description: "Final assessed risk level (critical/high/medium/low/info)."

  approve:
    value: "{{ blocks.llm_synthesis.outputs.response.approve }}"
    type: bool
    description: "Whether the PR should be approved."

  approval_rationale:
    value: "{{ blocks.llm_synthesis.outputs.response.approval_rationale }}"
    type: str
    description: "Reasoning for approval decision."

  top_issues:
    value: "{{ blocks.llm_synthesis.outputs.response.top_issues }}"
    type: list
    description: "Top prioritized issues from the investigation."

  action_items:
    value: "{{ blocks.llm_synthesis.outputs.response.action_items | default([]) }}"
    type: list
    description: "Required actions before approval."

  all_findings:
    value: "{{ (blocks.filter_by_threshold.outputs.stdout | fromjson).findings }}"
    type: list
    description: "Complete list of all unique findings (after threshold filtering)."

  filtered_out_count:
    value: "{{ (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count }}"
    type: num
    description: "Number of findings filtered out due to threshold."

  statistics:
    value: |
      {{
        {
          'findings_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).findings_count,
          'filtered_out_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).filtered_out_count,
          'files_analyzed': (blocks.filter_by_threshold.outputs.stdout | fromjson).files_count,
          'investigations_count': (blocks.filter_by_threshold.outputs.stdout | fromjson).investigations_count,
          'severity_counts': (blocks.filter_by_threshold.outputs.stdout | fromjson).severity_counts,
          'depth_distribution': (blocks.filter_by_threshold.outputs.stdout | fromjson).depth_distribution,
          'threshold_applied': (blocks.filter_by_threshold.outputs.stdout | fromjson).threshold_applied
        }
      }}
    type: dict
    description: "Statistics about the investigation."
