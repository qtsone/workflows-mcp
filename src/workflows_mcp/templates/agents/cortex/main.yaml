# =============================================================================
# CORTEX Cell - The Fundamental Cognitive Unit (v2)
# =============================================================================
#
# The cell is the spawnable cognitive unit that phases invoke when they
# need to investigate uncertainty. It handles:
#   1. Cycle detection (query hash checking)
#   2. Depth tracking (for cognitive spawning - investigations)
#   3. Iteration tracking (for task completion - ACT continuation)
#   4. Task registration (category: cortex-cell)
#   5. Phase orchestration (categorize → gather → reason → act)
#   6. Pipeline configuration flow with override merging
#   7. Synthesis storage in task.data for child querying
#
# Key v2 Changes:
#   - ALL phases always run (no skipping)
#   - Merged ANALYZE + SYNTHESIZE into REASON phase
#   - Category → pipeline_config via mapping table
#   - Config override: later phases can update config based on discoveries
#   - Fractal continuation: ACT can spawn continuation (same depth, iter+1)
#   - Two counters: depth (investigations) vs iterations (task completion)
#
# Each phase follows the universal fractal pattern and can spawn child cells
# when uncertain. The reason phase queries all child cells to merge their
# findings.
#
# =============================================================================

name: cortex-cell
description: |
  CORTEX Cell - the fundamental cognitive unit. Processes queries through
  the complete cognitive cycle: categorize → gather → reason → act.
  ALL phases always run. Can be spawned by any phase that encounters uncertainty.

tags: [agent, cortex, cell, fractal, recursive, v2]

inputs:
  prompt:
    type: str
    description: The detailed task description.
    required: true

  context:
    type: dict
    description: |
      Shared context:
      - repo_path: Working directory
      - parent_prompt: Query that spawned this cell
    default: {}

  depth:
    type: num
    description: Current cell depth (0 = root).
    default: 0

  max_depth:
    type: num
    description: Maximum investigation depth (cognitive spawning for sub-questions).
    default: 5

  iterations:
    type: num
    description: Current task completion attempt (ACT continuation counter).
    default: 0

  max_iterations:
    type: num
    description: Maximum continuation attempts for incomplete tasks.
    default: 3

  confidence_threshold:
    type: num
    description: Minimum confidence to proceed without investigation.
    default: 0.95

  state:
    type: str
    description: Path to SQLite state database. Created if empty.
    default: ""

  parent_id:
    type: str
    description: Parent task ID in the task tree.
    default: ""

  profile:
    type: str
    description: LLM profile to use.
    default: "default"

  # ===========================================================================
  # STATE MODELS - ActiveRecord-style table definitions
  # ===========================================================================
  models:
    type: dict
    description: |
      Table model definitions for state management.
      Defined once here, passed to all child phases.
    default:
      task:
        table: tasks
        columns:
          id: { type: text, primary: true, auto: uuid }
          parent_id: { type: text }
          kind: { type: text, required: true }
          name: { type: text, required: true }
          metadata: { type: json, default: "{}" }
          inputs: { type: json, default: "{}" }
          outputs: { type: json, default: "{}" }
          status: { type: text, default: "pending" }
          depth: { type: integer, default: 0 }
          iteration: { type: integer, default: 0 }
          created_at: { type: timestamp, auto: created }
          updated_at: { type: timestamp, auto: updated }
        indexes:
          - columns: [parent_id]
          - columns: [status]
          - columns: [kind]

      memory:
        table: memory
        columns:
          namespace: { type: text, required: true }
          key: { type: text, required: true }
          value: { type: json }
          metadata: { type: json, default: "{}" }
          task_id: { type: text }
          created_at: { type: timestamp, auto: created }
          updated_at: { type: timestamp, auto: updated }
        indexes:
          - columns: [namespace, key]
            unique: true
          - columns: [task_id]

      fact:
        table: facts
        columns:
          id: { type: text, primary: true, auto: uuid }
          claim: { type: text, required: true }
          evidence_type: { type: text, default: "inferred" }
          severity: { type: text, default: "info" }
          confidence: { type: real, required: true }
          grounding: { type: json, default: "[]" }
          scope: { type: text }
          task_id: { type: text, required: true }
          status: { type: text, default: "active" }
          created_at: { type: timestamp, auto: created }
          updated_at: { type: timestamp, auto: updated }
        indexes:
          - columns: [status]
          - columns: [task_id]
          - columns: [severity]

      # LLM Call Tracking - stores full prompts and responses for debugging
      llm_call:
        table: llm_calls
        columns:
          id: { type: text, primary: true, auto: uuid }
          task_id: { type: text, required: true }
          phase: { type: text, required: true }
          system_instructions: { type: text }
          prompt: { type: text, required: true }
          response: { type: json }
          model: { type: text }
          prompt_tokens: { type: integer }
          completion_tokens: { type: integer }
          duration_ms: { type: real }
          created_at: { type: timestamp, auto: created }
        indexes:
          - columns: [task_id]
          - columns: [phase]

      action:
        table: actions
        columns:
          id: { type: text, primary: true, auto: uuid }
          task_id: { type: text, required: true }
          capability: { type: text, required: true }
          inputs: { type: text, required: true }
          status: { type: text, default: "pending" }
          result: { type: json }
          created_at: { type: timestamp, auto: created }
          completed_at: { type: timestamp }
        indexes:
          - columns: [capability, inputs]
            unique: true
          - columns: [task_id]

      audit:
        table: audit
        columns:
          id: { type: integer, primary: true }
          timestamp: { type: timestamp, auto: created }
          task_id: { type: text }
          table_name: { type: text, required: true }
          operation: { type: text, required: true }
          old_data: { type: json }
          new_data: { type: json }
        indexes:
          - columns: [task_id]
          - columns: [timestamp]

      # Simple key-value metadata store for workflow-level data
      metadata:
        table: metadata
        columns:
          key: { type: text, primary: true }
          value: { type: text }

  capabilities:
    type: list
    description: |
      Available capabilities registry (flat list).
      Each capability has: name, description, inputs (JSON Schema), requires (permission).
      Capabilities are filtered based on permissions before being passed to phases.

      Input schema format (JSON Schema subset):
        type: string | integer | boolean | array | object
        items: { type: ... } for arrays
        enum: [value1, value2] for string enums
        description: Human-readable description
        required: true/false (default: false)
    default:
      # =========================================================================
      # GATHER CAPABILITIES (read permission)
      # =========================================================================
      - name: cortex-gather-files
        description: Read files matching glob patterns
        inputs:
          patterns:
            type: array
            items: { type: string }
            description: Glob patterns to match (e.g., ["src/**/*.py", "*.md"])
            required: true
          base_path:
            type: string
            description: Base directory to search from
            required: false
          mode:
            type: string
            enum: [outline, full, summary]
            description: Output mode - outline (structure), full (content), summary (brief)
            required: false
          max_files:
            type: integer
            description: Maximum number of files to read
            required: false
        requires: read

      - name: cortex-gather-search
        description: Search codebase with regex pattern
        inputs:
          pattern:
            type: string
            description: Regex pattern to search for
            required: true
          path:
            type: string
            description: Directory to search in
            required: false
          file_type:
            type: string
            description: File extension filter (e.g., py, js, yaml)
            required: false
          max_matches:
            type: integer
            description: Maximum number of results to return
            required: false
        requires: read

      # =========================================================================
      # ACTION CAPABILITIES (write/execute permissions)
      # =========================================================================
      - name: cortex-action-write
        description: Create or overwrite a file
        inputs:
          path:
            type: string
            description: File path to write
            required: true
          content:
            type: string
            description: Content to write to the file
            required: true
          overwrite:
            type: boolean
            description: Allow overwriting existing file (default false)
            required: false
        requires: write

      - name: cortex-action-edit
        description: Edit an existing file with multiple operation types
        inputs:
          path:
            type: string
            description: File path to edit
            required: true
          operations:
            type: array
            items: { type: object }
            description: |
              Edit operations list. Each operation object needs a "type" field:
              - replace_text: {type, old_text, new_text, count?}
              - replace_lines: {type, line_start, line_end, content}
              - insert_lines: {type, line_start, content}
              - delete_lines: {type, line_start, line_end}
              - regex_replace: {type, pattern, replacement, flags?}
            required: true
          backup:
            type: boolean
            description: Create .bak backup before editing
            required: false
          dry_run:
            type: boolean
            description: Preview changes without applying
            required: false
        requires: write

      - name: cortex-action-execute
        description: Execute a shell command
        inputs:
          command:
            type: string
            description: Shell command to execute
            required: true
          working_dir:
            type: string
            description: Working directory for command execution
            required: false
          timeout:
            type: integer
            description: Timeout in seconds
            required: false
        requires: execute

  permissions:
    type: dict
    description: |
      Permission flags controlling which capabilities are available.
      Capabilities are filtered based on these flags before phases receive them.
    default:
      read: true
      write: false
      execute: false

  prompt_history:
    type: list
    description: Hashes of ancestor queries (for cycle detection).
    default: []

blocks:
  - id: init
    description: Compute query hash, check for cycles, depth, and iteration limits.
    type: Shell
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import hashlib
        import os

        prompt = os.environ.get('PROMPT', '')
        depth = int(os.environ.get('DEPTH', '0'))
        max_depth = int(os.environ.get('MAX_DEPTH', '5'))
        iterations = int(os.environ.get('ITERATIONS', '0'))
        max_iterations = int(os.environ.get('MAX_ITERATIONS', '3'))
        history_json = os.environ.get('PROMPT_HISTORY', '[]')

        try:
            prompt_history = json.loads(history_json) if history_json else []
        except json.JSONDecodeError:
            prompt_history = []

        hash = hashlib.sha256(prompt.encode()).hexdigest()[:16]

        depth_exceeded = depth >= max_depth
        iterations_exceeded = iterations >= max_iterations
        cycle_detected = hash in prompt_history
        terminate = depth_exceeded or cycle_detected or iterations_exceeded

        new_history = prompt_history + [hash]

        if depth_exceeded:
            reason = 'depth_exceeded'
        elif iterations_exceeded:
            reason = 'iterations_exceeded'
        elif cycle_detected:
            reason = 'cycle_detected'
        else:
            reason = None

        # Compute state path
        state_input = os.environ.get('STATE', '')
        if state_input:
            state_path = state_input
        else:
            import os.path
            state_path = os.path.expanduser(f'~/.workflows/tasks/{hash}.db')

        result = {
            'hash': hash,
            'depth': depth,
            'iterations': iterations,
            'terminate': terminate,
            'reason': reason,
            'prompt_history': new_history,
            'state_path': state_path
        }

        print(json.dumps(result))
        EOF
      env:
        PROMPT: "{{ inputs.prompt }}"
        DEPTH: "{{ inputs.depth }}"
        MAX_DEPTH: "{{ inputs.max_depth }}"
        ITERATIONS: "{{ inputs.iterations }}"
        MAX_ITERATIONS: "{{ inputs.max_iterations }}"
        PROMPT_HISTORY: "{{ inputs.prompt_history | tojson }}"
        STATE: "{{ inputs.state }}"

  # ===========================================================================
  # Initialize Database Schema - Model-based table creation (only at root level)
  # ===========================================================================
  - id: init_tables
    type: Sql
    description: Create all tables from model definitions using op:schema.
    condition: "{{ inputs.state == '' }}"
    depends_on: [init]
    for_each: "{{ inputs.models | dictsort }}"
    for_each_mode: sequential
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      model: "{{ each.value[1] }}"
      op: schema

  # ===========================================================================
  # Initialize FTS5 and Triggers (only at root level)
  # ===========================================================================
  - id: init_db
    type: Sql
    description: Create FTS5 virtual table and triggers (not supported by model system).
    condition: "{{ inputs.state == '' }}"
    depends_on: [init_tables]
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      init_sql: |
        -- =========================================================================
        -- CORTEX FTS5 + TRIGGERS (model system doesn't support these)
        -- =========================================================================

        -- FACTS FTS5: Full-text search on claim
        CREATE VIRTUAL TABLE IF NOT EXISTS "facts_fts" USING fts5(
            id UNINDEXED,
            claim,
            content="facts",
            content_rowid="rowid"
        );

        -- FTS5 Sync Triggers
        CREATE TRIGGER IF NOT EXISTS facts_ai AFTER INSERT ON facts BEGIN
            INSERT INTO facts_fts(rowid, id, claim)
            VALUES (NEW.rowid, NEW.id, NEW.claim);
        END;
        CREATE TRIGGER IF NOT EXISTS facts_ad AFTER DELETE ON facts BEGIN
            INSERT INTO facts_fts(facts_fts, rowid, id, claim)
            VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
        END;
        CREATE TRIGGER IF NOT EXISTS facts_au AFTER UPDATE ON facts BEGIN
            INSERT INTO facts_fts(facts_fts, rowid, id, claim)
            VALUES('delete', OLD.rowid, OLD.id, OLD.claim);
            INSERT INTO facts_fts(rowid, id, claim)
            VALUES (NEW.rowid, NEW.id, NEW.claim);
        END;

        -- Audit Triggers for tasks table
        CREATE TRIGGER IF NOT EXISTS tasks_audit_insert AFTER INSERT ON tasks BEGIN
            INSERT INTO audit (task_id, table_name, operation, new_data)
            VALUES (NEW.id, 'tasks', 'INSERT', json_object(
                'id', NEW.id, 'kind', NEW.kind, 'name', NEW.name, 'status', NEW.status
            ));
        END;
        CREATE TRIGGER IF NOT EXISTS tasks_audit_update AFTER UPDATE ON tasks BEGIN
            INSERT INTO audit (task_id, table_name, operation, old_data, new_data)
            VALUES (NEW.id, 'tasks', 'UPDATE',
                json_object('status', OLD.status, 'outputs', OLD.outputs),
                json_object('status', NEW.status, 'outputs', NEW.outputs)
            );
        END;
      sql: "SELECT 1 as initialized"

  # ===========================================================================
  # Filter capabilities based on permissions
  # ===========================================================================
  - id: filter_capabilities
    type: Shell
    description: Filter capabilities based on permission flags.
    depends_on: [init]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        capabilities = json.loads(os.environ.get('CAPABILITIES', '[]'))
        permissions = json.loads(os.environ.get('PERMISSIONS', '{}'))

        # Filter to only capabilities whose required permission is granted
        filtered = [
            cap for cap in capabilities
            if permissions.get(cap.get('requires', 'read'), False)
        ]

        print(json.dumps(filtered))
        EOF
      env:
        CAPABILITIES: "{{ inputs.capabilities | tojson }}"
        PERMISSIONS: "{{ inputs.permissions | tojson }}"

  # ===========================================================================
  # Register Task in Database (model-based insert with auto-generated UUID)
  # ===========================================================================
  - id: register
    type: Sql
    description: Register this cell in the task tree. ID auto-generated via model's auto:uuid.
    depends_on:
      - block: init_db
        required: false
      - block: filter_capabilities
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        # id: omitted - auto-generated by model's "auto: uuid"
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: "cell"
        name: "cortex-cell"
        metadata: "{{ {'labels': {}, 'annotations': {'prompt_hash': (blocks.init.outputs.stdout | fromjson).hash, 'max_depth': inputs.max_depth, 'max_iterations': inputs.max_iterations}} | tojson }}"
        inputs: "{{ {'prompt': inputs.prompt, 'context': inputs.context, 'iterations': inputs.iterations} | tojson }}"
        status: "running"
        depth: "{{ inputs.depth }}"
        iteration: "{{ inputs.iterations }}"

  # ===========================================================================
  # Store Root Task ID in Metadata (only for root cell)
  # ===========================================================================
  - id: register_root_meta
    type: Sql
    description: Store root_task_id in metadata table for child cell lookups.
    condition: "{{ inputs.parent_id == '' }}"
    depends_on: [register]
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      sql: |
        INSERT OR REPLACE INTO metadata (key, value)
        VALUES ('root_task_id', ?), ('created_at', datetime('now'))
      params:
        - "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

  - id: early_termination
    type: Shell
    description: Return early termination result.
    depends_on: [init, register]
    condition: "{{ (blocks.init.outputs.stdout | fromjson).terminate }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        reason = os.environ.get('TERMINATION_REASON', 'unknown')

        result = {
            'synthesis': {
                'findings': [],
                'summary': f'Cell terminated: {reason}',
                'recommendations': [],
                'actions_needed': False
            },
            'response': f'Unable to complete: {reason}',
            'categorization': {},
            'actions': {}
        }

        print(json.dumps(result))
        EOF
      env:
        TERMINATION_REASON: "{{ (blocks.init.outputs.stdout | fromjson).reason }}"

  # ===========================================================================
  # PHASE 1: CATEGORIZE - classify query and set pipeline_config
  # ===========================================================================
  - id: categorize
    description: Classify the query and set pipeline configuration from mapping table.
    type: Workflow
    depends_on: [register]
    condition: "{{ not (blocks.init.outputs.stdout | fromjson).terminate }}"
    inputs:
      workflow: cortex-phase-categorize
      inputs:
        prompt: "{{ inputs.prompt }}"
        context: "{{ inputs.context }}"
        depth: "{{ inputs.depth }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        state: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        capabilities: "{{ blocks.filter_capabilities.outputs.stdout | fromjson }}"
        permissions: "{{ inputs.permissions }}"

  # ===========================================================================
  # PHASE 2: GATHER - collect evidence (always runs)
  # ===========================================================================
  - id: gather
    type: Workflow
    description: Collect evidence based on pipeline_config.gather settings.
    depends_on: [categorize]
    inputs:
      workflow: cortex-phase-gather
      inputs:
        prompt: "{{ inputs.prompt }}"
        context:
          repo_path: "{{ inputs.context.repo_path | default('.') }}"
          category: "{{ blocks.categorize.outputs.result.category | default('understanding') }}"
        # Pipeline config from CATEGORIZE
        pipeline_config: "{{ blocks.categorize.outputs.pipeline_config | default({}) }}"
        depth: "{{ inputs.depth }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        state: "{{ blocks.categorize.outputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        capabilities: "{{ blocks.filter_capabilities.outputs.stdout | fromjson }}"
        permissions: "{{ inputs.permissions }}"

  # ===========================================================================
  # PHASE 3: REASON - analyze + synthesize (always runs)
  # ===========================================================================
  - id: reason
    description: Analyze evidence and synthesize into unified conclusions.
    depends_on: [gather]
    condition: "{{ blocks.gather.succeeded }}"
    type: Workflow
    inputs:
      workflow: cortex-phase-reason
      inputs:
        prompt: "{{ inputs.prompt }}"
        context:
          repo_path: "{{ inputs.context.repo_path | default('.') }}"
          category: "{{ blocks.categorize.outputs.result.category | default('understanding') }}"
          gather_result: "{{ blocks.gather.outputs.result | default({}) }}"
        # Merge GATHER's config override into pipeline_config
        pipeline_config: |
          {{ blocks.categorize.outputs.pipeline_config | default({}) | combine(blocks.gather.outputs.config_override | default({}), recursive=True) }}
        depth: "{{ inputs.depth }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        state: "{{ blocks.gather.outputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        capabilities: "{{ blocks.filter_capabilities.outputs.stdout | fromjson }}"
        permissions: "{{ inputs.permissions }}"

  # ===========================================================================
  # PHASE 4: ACT - take action (always runs, respects trigger policy)
  # ===========================================================================
  - id: act
    description: Take action based on synthesis and pipeline_config.act settings.
    depends_on: [reason]
    condition: "{{ blocks.reason.succeeded }}"
    type: Workflow
    inputs:
      workflow: cortex-phase-act
      inputs:
        prompt: "{{ inputs.prompt }}"
        context:
          repo_path: "{{ inputs.context.repo_path | default('.') }}"
          category: "{{ blocks.categorize.outputs.result.category | default('understanding') }}"
          synthesis: "{{ blocks.reason.outputs.synthesis | default({}) }}"
          original_prompt: "{{ inputs.context.original_prompt | default(inputs.prompt) }}"
        # Merge REASON's config override into pipeline_config
        pipeline_config: |
          {{ blocks.categorize.outputs.pipeline_config | default({}) | combine(blocks.gather.outputs.config_override | default({}), recursive=True) | combine(blocks.reason.outputs.config_override | default({}), recursive=True) }}
        depth: "{{ inputs.depth }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        state: "{{ blocks.reason.outputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        capabilities: "{{ blocks.filter_capabilities.outputs.stdout | fromjson }}"
        permissions: "{{ inputs.permissions }}"

  # ===========================================================================
  # AGGREGATE TOKEN USAGE - Sum up all LLM calls for this cell
  # ===========================================================================
  - id: aggregate_tokens
    type: Sql
    description: Aggregate token usage from all LLM calls for this cell.
    depends_on:
      - block: early_termination
        required: false
      - block: reason
        required: false
      - block: act
        required: false
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      sql: |
        SELECT
          COALESCE(SUM(prompt_tokens), 0) as total_prompt_tokens,
          COALESCE(SUM(completion_tokens), 0) as total_completion_tokens,
          COALESCE(SUM(prompt_tokens), 0) + COALESCE(SUM(completion_tokens), 0) as total_tokens,
          COALESCE(SUM(duration_ms), 0) as total_duration_ms,
          COUNT(*) as call_count
        FROM llm_calls

  # ===========================================================================
  # TRACK COMPLETION - store synthesis for child querying (model-based update)
  # ===========================================================================
  - id: track_done
    type: Sql
    description: Mark cell complete and store synthesis for child querying.
    depends_on:
      - block: early_termination
        required: false
      - block: reason
        required: false
      - block: act
        required: false
      - block: aggregate_tokens
        required: false
    inputs:
      engine: sqlite
      path: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
      data:
        status: "done"
        outputs: >-
          {{ {
            'response': blocks.reason.outputs.response if blocks.reason.succeeded
                        else (blocks.early_termination.outputs.stdout | fromjson).response if blocks.early_termination.succeeded
                        else 'No response produced',
            'synthesis': blocks.reason.outputs.synthesis if blocks.reason.succeeded
                         else (blocks.early_termination.outputs.stdout | fromjson).synthesis if blocks.early_termination.succeeded
                         else {},
            'complete': blocks.act.outputs.complete | default(true),
            'continuation_needed': blocks.act.outputs.continuation_needed | default(false)
          } | tojson }}

# =============================================================================
# OUTPUTS
# =============================================================================
outputs:
  synthesis:
    type: dict
    description: "Unified synthesis from the cognitive cycle."
    value: |
      {{ blocks.reason.outputs.synthesis if blocks.reason.succeeded
         else (blocks.early_termination.outputs.stdout | fromjson).synthesis if blocks.early_termination.succeeded
         else {'findings': [], 'summary': 'No synthesis'} }}

  response:
    type: str
    description: "Direct response to the query."
    value: |
      {{ blocks.reason.outputs.response if blocks.reason.succeeded
         else (blocks.early_termination.outputs.stdout | fromjson).response if blocks.early_termination.succeeded
         else 'No response produced' }}

  categorization:
    type: dict
    description: "Query categorization result with pipeline_config."
    value: "{{ blocks.categorize.outputs.result if blocks.categorize.succeeded else {} }}"

  pipeline_config:
    type: dict
    description: "Final pipeline configuration (with all overrides applied)."
    value: |
      {{ blocks.categorize.outputs.pipeline_config | default({}) | combine(blocks.gather.outputs.config_override | default({}), recursive=True) | combine(blocks.reason.outputs.config_override | default({}), recursive=True) if blocks.reason.succeeded else {} }}

  actions:
    type: dict
    description: "Action results (if any were taken)."
    value: "{{ blocks.act.outputs.result if blocks.act.succeeded else {} }}"

  complete:
    type: bool
    description: "Whether the task was fully completed."
    value: "{{ blocks.act.outputs.complete | default(true) }}"

  continuation_needed:
    type: bool
    description: "Whether a continuation cell should be spawned."
    value: "{{ blocks.act.outputs.continuation_needed | default(false) }}"

  continuation_prompt:
    type: str
    description: "Prompt for continuation cell if needed."
    value: "{{ blocks.act.outputs.continuation_prompt | default('') }}"

  task_id:
    type: str
    description: "This cell's task ID."
    value: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

  state:
    type: str
    description: "Path to state database."
    value: "{{ (blocks.init.outputs.stdout | fromjson).state_path }}"

  token_usage:
    type: dict
    description: "Aggregated token usage across all LLM calls in this cell."
    value: |
      {{ {
        'prompt_tokens': (blocks.aggregate_tokens.outputs.rows | default([{}]))[0].total_prompt_tokens | default(0),
        'completion_tokens': (blocks.aggregate_tokens.outputs.rows | default([{}]))[0].total_completion_tokens | default(0),
        'total_tokens': (blocks.aggregate_tokens.outputs.rows | default([{}]))[0].total_tokens | default(0),
        'total_duration_ms': (blocks.aggregate_tokens.outputs.rows | default([{}]))[0].total_duration_ms | default(0),
        'call_count': (blocks.aggregate_tokens.outputs.rows | default([{}]))[0].call_count | default(0)
      } if blocks.aggregate_tokens.succeeded else {'prompt_tokens': 0, 'completion_tokens': 0, 'total_tokens': 0, 'total_duration_ms': 0, 'call_count': 0} }}
