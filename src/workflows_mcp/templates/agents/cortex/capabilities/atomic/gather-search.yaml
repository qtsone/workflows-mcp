name: cortex-gather-search
description: |
  Atomic capability: Search for patterns in codebase and store as evidence.
  Wraps ripgrep via Shell block.
  Automatically stores search results in episodic memory (evidence table).

tags: [cortex, capability, atomic, gather, search, evidence]

inputs:
  pattern:
    type: str
    description: Regex pattern to search for.
    required: true

  path:
    type: str
    description: Directory to search in.
    required: false
    default: "."

  file_type:
    type: str
    description: File type filter (e.g., py, js, yaml).
    required: false
    default: ""

  max_matches:
    type: num
    description: Maximum matches to return.
    required: false
    default: 100

  state:
    type: str
    description: Path to SQLite state database (for evidence storage and task tracking).
    required: false
    default: ""

  parent_id:
    type: str
    description: Parent task ID for task tree registration.
    required: false
    default: ""

  task_id:
    type: str
    description: Task ID for evidence tracking.
    required: false
    default: ""

blocks:
  # ===========================================================================
  # STEP 1: Register capability in task tree
  # ===========================================================================
  - id: register
    type: Workflow
    description: Register this capability in the task tree.
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ inputs.state }}"
        parent_id: "{{ inputs.parent_id }}"
        kind: "capability"
        name: "cortex-gather-search"
        metadata:
          labels:
            capability_type: "gather"
            side_effects: "false"
        inputs_data:
          pattern: "{{ inputs.pattern }}"
          path: "{{ inputs.path }}"
          file_type: "{{ inputs.file_type }}"
          max_matches: "{{ inputs.max_matches }}"
        status: "running"
        caller: "cortex-gather-search"

  # ===========================================================================
  # STEP 2: Execute search
  # ===========================================================================
  - id: search
    type: Shell
    depends_on: [register]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import subprocess
        import os
        import shutil
        import re

        pattern = os.environ.get('PATTERN', '')
        path = os.environ.get('PATH_ARG', '.')
        file_type = os.environ.get('FILE_TYPE', '')
        max_matches = int(os.environ.get('MAX_MATCHES', '50'))

        # Max line length to prevent mega-line bloat (e.g., minified JSON)
        MAX_LINE_LEN = 500

        def truncate_line(text, max_len=MAX_LINE_LEN):
            """Truncate line to prevent mega-lines from bloating results."""
            text = text.strip()
            if len(text) > max_len:
                return text[:max_len] + '...[truncated]'
            return text

        def search_with_ripgrep(pattern, path, file_type, max_matches):
            """Search using ripgrep (preferred - faster)."""
            cmd = [
                'rg', '--json', '-e', pattern, path,
                # Exclude hidden directories (e.g., .git, .mypy_cache, .venv)
                '-g', '!.*',
                # Also exclude common cache/build directories
                '-g', '!node_modules',
                '-g', '!__pycache__',
                '-g', '!*.pyc',
                '-g', '!dist',
                '-g', '!build',
            ]
            if file_type:
                cmd.extend(['--type', file_type])

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            matches = []
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if data.get('type') == 'match':
                        match_data = data.get('data', {})
                        line_text = match_data.get('lines', {}).get('text', '').strip()
                        matches.append({
                            'path': match_data.get('path', {}).get('text', ''),
                            'line_number': match_data.get('line_number', 0),
                            'line_text': truncate_line(line_text)
                        })
                        if len(matches) >= max_matches:
                            break
                except json.JSONDecodeError:
                    pass
            return matches

        def search_with_grep(pattern, path, file_type, max_matches):
            """Fallback search using grep (slower but always available)."""
            # Build grep command with recursive search
            # Exclude hidden directories and common cache directories
            exclude_dirs = [
                '--exclude-dir=.*',      # Hidden directories
                '--exclude-dir=node_modules',
                '--exclude-dir=__pycache__',
                '--exclude-dir=dist',
                '--exclude-dir=build',
            ]
            cmd = ['grep', '-r', '-n', '-E'] + exclude_dirs + [pattern, path]
            if file_type:
                # Convert file type to include pattern
                cmd = ['grep', '-r', '-n', '-E', '--include=*.' + file_type] + exclude_dirs + [pattern, path]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            matches = []

            # Parse grep output: file:line_number:line_text
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                # Handle format: path:line_number:line_text
                parts = line.split(':', 2)
                if len(parts) >= 3:
                    matches.append({
                        'path': parts[0],
                        'line_number': int(parts[1]) if parts[1].isdigit() else 0,
                        'line_text': truncate_line(parts[2])
                    })
                elif len(parts) == 2:
                    matches.append({
                        'path': parts[0],
                        'line_number': 0,
                        'line_text': truncate_line(parts[1])
                    })
                if len(matches) >= max_matches:
                    break
            return matches

        try:
            # Try ripgrep first (faster)
            if shutil.which('rg'):
                matches = search_with_ripgrep(pattern, path, file_type, max_matches)
                tool_used = 'ripgrep'
            else:
                # Fall back to grep (always available)
                matches = search_with_grep(pattern, path, file_type, max_matches)
                tool_used = 'grep'

            print(json.dumps({
                'matches': matches,
                'count': len(matches),
                'pattern': pattern,
                'truncated': len(matches) >= max_matches,
                'tool': tool_used
            }))
        except subprocess.TimeoutExpired:
            print(json.dumps({'matches': [], 'count': 0, 'error': 'timeout'}))
        except Exception as e:
            print(json.dumps({'matches': [], 'count': 0, 'error': str(e)}))
        EOF
      env:
        PATTERN: "{{ inputs.pattern }}"
        PATH_ARG: "{{ inputs.path }}"
        FILE_TYPE: "{{ inputs.file_type }}"
        MAX_MATCHES: "{{ inputs.max_matches }}"

  # ===========================================================================
  # STEP 3: Store search results as evidence in episodic memory
  # ===========================================================================
  - id: store_evidence
    type: Workflow
    description: Store search results as evidence.
    depends_on: [search]
    condition: "{{ (blocks.search.outputs.stdout | fromjson).count > 0 }}"
    inputs:
      workflow: cortex-evidence-store
      inputs:
        state: "{{ inputs.state }}"
        key: "search:{{ inputs.pattern }}"
        value: "{{ blocks.search.outputs.stdout }}"
        evidence_type: "search"
        task_id: "{{ inputs.task_id }}"

  # ===========================================================================
  # STEP 4: Track completion
  # ===========================================================================
  - id: track_done
    type: Workflow
    description: Mark capability complete in task tree.
    depends_on:
      - block: search
        required: true
      - block: store_evidence
        required: false
    inputs:
      workflow: agent-state-management
      inputs:
        state: "{{ blocks.register.outputs.state }}"
        task_id: "{{ blocks.register.outputs.task.task_id }}"
        status: "done"
        outputs_data:
          count: "{{ (blocks.search.outputs.stdout | fromjson).count }}"
          pattern: "{{ inputs.pattern }}"
          evidence_stored: "{{ blocks.store_evidence.succeeded | default(false) }}"
        caller: "cortex-gather-search"

outputs:
  matches:
    value: "{{ (blocks.search.outputs.stdout | fromjson).matches }}"
    type: list
    description: List of matches with path, line_number, line_text.

  count:
    value: "{{ (blocks.search.outputs.stdout | fromjson).count }}"
    type: num
    description: Number of matches found.

  pattern:
    value: "{{ inputs.pattern }}"
    type: str
    description: Pattern that was searched.

  evidence:
    value: "{{ ['search:' ~ inputs.pattern ~ ':' ~ (blocks.search.outputs.stdout | fromjson).count ~ ' matches'] }}"
    type: list
    description: Evidence reference for findings.

  evidence_stored:
    value: "{{ blocks.store_evidence.succeeded | default(false) }}"
    type: bool
    description: Whether evidence was stored.

  task_id:
    value: "{{ blocks.register.outputs.task.task_id | default('') }}"
    type: str
    description: Task ID of this capability invocation.

  state:
    value: "{{ blocks.register.outputs.state | default(inputs.state) }}"
    type: str
    description: Path to state database (passthrough).
