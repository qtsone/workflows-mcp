name: cortex-gather-search
description: |
  Atomic capability: Search for patterns in codebase and store as evidence.
  Wraps ripgrep via Shell block.
  Automatically stores search results in episodic memory (evidence table).

tags: [cortex, capability, atomic, gather, search, evidence]

inputs:
  pattern:
    type: str
    description: Regex pattern to search for.
    required: true

  path:
    type: str
    description: Directory to search in.
    required: false
    default: "."

  file_type:
    type: str
    description: File type filter (e.g., py, js, yaml).
    required: false
    default: ""

  max_matches:
    type: num
    description: Maximum matches to return.
    required: false
    default: 50

  state:
    type: str
    description: Path to SQLite state database (for evidence storage).
    required: false
    default: ""

  source_cell:
    type: str
    description: Cell ID gathering this evidence.
    required: false
    default: ""

blocks:
  - id: search
    type: Shell
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import subprocess
        import os
        import shutil
        import re

        pattern = os.environ.get('PATTERN', '')
        path = os.environ.get('PATH_ARG', '.')
        file_type = os.environ.get('FILE_TYPE', '')
        max_matches = int(os.environ.get('MAX_MATCHES', '50'))

        # Max line length to prevent mega-line bloat (e.g., minified JSON)
        MAX_LINE_LEN = 500

        def truncate_line(text, max_len=MAX_LINE_LEN):
            """Truncate line to prevent mega-lines from bloating results."""
            text = text.strip()
            if len(text) > max_len:
                return text[:max_len] + '...[truncated]'
            return text

        def search_with_ripgrep(pattern, path, file_type, max_matches):
            """Search using ripgrep (preferred - faster)."""
            cmd = [
                'rg', '--json', '-e', pattern, path,
                # Exclude hidden directories (e.g., .git, .mypy_cache, .venv)
                '-g', '!.*',
                # Also exclude common cache/build directories
                '-g', '!node_modules',
                '-g', '!__pycache__',
                '-g', '!*.pyc',
                '-g', '!dist',
                '-g', '!build',
            ]
            if file_type:
                cmd.extend(['--type', file_type])

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            matches = []
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if data.get('type') == 'match':
                        match_data = data.get('data', {})
                        line_text = match_data.get('lines', {}).get('text', '').strip()
                        matches.append({
                            'path': match_data.get('path', {}).get('text', ''),
                            'line_number': match_data.get('line_number', 0),
                            'line_text': truncate_line(line_text)
                        })
                        if len(matches) >= max_matches:
                            break
                except json.JSONDecodeError:
                    pass
            return matches

        def search_with_grep(pattern, path, file_type, max_matches):
            """Fallback search using grep (slower but always available)."""
            # Build grep command with recursive search
            # Exclude hidden directories and common cache directories
            exclude_dirs = [
                '--exclude-dir=.*',      # Hidden directories
                '--exclude-dir=node_modules',
                '--exclude-dir=__pycache__',
                '--exclude-dir=dist',
                '--exclude-dir=build',
            ]
            cmd = ['grep', '-r', '-n', '-E'] + exclude_dirs + [pattern, path]
            if file_type:
                # Convert file type to include pattern
                cmd = ['grep', '-r', '-n', '-E', '--include=*.' + file_type] + exclude_dirs + [pattern, path]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            matches = []

            # Parse grep output: file:line_number:line_text
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                # Handle format: path:line_number:line_text
                parts = line.split(':', 2)
                if len(parts) >= 3:
                    matches.append({
                        'path': parts[0],
                        'line_number': int(parts[1]) if parts[1].isdigit() else 0,
                        'line_text': truncate_line(parts[2])
                    })
                elif len(parts) == 2:
                    matches.append({
                        'path': parts[0],
                        'line_number': 0,
                        'line_text': truncate_line(parts[1])
                    })
                if len(matches) >= max_matches:
                    break
            return matches

        try:
            # Try ripgrep first (faster)
            if shutil.which('rg'):
                matches = search_with_ripgrep(pattern, path, file_type, max_matches)
                tool_used = 'ripgrep'
            else:
                # Fall back to grep (always available)
                matches = search_with_grep(pattern, path, file_type, max_matches)
                tool_used = 'grep'

            print(json.dumps({
                'matches': matches,
                'count': len(matches),
                'pattern': pattern,
                'truncated': len(matches) >= max_matches,
                'tool': tool_used
            }))
        except subprocess.TimeoutExpired:
            print(json.dumps({'matches': [], 'count': 0, 'error': 'timeout'}))
        except Exception as e:
            print(json.dumps({'matches': [], 'count': 0, 'error': str(e)}))
        EOF
      env:
        PATTERN: "{{ inputs.pattern }}"
        PATH_ARG: "{{ inputs.path }}"
        FILE_TYPE: "{{ inputs.file_type }}"
        MAX_MATCHES: "{{ inputs.max_matches }}"

  # Store search results as evidence in episodic memory
  - id: store_evidence
    type: Workflow
    description: Store search results as evidence.
    depends_on: [search]
    condition: "{{ inputs.state != '' and (blocks.search.outputs.stdout | fromjson).count > 0 }}"
    inputs:
      workflow: cortex-evidence-store
      inputs:
        state: "{{ inputs.state }}"
        key: "search:{{ inputs.pattern }}"
        value: "{{ blocks.search.outputs.stdout }}"
        evidence_type: "search"
        source_cell: "{{ inputs.source_cell }}"
        source_operation: "cortex-gather-search"

outputs:
  matches:
    value: "{{ (blocks.search.outputs.stdout | fromjson).matches }}"
    type: list
    description: List of matches with path, line_number, line_text.

  count:
    value: "{{ (blocks.search.outputs.stdout | fromjson).count }}"
    type: num
    description: Number of matches found.

  pattern:
    value: "{{ inputs.pattern }}"
    type: str
    description: Pattern that was searched.

  evidence:
    value: "{{ ['search:' ~ inputs.pattern ~ ':' ~ (blocks.search.outputs.stdout | fromjson).count ~ ' matches'] }}"
    type: list
    description: Evidence reference for findings.

  evidence_stored:
    value: "{{ blocks.store_evidence.succeeded | default(false) }}"
    type: bool
    description: Whether evidence was stored.
