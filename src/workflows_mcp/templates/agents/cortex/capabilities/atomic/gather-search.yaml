name: cortex-gather-search
description: |
  Atomic capability: Search for patterns in codebase and store as evidence.
  Wraps ripgrep via Shell block.
  Automatically stores search results in episodic memory (evidence table).

tags: [cortex, capability, atomic, gather, search, evidence, v2]

inputs:
  pattern:
    type: str
    description: Regex pattern to search for.
    required: true

  path:
    type: str
    description: Directory to search in.
    required: false
    default: "."

  file_type:
    type: str
    description: File type filter (e.g., py, js, yaml).
    required: false
    default: ""

  max_matches:
    type: num
    description: Maximum matches to return.
    required: false
    default: 100

  state:
    type: str
    description: Path to SQLite state database (for evidence storage and task tracking).
    required: true

  models:
    type: dict
    description: Model definitions for CRUD operations.
    required: false
    default: {}

  parent_id:
    type: str
    description: Parent task ID for task tree registration.
    required: false
    default: ""

  task_id:
    type: str
    description: Task ID for evidence tracking.
    required: false
    default: ""

  depth:
    type: num
    description: Task depth for task tree registration.
    required: false
    default: 0

blocks:
  - id: register
    type: Sql
    description: Register this capability in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: capability
        name: cortex-gather-search
        metadata: "{{ {'labels': {'capability_type': 'gather', 'side_effects': 'false'}} }}"
        inputs: "{{ {'pattern': inputs.pattern, 'path': inputs.path, 'file_type': inputs.file_type, 'max_matches': inputs.max_matches} }}"
        status: running
        depth: "{{ inputs.depth }}"

  - id: search
    type: Shell
    depends_on:
      - block: register
        required: false
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import subprocess
        import os
        import shutil
        import re

        pattern = os.environ.get('PATTERN', '')
        path = os.environ.get('PATH_ARG', '.')
        file_type = os.environ.get('FILE_TYPE', '')
        max_matches = int(os.environ.get('MAX_MATCHES', '50'))

        # Max line length to prevent mega-line bloat (e.g., minified JSON)
        MAX_LINE_LEN = 500

        def truncate_line(text, max_len=MAX_LINE_LEN):
            """Truncate line to prevent mega-lines from bloating results."""
            text = text.strip()
            if len(text) > max_len:
                return text[:max_len] + '...[truncated]'
            return text

        def search_with_ripgrep(pattern, path, file_type, max_matches):
            """Search using ripgrep (preferred - faster)."""
            cmd = [
                'rg', '--json', '-e', pattern, path,
                # Exclude hidden directories (e.g., .git, .mypy_cache, .venv)
                '-g', '!.*',
                # Also exclude common cache/build directories
                '-g', '!node_modules',
                '-g', '!__pycache__',
                '-g', '!*.pyc',
                '-g', '!dist',
                '-g', '!build',
            ]
            if file_type:
                cmd.extend(['--type', file_type])

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=30)
            matches = []
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                try:
                    data = json.loads(line)
                    if data.get('type') == 'match':
                        match_data = data.get('data', {})
                        line_text = match_data.get('lines', {}).get('text', '').strip()
                        matches.append({
                            'path': match_data.get('path', {}).get('text', ''),
                            'line_number': match_data.get('line_number', 0),
                            'line_text': truncate_line(line_text)
                        })
                        if len(matches) >= max_matches:
                            break
                except json.JSONDecodeError:
                    pass
            return matches

        def search_with_grep(pattern, path, file_type, max_matches):
            """Fallback search using grep (slower but always available)."""
            # Build grep command with recursive search
            # Exclude hidden directories and common cache directories
            exclude_dirs = [
                '--exclude-dir=.*',      # Hidden directories
                '--exclude-dir=node_modules',
                '--exclude-dir=__pycache__',
                '--exclude-dir=dist',
                '--exclude-dir=build',
            ]
            cmd = ['grep', '-r', '-n', '-E'] + exclude_dirs + [pattern, path]
            if file_type:
                # Convert file type to include pattern
                cmd = ['grep', '-r', '-n', '-E', '--include=*.' + file_type] + exclude_dirs + [pattern, path]

            result = subprocess.run(cmd, capture_output=True, text=True, timeout=60)
            matches = []

            # Parse grep output: file:line_number:line_text
            for line in result.stdout.strip().split('\n'):
                if not line:
                    continue
                # Handle format: path:line_number:line_text
                parts = line.split(':', 2)
                if len(parts) >= 3:
                    matches.append({
                        'path': parts[0],
                        'line_number': int(parts[1]) if parts[1].isdigit() else 0,
                        'line_text': truncate_line(parts[2])
                    })
                elif len(parts) == 2:
                    matches.append({
                        'path': parts[0],
                        'line_number': 0,
                        'line_text': truncate_line(parts[1])
                    })
                if len(matches) >= max_matches:
                    break
            return matches

        try:
            # Try ripgrep first (faster)
            if shutil.which('rg'):
                matches = search_with_ripgrep(pattern, path, file_type, max_matches)
                tool_used = 'ripgrep'
            else:
                # Fall back to grep (always available)
                matches = search_with_grep(pattern, path, file_type, max_matches)
                tool_used = 'grep'

            print(json.dumps({
                'matches': matches,
                'count': len(matches),
                'pattern': pattern,
                'truncated': len(matches) >= max_matches,
                'tool': tool_used
            }))
        except subprocess.TimeoutExpired:
            print(json.dumps({'matches': [], 'count': 0, 'error': 'timeout'}))
        except Exception as e:
            print(json.dumps({'matches': [], 'count': 0, 'error': str(e)}))
        EOF
      env:
        PATTERN: "{{ inputs.pattern }}"
        PATH_ARG: "{{ inputs.path }}"
        FILE_TYPE: "{{ inputs.file_type }}"
        MAX_MATCHES: "{{ inputs.max_matches }}"

  - id: store_evidence
    type: Sql
    description: Store search results as evidence.
    depends_on: [search]
    condition: "{{ get(blocks.search.outputs.stdout, 'count', 0) > 0 }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.memory }}"
      op: upsert
      data:
        namespace: search
        key: "search:{{ inputs.pattern }}"
        value: "{{ blocks.search.outputs.stdout }}"
        metadata: "{'count': {{ get(blocks.search.outputs.stdout, 'count', 0) }}, 'pattern': {{ inputs.pattern }}}"
        task_id: "{{ inputs.task_id if inputs.task_id else get(blocks.register.outputs, 'rows.0.id', '') }}"
      conflict: [namespace, key]

  - id: track_done
    type: Sql
    description: Mark capability complete in task tree.
    depends_on:
      - block: register
        required: true
      - block: search
        required: false
      - block: store_evidence
        required: false
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ get(blocks.register.outputs, 'rows.0.id', '') }}"
      data:
        status: "{{ 'done' if blocks.search.succeeded else 'failed' }}"
        outputs: "{'count': {{ get(blocks.search.outputs.stdout, 'count', 0) if blocks.search.succeeded else 0 }}, 'pattern': {{ inputs.pattern }}, 'evidence_stored': {{ get(blocks.store_evidence.succeeded, false) }}, 'error': {{ get(blocks.search.metadata, 'message', '') }}}"

outputs:
  matches:
    value: "{{ get(blocks.search.outputs.stdout, 'matches', []) }}"
    type: list
    description: List of matches with path, line_number, line_text.

  count:
    value: "{{ get(blocks.search.outputs.stdout, 'count', 0) }}"
    type: num
    description: Number of matches found.

  pattern:
    value: "{{ inputs.pattern }}"
    type: str
    description: Pattern that was searched.

  evidence:
    value: "{{ ['search:' ~ inputs.pattern ~ ':' ~ get(blocks.search.outputs.stdout, 'count', 0) ~ ' matches'] }}"
    type: list
    description: Evidence reference for findings.

  evidence_stored:
    value: "{{ get(blocks.store_evidence.succeeded, false) }}"
    type: bool
    description: Whether evidence was stored.

  task_id:
    value: "{{ get(blocks.register.outputs, 'rows.0.id', '') }}"
    type: str
    description: Task ID of this capability invocation.

  state:
    value: "{{ inputs.state }}"
    type: str
    description: Path to state database (passthrough).
