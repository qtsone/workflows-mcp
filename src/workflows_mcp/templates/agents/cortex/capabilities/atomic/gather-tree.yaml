# CORTEX Gather Tree - Directory Structure Discovery
# Atomic capability: Discover directory structure and store in memory.
# Critical for investigation tasks (debugging, quality, discovery) where the
# LLM needs to know what files exist before it can plan what to gather.
# Runs early in GATHER phase, before the planning LLM, so the tree is
# available in memory for pattern generation.

name: cortex-gather-tree
description: |
  Atomic capability: Discover directory structure and store in memory.
  Provides the GATHER phase LLM with a map of what files exist, enabling
  efficient pattern generation instead of blind guessing.

tags: [cortex, capability, atomic, gather, tree, structure, v4]

inputs:
  base_path:
    type: str
    description: Root directory to scan.
    required: true

  max_depth:
    type: num
    description: Maximum depth for tree traversal.
    default: 4

  include_patterns:
    type: list
    description: |
      Glob patterns to include (e.g., ["*.py", "*.yaml", "*.ts"]).
      Empty list means include all files.
    default: []

  exclude_patterns:
    type: list
    description: |
      Patterns to exclude from tree (common: node_modules, __pycache__, .git).
    default:
      - "node_modules"
      - "__pycache__"
      - ".git"
      - ".venv"
      - "venv"
      - ".mypy_cache"
      - ".pytest_cache"
      - ".ruff_cache"
      - "dist"
      - "build"
      - ".eggs"
      - "*.egg-info"
      - ".tox"
      - "htmlcov"
      - ".coverage"

  max_files:
    type: num
    description: Maximum files to include in tree output.
    default: 500

  state:
    type: str
    description: Path to SQLite state database.
    required: true

  models:
    type: dict
    description: Model definitions for CRUD operations.
    required: true

  parent_id:
    type: str
    description: Parent task ID for task tree registration.
    default: ""

  task_id:
    type: str
    description: Task ID for evidence tracking.
    default: ""

  depth:
    type: num
    description: Current depth of tree traversal.
    default: 0

blocks:
  - id: register
    type: Sql
    description: Register this capability in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: capability
        name: cortex-gather-tree
        metadata: "{'labels': {'capability_type': 'gather', 'side_effects': 'false'}}"
        inputs: "{'base_path': {{ inputs.base_path }}, 'max_depth': {{ inputs.max_depth }}}"
        status: running
        depth: "{{ inputs.depth }}"

  - id: generate_tree
    type: Shell
    description: Generate directory tree structure.
    depends_on: [register]
    inputs:
      command: |
        python3 << 'EOF'
        import os
        import json
        import fnmatch
        from pathlib import Path
        from collections import defaultdict

        base_path = os.environ.get('BASE_PATH', '.')
        max_depth = int(os.environ.get('MAX_DEPTH', '4'))
        max_files = int(os.environ.get('MAX_FILES', '500'))
        exclude_json = os.environ.get('EXCLUDE_PATTERNS', '[]')
        include_json = os.environ.get('INCLUDE_PATTERNS', '[]')

        try:
            exclude_patterns = json.loads(exclude_json)
        except:
            exclude_patterns = []

        try:
            include_patterns = json.loads(include_json)
        except:
            include_patterns = []

        def should_exclude(path_str, name):
            """Check if path should be excluded."""
            for pattern in exclude_patterns:
                if fnmatch.fnmatch(name, pattern):
                    return True
                if fnmatch.fnmatch(path_str, f"*/{pattern}/*"):
                    return True
            return False

        def should_include(name):
            """Check if file matches include patterns (empty = include all)."""
            if not include_patterns:
                return True
            return any(fnmatch.fnmatch(name, p) for p in include_patterns)

        def get_file_type(name):
            """Categorize file by extension."""
            ext = Path(name).suffix.lower()
            type_map = {
                '.py': 'python',
                '.yaml': 'yaml', '.yml': 'yaml',
                '.json': 'json',
                '.ts': 'typescript', '.tsx': 'typescript',
                '.js': 'javascript', '.jsx': 'javascript',
                '.md': 'markdown',
                '.txt': 'text',
                '.sql': 'sql',
                '.sh': 'shell',
                '.toml': 'toml',
                '.cfg': 'config', '.ini': 'config', '.conf': 'config',
            }
            return type_map.get(ext, 'other')

        # Walk directory tree
        base = Path(base_path).resolve()
        files = []
        dirs = []
        type_counts = defaultdict(int)
        total_size = 0
        truncated = False

        try:
            for root, dirnames, filenames in os.walk(base):
                # Calculate depth
                rel_root = Path(root).relative_to(base)
                depth = len(rel_root.parts) if str(rel_root) != '.' else 0

                if depth > max_depth:
                    continue

                # Filter directories in-place to skip excluded
                dirnames[:] = [d for d in dirnames if not should_exclude(str(Path(root) / d), d)]

                rel_path = str(rel_root) if str(rel_root) != '.' else ''

                # Track directories
                if rel_path and rel_path not in dirs:
                    dirs.append(rel_path)

                # Process files
                for filename in filenames:
                    if should_exclude(str(Path(root) / filename), filename):
                        continue

                    if not should_include(filename):
                        continue

                    if len(files) >= max_files:
                        truncated = True
                        break

                    file_path = Path(root) / filename
                    rel_file = str(file_path.relative_to(base))

                    try:
                        size = file_path.stat().st_size
                    except:
                        size = 0

                    file_type = get_file_type(filename)
                    type_counts[file_type] += 1
                    total_size += size

                    files.append({
                        'path': rel_file,
                        'name': filename,
                        'dir': rel_path,
                        'type': file_type,
                        'size': size
                    })

                if truncated:
                    break

        except Exception as e:
            print(json.dumps({
                'error': str(e),
                'files': [],
                'dirs': [],
                'tree_text': f'Error scanning {base_path}: {e}'
            }))
            exit(0)

        # Generate tree text representation
        tree_lines = [f"{base.name}/"]
        dirs_set = set()

        for f in sorted(files, key=lambda x: x['path']):
            parts = Path(f['path']).parts
            # Add directory entries
            for i in range(len(parts) - 1):
                dir_path = '/'.join(parts[:i+1])
                if dir_path not in dirs_set:
                    dirs_set.add(dir_path)
                    indent = "  " * (i + 1)
                    tree_lines.append(f"{indent}{parts[i]}/")
            # Add file entry
            indent = "  " * len(parts)
            size_kb = f['size'] / 1024
            size_str = f"{size_kb:.1f}KB" if size_kb >= 1 else f"{f['size']}B"
            tree_lines.append(f"{indent}{f['name']} ({size_str})")

        tree_text = '\n'.join(tree_lines[:200])  # Limit text output
        if len(tree_lines) > 200:
            tree_text += f"\n... and {len(tree_lines) - 200} more entries"

        # Summary by type
        type_summary = [f"{t}: {c}" for t, c in sorted(type_counts.items(), key=lambda x: -x[1])]

        result = {
            'base_path': str(base),
            'files': files,
            'dirs': sorted(list(set(dirs))),
            'file_count': len(files),
            'dir_count': len(set(dirs)),
            'total_size_kb': round(total_size / 1024, 2),
            'type_counts': dict(type_counts),
            'type_summary': ', '.join(type_summary),
            'tree_text': tree_text,
            'truncated': truncated,
            'max_depth': max_depth
        }

        print(json.dumps(result))
        EOF
      env:
        BASE_PATH: "{{ inputs.base_path }}"
        MAX_DEPTH: "{{ inputs.max_depth }}"
        MAX_FILES: "{{ inputs.max_files }}"
        EXCLUDE_PATTERNS: "{{ inputs.exclude_patterns | tojson }}"
        INCLUDE_PATTERNS: "{{ inputs.include_patterns | tojson }}"

  - id: store_tree
    type: Sql
    description: Store directory tree in memory for GATHER phase.
    depends_on: [generate_tree]
    condition: "{{ blocks.generate_tree.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.memory }}"
      op: upsert
      data:
        namespace: "structure"
        key: "tree"
        value: "{{ get(blocks.generate_tree.outputs.stdout, 'tree_text', '') }}"
        metadata: |
          {
            'base_path': {{ get(blocks.generate_tree.outputs.stdout, 'base_path', '.') }},
            'file_count': {{ get(blocks.generate_tree.outputs.stdout, 'file_count', 0) }},
            'dir_count': {{ get(blocks.generate_tree.outputs.stdout, 'dir_count', 0) }},
            'total_size_kb': {{ get(blocks.generate_tree.outputs.stdout, 'total_size_kb', 0) }},
            'type_summary': {{ get(blocks.generate_tree.outputs.stdout, 'type_summary', '') }},
            'truncated': {{ get(blocks.generate_tree.outputs.stdout, 'truncated', false) }}
          }
        task_id: "{{ inputs.task_id if inputs.task_id else get(blocks.register.outputs, 'rows.0.id', '') }}"
      conflict: [namespace, key]

  - id: store_file_list
    type: Sql
    description: Store detailed file listing in memory.
    depends_on: [generate_tree]
    condition: "{{ blocks.generate_tree.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.memory }}"
      op: upsert
      data:
        namespace: "structure"
        key: "files"
        value: "{{ get(blocks.generate_tree.outputs.stdout, 'files', []) | tojson }}"
        metadata: |
          {
            'count': {{ get(blocks.generate_tree.outputs.stdout, 'file_count', 0) }},
            'type_counts': {{ get(blocks.generate_tree.outputs.stdout, 'type_counts', {}) }}
          }
        task_id: "{{ inputs.task_id if inputs.task_id else get(blocks.register.outputs, 'rows.0.id', '') }}"
      conflict: [namespace, key]

  - id: store_dirs
    type: Sql
    description: Store directory listing in memory.
    depends_on: [generate_tree]
    condition: "{{ blocks.generate_tree.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.memory }}"
      op: upsert
      data:
        namespace: "structure"
        key: "dirs"
        value: "{{ get(blocks.generate_tree.outputs.stdout, 'dirs', []) | tojson }}"
        metadata: |
          {
            'count': {{ get(blocks.generate_tree.outputs.stdout, 'dir_count', 0) }}
          }
        task_id: "{{ inputs.task_id if inputs.task_id else get(blocks.register.outputs, 'rows.0.id', '') }}"
      conflict: [namespace, key]

  - id: track_done
    type: Sql
    description: Mark capability complete in task tree.
    depends_on:
      - block: register
        required: true
      - block: generate_tree
        required: false
      - block: store_tree
        required: false
      - block: store_file_list
        required: false
      - block: store_dirs
        required: false
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ get(blocks.register.outputs, 'rows.0.id', '') }}"
      data:
        status: "{{ 'done' if blocks.generate_tree.succeeded else 'failed' }}"
        outputs: |
          {%- if blocks.generate_tree.succeeded -%}
          {
            'file_count': {{ get(blocks.generate_tree.outputs.stdout, 'file_count', 0) }},
            'dir_count': {{ get(blocks.generate_tree.outputs.stdout, 'dir_count', 0) }},
            'total_size_kb': {{ get(blocks.generate_tree.outputs.stdout, 'total_size_kb', 0) }},
            'type_summary': {{ get(blocks.generate_tree.outputs.stdout, 'type_summary', '') }},
            'truncated': {{ get(blocks.generate_tree.outputs.stdout, 'truncated', false) }}
          }
          {%- else -%}
          {'error': 'Tree generation failed'}
          {%- endif %}

outputs:
  tree_text:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'tree_text', '') }}"
    type: str
    description: Human-readable tree representation.

  files:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'files', []) }}"
    type: list
    description: List of files with metadata.

  dirs:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'dirs', []) }}"
    type: list
    description: List of directories.

  file_count:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'file_count', 0) }}"
    type: num
    description: Total number of files discovered.

  dir_count:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'dir_count', 0) }}"
    type: num
    description: Total number of directories.

  type_counts:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'type_counts', {}) }}"
    type: dict
    description: File count by type.

  type_summary:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'type_summary', '') }}"
    type: str
    description: "Summary of file types (e.g., 'python: 45, yaml: 12')."

  total_size_kb:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'total_size_kb', 0) }}"
    type: num
    description: Total size in KB.

  truncated:
    value: "{{ get(blocks.generate_tree.outputs.stdout, 'truncated', false) }}"
    type: bool
    description: Whether the tree was truncated due to max_files limit.

  task_id:
    value: "{{ get(blocks.register.outputs, 'rows.0.id', '') }}"
    type: str
    description: Task ID of this capability invocation.

  state:
    value: "{{ inputs.state }}"
    type: str
    description: Path to state database (passthrough).
