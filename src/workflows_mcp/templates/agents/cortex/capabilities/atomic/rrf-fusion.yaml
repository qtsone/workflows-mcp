# CORTEX RRF Fusion Capability
# Implements Reciprocal Rank Fusion (RRF) to combine multiple rankings.
# RRF is a simple but effective rank aggregation method that:
#   - Doesn't require score normalization across different ranking sources
#   - Is robust to outliers and score distribution differences
#   - Uses a constant k=60 to mitigate sensitivity to high ranks
#
# Formula: RRF_score(d) = Σ 1/(k + rank_r(d))
#
# Reference: Cormack, Clarke, Buettcher (2009) "Reciprocal Rank Fusion
#            outperforms Condorcet and individual Rank Learning Methods"

name: cortex-rrf-fusion
description: "Reciprocal Rank Fusion to combine multiple file rankings"

tags: [cortex, capability, salience, rrf, fusion]

inputs:
  rankings:
    type: list
    description: |
      List of rankings to fuse. Each ranking is a dict with:
        - source: Name of the ranking source (e.g., 'keyword', 'semantic', 'dependency')
        - weight: Optional weight multiplier (default 1.0)
        - files: List of {path, rank, score} objects
    required: true

  k:
    type: num
    description: RRF constant (default 60 per original paper).
    default: 60

  context_budget:
    type: num
    description: Maximum total tokens/chars to include (0 = no limit).
    default: 0

  max_results:
    type: num
    description: Maximum number of results to return.
    default: 100

  state:
    type: str
    description: Path to SQLite state database.
    default: ""
    required: true

  models:
    type: dict
    description: Model definitions for CRUD operations.
    default: {}

  parent_id:
    type: str
    description: Parent task ID for task tree.
    default: ""

  task_id:
    type: str
    description: Task ID for salience tracking.
    default: ""

  depth:
    type: num
    description: Depth of the task tree.
    default: 0

blocks:
  - id: register
    type: Sql
    description: Register this capability in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: capability
        name: cortex-rrf-fusion
        metadata: "{'labels': {'capability_type': 'salience', 'algorithm': 'rrf'}}"
        inputs: "{'k': {{ inputs.k }}, 'context_budget': {{ inputs.context_budget }}, 'max_results': {{ inputs.max_results }}, 'ranking_sources': {{ inputs.rankings | length}} }"
        status: running
        depth: "{{ inputs.depth }}"

  - id: fuse
    type: Shell
    depends_on:
      - block: register
        required: false
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        from collections import defaultdict
        from pathlib import Path

        rankings_json = os.environ.get('RANKINGS', '[]')
        k = int(os.environ.get('K', '60'))
        context_budget = int(os.environ.get('CONTEXT_BUDGET', '0'))
        max_results = int(os.environ.get('MAX_RESULTS', '100'))

        try:
            rankings = json.loads(rankings_json)
        except:
            rankings = []

        def compute_rrf_scores(rankings, k):
            """
            Compute RRF scores for all documents across rankings.

            RRF_score(d) = Σ weight_r * 1/(k + rank_r(d))

            Where:
            - k = 60 (mitigates sensitivity to high ranks)
            - weight_r = optional weight for ranking r (default 1.0)
            - rank_r(d) = rank of document d in ranking r (1-indexed)
            """
            doc_scores = defaultdict(lambda: {'rrf_score': 0.0, 'sources': [], 'ranks': {}})

            for ranking in rankings:
                source = ranking.get('source', 'unknown')
                weight = float(ranking.get('weight', 1.0))
                files = ranking.get('files', [])

                for item in files:
                    path = item.get('path', '')
                    if not path:
                        continue

                    rank = item.get('rank', 0)
                    if rank <= 0:
                        continue

                    # RRF contribution
                    rrf_contribution = weight * (1.0 / (k + rank))
                    doc_scores[path]['rrf_score'] += rrf_contribution
                    doc_scores[path]['sources'].append(source)
                    doc_scores[path]['ranks'][source] = rank

                    # Preserve original scores for debugging
                    if 'bm25_score' in item:
                        doc_scores[path]['bm25_score'] = item['bm25_score']
                    if 'semantic_score' in item:
                        doc_scores[path]['semantic_score'] = item['semantic_score']

            return doc_scores

        def estimate_file_size(path):
            """Estimate file size in tokens (rough: 4 chars = 1 token)."""
            try:
                size = Path(path).stat().st_size
                return size // 4  # Rough token estimate
            except:
                return 1000  # Default estimate

        def apply_context_budget(ranked_files, budget):
            """Filter files to fit within context budget."""
            if budget <= 0:
                return ranked_files, 0, []

            included = []
            excluded = []
            total_tokens = 0

            for doc in ranked_files:
                tokens = estimate_file_size(doc['path'])
                if total_tokens + tokens <= budget:
                    doc['estimated_tokens'] = tokens
                    doc['included'] = True
                    included.append(doc)
                    total_tokens += tokens
                else:
                    doc['estimated_tokens'] = tokens
                    doc['included'] = False
                    excluded.append(doc)

            return included, total_tokens, excluded

        # Compute RRF scores
        doc_scores = compute_rrf_scores(rankings, k)

        # Convert to sorted list
        ranked_files = []
        for path, data in doc_scores.items():
            ranked_files.append({
                'path': path,
                'rrf_score': round(data['rrf_score'], 6),
                'sources': list(set(data['sources'])),
                'source_count': len(set(data['sources'])),
                'ranks': data['ranks'],
                'bm25_score': data.get('bm25_score'),
                'semantic_score': data.get('semantic_score')
            })

        # Sort by RRF score descending
        ranked_files.sort(key=lambda x: -x['rrf_score'])

        # Apply rank positions
        for i, doc in enumerate(ranked_files):
            doc['fused_rank'] = i + 1

        # Apply context budget
        ranked_files = ranked_files[:max_results]
        if context_budget > 0:
            included, tokens_used, excluded = apply_context_budget(ranked_files, context_budget)
        else:
            included = ranked_files
            tokens_used = 0
            excluded = []
            for doc in included:
                doc['included'] = True

        # Summary stats
        sources_used = set()
        for r in rankings:
            sources_used.add(r.get('source', 'unknown'))

        result = {
            'fused_ranking': included,
            'excluded': excluded[:20],  # Keep first 20 excluded for transparency
            'count': len(included),
            'total_candidates': len(doc_scores),
            'context_budget': context_budget,
            'tokens_used': tokens_used,
            'tokens_remaining': context_budget - tokens_used if context_budget > 0 else 0,
            'sources_fused': list(sources_used),
            'k_parameter': k
        }

        print(json.dumps(result))
        EOF
      env:
        RANKINGS: "{{ inputs.rankings | tojson }}"
        K: "{{ inputs.k }}"
        CONTEXT_BUDGET: "{{ inputs.context_budget }}"
        MAX_RESULTS: "{{ inputs.max_results }}"

  - id: store_fusion
    type: Sql
    description: Store fused ranking in salience cache.
    depends_on: [fuse]
    condition: "{{ blocks.fuse.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.salience_cache }}"
      op: insert
      data:
        task_id: "{{ inputs.task_id | default(inputs.parent_id) }}"
        ranking_type: rrf_fused
        query: combined
        results: "{{ blocks.fuse.outputs.stdout }}"

  - id: track_done
    type: Sql
    description: Mark capability complete.
    depends_on:
      - block: register
        required: true
      - block: fuse
        required: false
      - block: store_fusion
        required: false
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ get(blocks.register.outputs, 'rows.0.id', '') }}"
      data:
        status: "{{ 'done' if blocks.fuse.succeeded else 'failed' }}"
        outputs: |
          {
            'count': {{ get(blocks.fuse.outputs.stdout, 'count', 0) if blocks.fuse.succeeded else 0 }},
            'sources_fused': {{ get(blocks.fuse.outputs.stdout, 'sources_fused', []) if blocks.fuse.succeeded else [] }}
          }

outputs:
  fused_ranking:
    type: list
    description: "Files ranked by RRF score with inclusion status."
    value: "{{ get(blocks.fuse.outputs.stdout, 'fused_ranking', []) if blocks.fuse.succeeded else [] }}"

  excluded:
    type: list
    description: "Files excluded due to context budget."
    value: "{{ get(blocks.fuse.outputs.stdout, 'excluded', []) if blocks.fuse.succeeded else [] }}"

  count:
    type: num
    description: "Number of files included."
    value: "{{ get(blocks.fuse.outputs.stdout, 'count', 0) if blocks.fuse.succeeded else 0 }}"

  tokens_used:
    type: num
    description: "Estimated tokens used by included files."
    value: "{{ get(blocks.fuse.outputs.stdout, 'tokens_used', 0) if blocks.fuse.succeeded else 0 }}"

  tokens_remaining:
    type: num
    description: "Remaining context budget."
    value: "{{ get(blocks.fuse.outputs.stdout, 'tokens_remaining', 0) if blocks.fuse.succeeded else 0 }}"

  sources_fused:
    type: list
    description: "Ranking sources that were combined."
    value: "{{ get(blocks.fuse.outputs.stdout, 'sources_fused', []) if blocks.fuse.succeeded else [] }}"

  state:
    type: str
    description: "Path to state database (passthrough)."
    value: "{{ inputs.state }}"
