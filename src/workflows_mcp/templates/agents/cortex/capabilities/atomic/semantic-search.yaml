# =============================================================================
# CORTEX Semantic Search Capability
# =============================================================================
#
# Searches files by semantic similarity using vector embeddings. This capability:
#
# 1. Generates an embedding for the query text
# 2. Lazily generates embeddings for candidate files (using embed-file.yaml)
# 3. Uses sqlite-vec's vec_distance_cosine() for similarity ranking
# 4. Returns files ranked by semantic similarity
#
# Design decisions:
# - Lazy embedding generation: Only embeds files when first accessed
# - Content-hash based cache invalidation: Re-embeds when file content changes
# - Limits to 50 files per search to control embedding costs
#
# The output can be fed into rrf-fusion.yaml as the semantic_rankings input.
#
# =============================================================================

name: cortex-semantic-search
description: "Search files by semantic similarity using vector embeddings"

tags: [cortex, capability, semantic, search, embedding]

inputs:
  query:
    type: str
    description: Query text to search for.
    required: true

  files:
    type: list
    description: List of file paths to search (from keyword search or gather).
    required: true

  profile:
    type: str
    description: LLM profile name for embedding model.
    default: "embedding"

  state:
    type: str
    description: Path to SQLite state database.
    required: true

  models:
    type: dict
    description: Model definitions for CRUD operations.
    default: {}

  parent_id:
    type: str
    description: Parent task ID for task tree.
    default: ""

  max_results:
    type: num
    description: Maximum number of results to return.
    default: 50

blocks:
  # ===========================================================================
  # STEP 1: Register capability in task tree
  # ===========================================================================
  - id: register
    type: Sql
    description: Register this capability in the task tree.
    condition: "{{ inputs.state | trim != '' and inputs.parent_id | trim != '' }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: capability
        name: cortex-semantic-search
        metadata: "{{ {'labels': {'capability_type': 'semantic_search'}} }}"
        inputs: "{{ {'query_length': inputs.query | length, 'file_count': inputs.files | length} }}"
        status: running
        depth: 2

  # ===========================================================================
  # STEP 2: Generate query embedding
  # ===========================================================================
  - id: query_embed
    type: Workflow
    description: Generate embedding for search query.
    depends_on:
      - block: register
        required: false
    inputs:
      workflow: cortex-generate-embedding
      inputs:
        text: "{{ inputs.query }}"
        profile: "{{ inputs.profile }}"
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ inputs.parent_id }}"

  # ===========================================================================
  # STEP 3: Compute file hashes to check cache
  # ===========================================================================
  - id: compute_hashes
    type: Shell
    description: Compute content hashes for candidate files.
    depends_on: [query_embed]
    condition: "{{ blocks.query_embed.succeeded and blocks.query_embed.outputs.success }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        import hashlib

        files_json = os.environ.get('FILES', '[]')
        max_files = int(os.environ.get('MAX_FILES', '50'))

        try:
            files = json.loads(files_json)
        except:
            files = []

        results = []
        for path in files[:max_files]:
            try:
                with open(path, 'rb') as f:
                    content = f.read(50000)  # First 50KB
                    content_hash = hashlib.sha256(content).hexdigest()[:16]
                    results.append({
                        'path': path,
                        'hash': content_hash,
                        'size': len(content)
                    })
            except Exception as e:
                pass  # Skip unreadable files

        print(json.dumps({'files': results, 'count': len(results)}))
        EOF
      env:
        FILES: "{{ inputs.files | tojson }}"
        MAX_FILES: "{{ inputs.max_results }}"

  # ===========================================================================
  # STEP 4: Generate embeddings for files (parallel, lazy)
  # ===========================================================================
  - id: embed_files
    type: Workflow
    description: Generate embeddings for each file (cached).
    for_each: "{{ (blocks.compute_hashes.outputs.stdout | fromjson).files }}"
    for_each_mode: parallel
    depends_on: [compute_hashes]
    condition: "{{ blocks.compute_hashes.succeeded }}"
    inputs:
      workflow: cortex-embed-file
      inputs:
        path: "{{ each.value.path }}"
        content_hash: "{{ each.value.hash }}"
        profile: "{{ inputs.profile }}"
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ inputs.parent_id }}"

  # ===========================================================================
  # STEP 5: Search file embeddings by vector similarity
  # ===========================================================================
  - id: vector_search
    type: Sql
    description: Search cached file embeddings using cosine distance.
    depends_on: [embed_files]
    condition: "{{ blocks.query_embed.succeeded and blocks.query_embed.outputs.success }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        SELECT
          path,
          dimensions,
          vec_distance_cosine(embedding, ?) as distance
        FROM file_embeddings
        WHERE embedding IS NOT NULL
        ORDER BY distance ASC
        LIMIT ?
      params:
        - "{{ blocks.query_embed.outputs.embedding | tojson }}"
        - "{{ inputs.max_results }}"

  # ===========================================================================
  # STEP 6: Format rankings for RRF fusion
  # ===========================================================================
  - id: format_rankings
    type: Shell
    description: Format results as rankings for RRF fusion.
    depends_on: [vector_search]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        rows_json = os.environ.get('ROWS', '[]')

        try:
            rows = json.loads(rows_json)
        except:
            rows = []

        rankings = []
        for rank, row in enumerate(rows, 1):
            rankings.append({
                'path': row.get('path', ''),
                'rank': rank,
                'distance': row.get('distance', 1.0),
                'similarity': 1.0 - min(row.get('distance', 1.0), 1.0)  # Convert distance to similarity
            })

        result = {
            'rankings': rankings,
            'count': len(rankings),
            'has_results': len(rankings) > 0
        }

        print(json.dumps(result))
        EOF
      env:
        ROWS: "{{ blocks.vector_search.outputs.rows | default([]) | tojson }}"

  # ===========================================================================
  # STEP 7: Track completion
  # ===========================================================================
  - id: track_done
    type: Sql
    description: Mark capability complete.
    condition: "{{ inputs.state | trim != '' and inputs.parent_id | trim != '' }}"
    depends_on:
      - block: register
        required: true
      - block: format_rankings
        required: true
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
      data:
        status: "{{ 'done' if blocks.format_rankings.succeeded else 'failed' }}"
        outputs: "{{ {'count': (blocks.format_rankings.outputs.stdout | fromjson).count if blocks.format_rankings.succeeded else 0} }}"

# =============================================================================
# OUTPUTS
# =============================================================================
outputs:
  rankings:
    type: list
    description: "Files ranked by semantic similarity (for RRF fusion)."
    value: "{{ (blocks.format_rankings.outputs.stdout | fromjson).rankings if blocks.format_rankings.succeeded else [] }}"

  count:
    type: num
    description: "Number of files in rankings."
    value: "{{ (blocks.format_rankings.outputs.stdout | fromjson).count if blocks.format_rankings.succeeded else 0 }}"

  has_results:
    type: bool
    description: "Whether any results were found."
    value: "{{ (blocks.format_rankings.outputs.stdout | fromjson).has_results if blocks.format_rankings.succeeded else false }}"

  query_embedding:
    type: list
    description: "Query embedding vector (for reuse)."
    value: "{{ blocks.query_embed.outputs.embedding if blocks.query_embed.succeeded else [] }}"

  state:
    type: str
    description: "Path to state database (passthrough)."
    value: "{{ inputs.state }}"
