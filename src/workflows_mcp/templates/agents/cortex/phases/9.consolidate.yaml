# =============================================================================
# CORTEX Consolidate Phase
# =============================================================================
#
# Memory consolidation: converts episodic memories into semantic heuristics.
#
# This phase runs asynchronously after SYNTHESIZE completes, processing
# unconsolidated episodes to extract generalizable patterns.
#
# Process:
#   1. Query unconsolidated episodes (including the current one)
#   2. Cluster episodes by category and context features
#   3. For clusters meeting criteria, generate heuristics:
#      - Minimum 3 episodes in cluster
#      - Minimum 70% success rate
#   4. Store heuristics with source episode linkage
#   5. Mark episodes as consolidated
#
# The generated heuristics are used by CATEGORIZE to guide future decisions.
#
# =============================================================================

name: cortex-phase-consolidate
description: "CORTEX Consolidate Phase - async memory consolidation"

tags: [cortex, phase, consolidate, memory, heuristics, learning]

inputs:
  state:
    type: str
    description: Path to SQLite state database.
    required: true

  models:
    type: dict
    description: Model definitions.
    default: {}

  parent_id:
    type: str
    description: Parent task ID.
    default: ""

  features:
    type: dict
    description: Feature toggles.
    default:
      consolidation_enabled: false
      consolidation:
        min_cluster_size: 3
        min_success_rate: 0.7
        async: true

  current_episode_id:
    type: str
    description: ID of the episode just recorded (to include in consolidation).
    default: ""

  profile:
    type: str
    description: LLM profile to use for heuristic generation.
    default: "default"

blocks:
  # ===========================================================================
  # STEP 1: Register Phase Task
  # ===========================================================================
  - id: register_phase
    type: Sql
    description: Register this phase in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: phase
        name: consolidate
        metadata: "{{ {'labels': {'phase_order': 9, 'async': true}} }}"
        inputs: "{{ {'current_episode_id': inputs.current_episode_id} }}"
        status: running

  # ===========================================================================
  # STEP 2: Query unconsolidated episodes
  # ===========================================================================
  - id: query_episodes
    type: Sql
    description: Get all unconsolidated episodes for clustering.
    depends_on: [register_phase]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        SELECT
          ep.id,
          ep.query_hash,
          ep.category,
          ep.context_features,
          ep.actions_taken,
          ep.outcome,
          ep.surprisal_bits,
          ep.learnings,
          ep.created_at
        FROM episodes ep
        WHERE ep.consolidated = 0
        ORDER BY ep.category, ep.created_at DESC
        LIMIT 100

  # ===========================================================================
  # STEP 3: Cluster episodes by category and features
  # ===========================================================================
  - id: cluster_episodes
    type: Shell
    description: Cluster unconsolidated episodes by similarity.
    depends_on: [query_episodes]
    condition: "{{ blocks.query_episodes.succeeded and (blocks.query_episodes.outputs.rows | default([]) | length) > 0 }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        from collections import defaultdict

        episodes_json = os.environ.get('EPISODES', '[]')
        min_cluster_size = int(os.environ.get('MIN_CLUSTER_SIZE', '3'))
        min_success_rate = float(os.environ.get('MIN_SUCCESS_RATE', '0.7'))

        try:
            episodes = json.loads(episodes_json) if episodes_json else []
        except:
            episodes = []

        # =======================================================================
        # CLUSTERING STRATEGY
        # =======================================================================
        # Primary clustering: by category
        # Secondary clustering: by outcome pattern
        # Future: context_features similarity (requires embeddings)
        # =======================================================================

        # Group episodes by category
        by_category = defaultdict(list)
        for ep in episodes:
            category = ep.get('category', 'unknown')
            by_category[category].append(ep)

        clusters = []
        cluster_id = 0

        for category, cat_episodes in by_category.items():
            # Sub-cluster by outcome
            by_outcome = defaultdict(list)
            for ep in cat_episodes:
                outcome = ep.get('outcome', 'unknown')
                by_outcome[outcome].append(ep)

            # Create clusters for each category-outcome combination
            for outcome, outcome_episodes in by_outcome.items():
                cluster_id += 1

                # Calculate success rate
                success_outcomes = {'success', 'partial', 'failure_diagnosed'}
                successes = sum(
                    1 for ep in outcome_episodes
                    if ep.get('outcome', '') in success_outcomes
                )
                success_rate = successes / len(outcome_episodes) if outcome_episodes else 0

                # Extract common learnings
                all_learnings = []
                for ep in outcome_episodes:
                    learnings = ep.get('learnings', [])
                    if isinstance(learnings, str):
                        try:
                            learnings = json.loads(learnings)
                        except:
                            learnings = []
                    all_learnings.extend(learnings)

                # Extract common actions
                all_actions = []
                for ep in outcome_episodes:
                    actions = ep.get('actions_taken', [])
                    if isinstance(actions, str):
                        try:
                            actions = json.loads(actions)
                        except:
                            actions = []
                    all_actions.extend(actions)

                # Determine if cluster is eligible for heuristic generation
                eligible = (
                    len(outcome_episodes) >= min_cluster_size and
                    success_rate >= min_success_rate
                )

                clusters.append({
                    'cluster_id': cluster_id,
                    'category': category,
                    'outcome': outcome,
                    'episode_count': len(outcome_episodes),
                    'episode_ids': [ep['id'] for ep in outcome_episodes],
                    'success_rate': round(success_rate, 3),
                    'avg_surprisal': round(
                        sum(float(ep.get('surprisal_bits', 0) or 0) for ep in outcome_episodes) /
                        len(outcome_episodes) if outcome_episodes else 0,
                        3
                    ),
                    'learnings': all_learnings[:10],  # Limit for processing
                    'actions': all_actions[:10],
                    'eligible': eligible
                })

        # Filter to only eligible clusters for heuristic generation
        eligible_clusters = [c for c in clusters if c['eligible']]

        result = {
            'clusters': clusters,
            'eligible_clusters': eligible_clusters,
            'total_clusters': len(clusters),
            'eligible_count': len(eligible_clusters),
            'total_episodes': len(episodes)
        }

        print(json.dumps(result))
        EOF
      env:
        EPISODES: "{{ blocks.query_episodes.outputs.rows | tojson }}"
        MIN_CLUSTER_SIZE: "{{ inputs.features.consolidation.min_cluster_size | default(3) }}"
        MIN_SUCCESS_RATE: "{{ inputs.features.consolidation.min_success_rate | default(0.7) }}"

  # ===========================================================================
  # STEP 4: Generate heuristics from eligible clusters
  # ===========================================================================
  - id: generate_heuristics
    type: Shell
    description: Generate heuristic rules from eligible clusters.
    depends_on: [cluster_episodes]
    condition: "{{ blocks.cluster_episodes.succeeded and (blocks.cluster_episodes.outputs.stdout | fromjson).eligible_count > 0 }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        clusters_json = os.environ.get('CLUSTERS', '[]')

        try:
            data = json.loads(clusters_json) if clusters_json else {}
            eligible_clusters = data.get('eligible_clusters', [])
        except:
            eligible_clusters = []

        # =======================================================================
        # HEURISTIC GENERATION
        # =======================================================================
        # For each eligible cluster, create a heuristic rule.
        # The rule summarizes the common pattern that led to success.
        # =======================================================================

        heuristics = []

        for cluster in eligible_clusters:
            category = cluster.get('category', 'unknown')
            outcome = cluster.get('outcome', 'unknown')
            episode_count = cluster.get('episode_count', 0)
            success_rate = cluster.get('success_rate', 0)
            learnings = cluster.get('learnings', [])
            actions = cluster.get('actions', [])

            # Build rule description
            rule_parts = []

            # Category-specific prefix
            rule_parts.append(f"For {category} tasks")

            # Outcome pattern
            if outcome == 'success':
                rule_parts.append("that succeeded")
            elif outcome == 'failure_diagnosed':
                rule_parts.append("where failures were diagnosed")

            # Extract key learnings
            learning_summaries = []
            for learning in learnings[:3]:
                if isinstance(learning, dict):
                    content = learning.get('content', '')
                    if isinstance(content, str) and content:
                        learning_summaries.append(content[:100])
                    elif isinstance(content, dict):
                        learning_summaries.append(str(content.get('pattern', ''))[:100])

            if learning_summaries:
                rule_parts.append(f": {'; '.join(learning_summaries)}")

            # Extract common action patterns
            action_types = set()
            for action in actions[:5]:
                if isinstance(action, dict):
                    cap = action.get('capability', action.get('type', ''))
                    if cap:
                        action_types.add(cap.split('-')[-1] if '-' in cap else cap)

            if action_types:
                rule_parts.append(f" [common actions: {', '.join(list(action_types)[:3])}]")

            rule = ''.join(rule_parts)

            # Calculate initial confidence from success rate
            confidence = min(0.95, success_rate * 0.9)  # Cap at 0.95

            heuristics.append({
                'rule': rule,
                'confidence': round(confidence, 3),
                'category_filter': category,
                'source_episodes': cluster.get('episode_ids', []),
                'times_applied': 0,
                'times_succeeded': 0,
                'status': 'active',
                'metadata': {
                    'episode_count': episode_count,
                    'success_rate': success_rate,
                    'outcome_pattern': outcome
                }
            })

        result = {
            'heuristics': heuristics,
            'count': len(heuristics)
        }

        print(json.dumps(result))
        EOF
      env:
        CLUSTERS: "{{ blocks.cluster_episodes.outputs.stdout }}"

  # ===========================================================================
  # STEP 5: Store generated heuristics
  # ===========================================================================
  - id: store_heuristics
    type: Shell
    description: Store heuristics in database using SQL.
    depends_on: [generate_heuristics]
    condition: "{{ blocks.generate_heuristics.succeeded and (blocks.generate_heuristics.outputs.stdout | fromjson).count > 0 }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        import sqlite3
        import uuid
        from datetime import datetime

        state_path = os.environ.get('STATE', '')
        heuristics_json = os.environ.get('HEURISTICS', '{}')

        if not state_path:
            print(json.dumps({'stored': 0, 'error': 'no_state_path'}))
            exit(0)

        try:
            data = json.loads(heuristics_json) if heuristics_json else {}
            heuristics = data.get('heuristics', [])
        except:
            heuristics = []

        if not heuristics:
            print(json.dumps({'stored': 0}))
            exit(0)

        conn = sqlite3.connect(state_path)
        cursor = conn.cursor()

        stored_ids = []

        for h in heuristics:
            heuristic_id = str(uuid.uuid4())

            cursor.execute("""
                INSERT INTO heuristics (
                    id, rule, confidence, times_applied, times_succeeded,
                    source_episodes, category_filter, status, created_at, updated_at
                ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, datetime('now'), datetime('now'))
            """, (
                heuristic_id,
                h.get('rule', ''),
                h.get('confidence', 0.5),
                h.get('times_applied', 0),
                h.get('times_succeeded', 0),
                json.dumps(h.get('source_episodes', [])),
                h.get('category_filter', ''),
                h.get('status', 'active')
            ))

            stored_ids.append(heuristic_id)

        conn.commit()
        conn.close()

        print(json.dumps({
            'stored': len(stored_ids),
            'heuristic_ids': stored_ids
        }))
        EOF
      env:
        STATE: "{{ inputs.state }}"
        HEURISTICS: "{{ blocks.generate_heuristics.outputs.stdout }}"

  # ===========================================================================
  # STEP 6: Mark episodes as consolidated
  # ===========================================================================
  - id: mark_consolidated
    type: Sql
    description: Mark processed episodes as consolidated.
    depends_on:
      - block: cluster_episodes
        required: true
      - block: store_heuristics
        required: false
    condition: "{{ blocks.cluster_episodes.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        UPDATE episodes
        SET consolidated = 1,
            consolidated_at = datetime('now')
        WHERE consolidated = 0

  # ===========================================================================
  # STEP 7: Generate consolidation summary
  # ===========================================================================
  - id: build_summary
    type: Shell
    description: Build summary of consolidation results.
    depends_on:
      - block: cluster_episodes
        required: false
      - block: generate_heuristics
        required: false
      - block: store_heuristics
        required: false
      - block: mark_consolidated
        required: false
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        cluster_data = os.environ.get('CLUSTER_DATA', '{}')
        heuristics_data = os.environ.get('HEURISTICS_DATA', '{}')
        stored_data = os.environ.get('STORED_DATA', '{}')

        try:
            clusters = json.loads(cluster_data) if cluster_data else {}
        except:
            clusters = {}

        try:
            heuristics = json.loads(heuristics_data) if heuristics_data else {}
        except:
            heuristics = {}

        try:
            stored = json.loads(stored_data) if stored_data else {}
        except:
            stored = {}

        summary = {
            'episodes_processed': clusters.get('total_episodes', 0),
            'clusters_formed': clusters.get('total_clusters', 0),
            'eligible_clusters': clusters.get('eligible_count', 0),
            'heuristics_generated': heuristics.get('count', 0),
            'heuristics_stored': stored.get('stored', 0),
            'heuristic_ids': stored.get('heuristic_ids', []),
            'success': True
        }

        print(json.dumps(summary))
        EOF
      env:
        CLUSTER_DATA: "{{ blocks.cluster_episodes.outputs.stdout if blocks.cluster_episodes.succeeded else '{}' }}"
        HEURISTICS_DATA: "{{ blocks.generate_heuristics.outputs.stdout if blocks.generate_heuristics.succeeded else '{}' }}"
        STORED_DATA: "{{ blocks.store_heuristics.outputs.stdout if blocks.store_heuristics.succeeded else '{}' }}"

  # ===========================================================================
  # STEP 8: Mark Phase Complete
  # ===========================================================================
  - id: track_done
    type: Sql
    description: Mark phase complete.
    depends_on:
      - block: register_phase
        required: true
      - block: build_summary
        required: true
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        UPDATE tasks
        SET status = 'done',
            outputs = ?,
            updated_at = datetime('now')
        WHERE id = ?
      params:
        - "{{ blocks.build_summary.outputs.stdout }}"
        - "{{ (blocks.register_phase.outputs.rows | default([{}]))[0].id | default('') }}"

# =============================================================================
# OUTPUTS
# =============================================================================
outputs:
  summary:
    type: dict
    description: "Consolidation summary."
    value: "{{ blocks.build_summary.outputs.stdout | fromjson if blocks.build_summary.succeeded else {} }}"

  episodes_processed:
    type: num
    description: "Number of episodes consolidated."
    value: "{{ (blocks.build_summary.outputs.stdout | fromjson).episodes_processed | default(0) if blocks.build_summary.succeeded else 0 }}"

  heuristics_generated:
    type: num
    description: "Number of new heuristics generated."
    value: "{{ (blocks.build_summary.outputs.stdout | fromjson).heuristics_generated | default(0) if blocks.build_summary.succeeded else 0 }}"

  heuristics_stored:
    type: num
    description: "Number of heuristics stored."
    value: "{{ (blocks.build_summary.outputs.stdout | fromjson).heuristics_stored | default(0) if blocks.build_summary.succeeded else 0 }}"

  heuristic_ids:
    type: list
    description: "IDs of newly created heuristics."
    value: "{{ (blocks.build_summary.outputs.stdout | fromjson).heuristic_ids | default([]) if blocks.build_summary.succeeded else [] }}"

  state:
    type: str
    description: "Path to state database."
    value: "{{ inputs.state }}"
