# CORTEX Reason Phase
# Receives execution_result from EXECUTE phase.
# This means REASON now analyzes ACTUAL command output, not just file contents.
#
# Compares actual outcome against prediction (expectation).
# Calculates surprisal to determine if learning is needed (DIAGNOSE action).
#
# Example flow:
#   1. CATEGORIZE: predicted "diagnosed" with 60% probability
#   2. EXECUTE: runs `pytest`, captures "5 tests failed in auth.py"
#   3. REASON: analyzes output, compares to expectation
#   4. Result: surprisal = 0.74 bits (expected), or 2.3 bits (surprising!)

name: cortex-phase-reason
description: "CORTEX Reason Phase - analyzes evidence with surprisal calculation"

tags: [cortex, phase, reason, predictive-coding]

inputs:
  prompt:
    type: str
    description: The original query.
    required: true

  system:
    type: str
    description: System prompt override.
    default: |
      You are the reasoning module of a recursive cognitive system (CORTEX).
      Your role is to analyze evidence and synthesize unified conclusions.

      KEY FEATURE: You receive ACTUAL command execution results, not just file contents.
      When analyzing, prioritize the execution output - it shows what ACTUALLY happened.

      KEY FEATURE: You track expectations vs reality. When outcomes surprise the model
      (high surprisal), you should note what was unexpected and why, to enable learning.

      Rules:
      1. Cite evidence - Every finding MUST reference specific sources
      2. Rate severity - critical, high, medium, low, info
      3. Prioritize execution results - they show reality
      4. Set actions_needed based on execution failures
      5. When surprised, identify what was unexpected and suggest diagnosis

  context:
    type: dict
    description: Context including gather_result, category, execution_result.
    default: {}

  pipeline_config:
    type: dict
    description: Pipeline configuration.
    default: {}

  memory:
    type: list
    description: List of memory keys to retrieve.
    default: []

  synthesis:
    type: list
    description: Accumulated synthesis from previous investigations.
    default: []

  depth:
    type: num
    description: Current recursion depth.
    default: 0

  max_depth:
    type: num
    description: Maximum recursion depth.
    default: 5

  iterations:
    type: num
    description: Current task completion iteration.
    default: 0

  max_iterations:
    type: num
    description: Maximum continuation attempts.
    default: 3

  confidence_threshold:
    type: num
    description: Minimum confidence to skip investigation.
    default: 0.7

  state:
    type: str
    description: Path to SQLite state database.
    default: ""

  models:
    type: dict
    description: Model definitions for CRUD operations.
    default: {}

  parent_id:
    type: str
    description: Parent cell's task ID.
    default: ""

  profile:
    type: str
    description: LLM profile to use.
    default: "default"

  capabilities:
    type: list
    description: Available capabilities registry.
    default: []

  permissions:
    type: dict
    description: Permission flags.
    default: { "read": true, "write": false, "execute": false }

  features:
    type: dict
    description: feature toggles.
    default:
      expectations_enabled: false

  expectation_id:
    type: str
    description: ID of expectation from CATEGORIZE (predictive coding).
    default: ""

  context_hash:
    type: str
    description: Context hash for clustering similar situations.
    default: ""

  expectation:
    type: dict
    description: The expectation formed during CATEGORIZE (predictive coding).
    default: {}

blocks:
  - id: register_phase
    type: Sql
    description: Register this phase in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: phase
        name: reason
        metadata: |
          {
            'labels': {
              'phase_order': 4,
              'goal': {{ get(inputs.pipeline_config.reason, 'goal', 'explain') }},
              'version': 'v1'
            }
          }
        inputs: "{{ {'prompt': inputs.prompt} }}"
        status: running
        depth: "{{ inputs.depth }}"

  - id: retrieve_evidence
    type: Sql
    description: Query all gathered evidence from memory.
    depends_on: [register_phase]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.memory }}"
      op: select
      columns: [namespace, key, value, metadata]
      where:
        namespace: { in: [content, outline, search] }
      order: [namespace:asc, key:asc]
      limit: 100

  - id: get_child_cells
    type: Sql
    description: Query all child cortex-cell syntheses from DB.
    depends_on: [register_phase]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: select
      columns: [id, inputs, outputs]
      where:
        parent_id: "{{ inputs.parent_id }}"
        kind: cell
        status: done
      order: [created_at:asc]

  - id: attempt
    type: LLMCall
    description: Analyze evidence AND execution results.
    depends_on:
      - block: register_phase
        required: true
      - block: retrieve_evidence
        required: false
      - block: get_child_cells
        required: false
    inputs:
      profile: "{{ inputs.profile }}"
      timeout: 600
      system_instructions: "{{ inputs.system }}"
      prompt: |
        # Reasoning Phase (CORTEX v2)

        ## Original Query
        {{ inputs.prompt }}

        ## Goal: {{ inputs.pipeline_config.reason.goal | default('explain') | upper }}

        {% if inputs.features.expectations_enabled | default(false) and inputs.expectation %}
        ## ⚡ EXPECTATION (Predictive Coding)

        Before seeing the evidence, the model predicted:
        - **Expected Outcome**: {{ inputs.expectation.prediction.outcome | default('unknown') }}
        - **Predicted Probability**: {{ (inputs.expectation.prediction.probability | default(0.5) * 100) | round }}%
        - **Basis**: {{ inputs.expectation.basis | default('No basis recorded') }}

        Compare your analysis against this expectation. If the actual outcome differs significantly,
        note what was surprising and why - this helps the system learn.
        {% endif %}

        {% if inputs.context.execution_result and inputs.context.execution_result.executed %}
        ## ⚡ EXECUTION RESULTS (ACTUAL COMMAND OUTPUT)

        **This is what ACTUALLY happened when the command ran. Prioritize this over file contents.**

        - **Command**: `{{ inputs.context.execution_result.command }}`
        - **Exit Code**: {{ inputs.context.execution_result.exit_code }}
        - **Success**: {{ inputs.context.execution_result.success }}
        - **Duration**: {{ inputs.context.execution_result.duration_ms }}ms
        {% if inputs.context.execution_result.summary %}
        - **Summary**: {{ inputs.context.execution_result.summary | tojson }}
        {% endif %}

        ### Command Output
        ```
        {{ inputs.context.execution_result.stdout | truncate(15000) }}
        ```

        {% if not inputs.context.execution_result.success %}
        **⚠️ COMMAND FAILED - Analyze the output above to determine root cause and fix.**
        {% endif %}
        {% endif %}

        {% if inputs.context %}
        ## Context
        {{ inputs.context | tojson }}
        {% endif %}

        {% if inputs.context.gather_result %}
        ## Gather Phase Summary
        - **Operations executed**: {{ inputs.context.gather_result.operations_executed | default(0) }}
        - **Questions spawned**: {{ inputs.context.gather_result.questions_spawned | default(0) }}
        {% endif %}

        {% if blocks.retrieve_evidence.succeeded and blocks.retrieve_evidence.outputs.row_count > 0 %}
        ## Evidence ({{ blocks.retrieve_evidence.outputs.row_count }} items)
        {% for item in blocks.retrieve_evidence.outputs.rows %}
        - **{{ item.key }}** ({{ item.namespace }})
        {% endfor %}
        {% endif %}

        {% if blocks.get_child_cells.succeeded and blocks.get_child_cells.outputs.rows | length > 0 %}
        ## Child Cell Syntheses ({{ blocks.get_child_cells.outputs.rows | length }} cells)
        {% for child in blocks.get_child_cells.outputs.rows %}
        ### Child {{ loop.index }}
        - Query: {{ child.inputs.prompt | default('N/A') }}
        - Answer: {{ child.outputs.response | default('N/A') }}
        {% endfor %}
        {% endif %}

        ## Instructions

        {% if inputs.context.execution_result and inputs.context.execution_result.executed and not inputs.context.execution_result.success %}
        **EXECUTION FAILED** - Your primary task is to:
        1. Analyze why the command failed (from output above)
        2. Identify the root cause
        3. Set `actions_needed: true` with specific fix recommendations
        {% endif %}

        Apply the **{{ inputs.pipeline_config.reason.goal | default('explain') | upper }}** goal:

        {% if inputs.pipeline_config.reason.goal == 'diagnose' %}
        **DIAGNOSE MODE**: Find root cause of the issue.
        - Trace the problem from execution output
        - Cross-reference with gathered evidence
        - Identify the specific failing component
        {% elif inputs.pipeline_config.reason.goal == 'evaluate' %}
        **EVALUATE MODE**: Assess quality based on execution results.
        - Use test/lint results as primary quality indicators
        - Identify specific issues from command output
        {% else %}
        - Synthesize findings from all sources
        - Set actions_needed based on what was discovered
        {% endif %}

        ## Response

        Provide synthesis with findings, response, and whether actions are needed.

      response_schema:
        type: object
        properties:
          synthesis:
            type: object
            properties:
              findings:
                type: array
                items:
                  type: object
                  properties:
                    claim:
                      type: string
                    severity:
                      type: string
                      enum: [critical, high, medium, low, info]
                    confidence:
                      type: number
                    evidence_chain:
                      type: array
                      items:
                        type: string
                  required: [claim, severity, confidence]
              summary:
                type: string
              risk_level:
                type: string
                enum: [critical, high, medium, low, info]
              recommendations:
                type: array
                items:
                  type: string
              actions_needed:
                type: boolean
              evidence_needed:
                type: array
                items:
                  type: string
                description: "Specific evidence gaps (for DECIDE refine action)"
              surprised:
                type: boolean
                description: "Whether the outcome was surprising (didn't match expectation)"
              surprise_explanation:
                type: string
                description: "If surprised, explain what was unexpected and why"
              diagnosis_questions:
                type: array
                items:
                  type: string
                description: "Questions to investigate if outcome was surprising"
            required: [findings, summary, risk_level, actions_needed]
          response:
            type: string
          config_override:
            type: object
            properties:
              act:
                type: object
                properties:
                  trigger:
                    type: string
                    enum: [never, findings_require, always]
                  mode:
                    type: string
                    enum: [none, report, implement, fix, verify]
          confidence:
            type: number
            minimum: 0
            maximum: 1
          uncertainty:
            type: object
            properties:
              exists:
                type: boolean
              question:
                type: string
            required: [exists]
        required: [synthesis, response, confidence, uncertainty]

  - id: store_llm_call
    type: Sql
    description: Store LLM call details.
    depends_on:
      - block: attempt
        required: false
    condition: "{{ inputs.state | trim != '' and blocks.attempt.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.llm_call }}"
      op: insert
      data:
        task_id: "{{ get(blocks.register_phase.outputs, 'rows.0.id', '') }}"
        phase: reason
        system_instructions: "{{ inputs.system | default('') }}"
        prompt: "{{ inputs.prompt }}"
        response: "{{ blocks.attempt.outputs.response }}"
        model: "{{ get(blocks.attempt.outputs.metadata, 'model', 'unknown') }}"
        prompt_tokens: "{{ get(blocks.attempt.outputs, 'metadata.usage.prompt_tokens', 0) }}"
        completion_tokens: "{{ get(blocks.attempt.outputs, 'metadata.usage.completion_tokens', 0) }}"
        duration_ms: "{{ get(blocks.attempt.metadata, 'duration_ms', 0) }}"

  - id: store_facts
    type: Sql
    description: Store synthesized findings as facts (requires non-empty evidence).
    depends_on: [attempt]
    condition: |
      {{ get(blocks.attempt.outputs.response.synthesis, 'findings', []) | length > 0 }}
    for_each: "{{ blocks.attempt.outputs.response.synthesis.findings | selectattr('evidence_chain') | list }}"
    for_each_mode: parallel
    max_parallel: 10
    continue_on_error: true
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.fact }}"
      op: insert
      data:
        claim: "{{ each.value.claim }}"
        evidence_type: "{{ 'direct' if each.value.confidence >= 0.9 else 'inferred' }}"
        severity: "{{ each.value.severity | default('info') }}"
        confidence: "{{ each.value.confidence }}"
        grounding: "{{ each.value.evidence_chain | tojson }}"
        task_id: "{{ get(blocks.register_phase.outputs, 'rows.0.id', '') }}"
        status: active

  - id: log_ungrounded
    type: Shell
    description: Log findings that lack evidence grounding.
    depends_on: [attempt]
    condition: "{{ get(blocks.attempt.outputs.response.synthesis, 'findings', []) | rejectattr('evidence_chain') | list | length > 0 }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        import sys

        findings_json = os.environ.get('FINDINGS', '[]')
        try:
            findings = json.loads(findings_json)
        except:
            findings = []

        ungrounded = [f for f in findings if not f.get('evidence_chain')]
        if ungrounded:
            print(f"WARNING: {len(ungrounded)} finding(s) without evidence grounding (not stored as facts):", file=sys.stderr)
            for f in ungrounded:
                print(f"  - {f.get('claim', 'N/A')[:100]}...", file=sys.stderr)

        print(json.dumps({'ungrounded_count': len(ungrounded), 'claims': [f.get('claim', '') for f in ungrounded]}))
        EOF
      env:
        FINDINGS: "{{ get(blocks.attempt.outputs.response.synthesis, 'findings', []) | tojson }}"

  - id: determine_outcome
    type: Shell
    description: Determine actual outcome for surprisal calculation.
    depends_on: [attempt]
    condition: "{{ inputs.features.expectations_enabled | default(false) and inputs.expectation_id | trim != '' and blocks.attempt.succeeded }}"
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        synthesis_json = os.environ.get('SYNTHESIS', '{}')
        execution_json = os.environ.get('EXECUTION', '{}')
        category = os.environ.get('CATEGORY', 'understanding')

        try:
            synthesis = json.loads(synthesis_json)
        except:
            synthesis = {}

        try:
            execution = json.loads(execution_json)
        except:
            execution = {}

        # Determine actual outcome based on synthesis and execution results
        actions_needed = synthesis.get('actions_needed', False)
        risk_level = synthesis.get('risk_level', 'info')
        execution_success = execution.get('success', True)
        executed = execution.get('executed', False)

        # Map to outcome categories matching CATEGORIZE predictions
        if category == 'existence':
            outcome = 'verified'
        elif category == 'understanding':
            outcome = 'explained'
        elif category == 'discovery':
            outcome = 'enumerated'
        elif category == 'quality':
            if executed and not execution_success:
                outcome = 'issues_found'
            elif actions_needed:
                outcome = 'issues_found'
            else:
                outcome = 'assessed'
        elif category == 'planning':
            outcome = 'planned'
        elif category == 'debugging':
            if actions_needed or (executed and not execution_success):
                outcome = 'diagnosed'
            else:
                outcome = 'unclear'  # Surprising: expected to find issue but didn't
        elif category == 'action':
            if actions_needed:
                outcome = 'executed'
            else:
                outcome = 'blocked'  # Surprising: couldn't proceed with action
        else:
            outcome = 'completed'

        result = {
            'outcome': outcome,
            'actions_needed': actions_needed,
            'risk_level': risk_level,
            'execution_success': execution_success,
            'category': category
        }

        print(json.dumps(result))
        EOF
      env:
        SYNTHESIS: "{{ blocks.attempt.outputs.response.synthesis | tojson }}"
        EXECUTION: "{{ get(inputs.context, 'execution_result', {}) | tojson }}"
        CATEGORY: "{{ get(inputs.context, 'category', 'understanding') }}"

  - id: compute_surprisal
    type: Workflow
    description: Calculate surprisal by comparing prediction to actual outcome.
    depends_on: [determine_outcome]
    condition: "{{ blocks.determine_outcome.succeeded }}"
    inputs:
      workflow: cortex-compute-surprisal
      inputs:
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        expectation_id: "{{ inputs.expectation_id }}"
        actual_outcome: "{{ blocks.determine_outcome.outputs.stdout }}"
        context_hash: "{{ inputs.context_hash }}"

  - id: track_done
    type: Sql
    description: Mark phase complete.
    depends_on:
      - block: register_phase
        required: true
      - block: attempt
        required: false
      - block: store_facts
        required: false
      - block: log_ungrounded
        required: false
      - block: compute_surprisal
        required: false
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ get(blocks.register_phase.outputs, 'rows.0.id', '') }}"
      data:
        status: "{{ 'done' if blocks.attempt.succeeded else 'failed' }}"
        outputs: |
          {
            'goal': {{ get(inputs.pipeline_config, 'reason.goal', 'explain') }},
            'success': {{ blocks.attempt.succeeded }},
            'had_execution_results': {{ get(inputs.context, 'execution_result.executed', false) }},
            'facts_stored': {{ get(blocks.store_facts.metadata, 'count', 0) }},
            {% if blocks.log_ungrounded.succeeded %}
              'ungrounded_findings': {{ get(blocks.log_ungrounded.outputs.stdout, 'ungrounded_count', 0) }}
            {% else %}
              'ungrounded_findings': 0
            {% endif %}
          }

outputs:
  synthesis:
    type: dict
    description: "Unified synthesis including execution analysis."
    value: "{{ blocks.attempt.outputs.response.synthesis if blocks.attempt.succeeded else {} }}"

  response:
    type: str
    description: "Direct response to query."
    value: "{{ blocks.attempt.outputs.response.response if blocks.attempt.succeeded else 'Analysis failed' }}"

  config_override:
    type: dict
    description: "Config override for downstream phases."
    value: "{{ blocks.attempt.outputs.response.config_override | default({}) }}"

  surprisal:
    type: dict
    description: "Surprisal calculation result (predictive coding)."
    value: "{{ blocks.compute_surprisal.outputs.result if blocks.compute_surprisal.succeeded else {} }}"

  surprisal_bits:
    type: num
    description: "Surprisal in bits (0 = expected, >2 = surprising)."
    value: "{{ blocks.compute_surprisal.outputs.surprisal_bits | default(0) }}"

  trigger_diagnose:
    type: bool
    description: "Whether surprisal is high enough to trigger DIAGNOSE."
    value: "{{ blocks.compute_surprisal.outputs.trigger_diagnose | default(false) }}"

  diagnosis_questions:
    type: list
    description: "Questions to investigate if outcome was surprising."
    value: "{{ blocks.attempt.outputs.response.synthesis.diagnosis_questions | default([]) if blocks.attempt.succeeded else [] }}"

  surprise_explanation:
    type: str
    description: "Explanation of what was surprising about the outcome."
    value: "{{ blocks.attempt.outputs.response.synthesis.surprise_explanation | default('') if blocks.attempt.succeeded else '' }}"

  state:
    type: str
    description: "Path to state database."
    value: "{{ inputs.state }}"
