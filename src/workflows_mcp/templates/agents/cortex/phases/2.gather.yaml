# =============================================================================
# CORTEX Gather Phase
# =============================================================================
#
# Enhancements:
#   - RRF salience filtering for context budget
#   - BM25 keyword search for file ranking
#   - Semantic vector search for embedding-based similarity
#   - Context budget enforcement (default 60% of context window)
#   - References to workflows (cortex-cell, cortex-phase-gather)
#   - Iteration support for DECIDE → REFINE loop
#
# When salience_enabled=true:
#   1. Run keyword search (BM25) to rank files by query relevance
#   2. Run semantic search (embeddings) to rank files by semantic similarity
#   3. Apply RRF fusion to combine both rankings
#   4. Enforce context budget - include only top-ranked files
#   5. Pass prioritized file list to LLM for focused gathering
#
# =============================================================================

name: cortex-phase-gather
description: "CORTEX Gather Phase - collects evidence with RRF salience filtering and cell spawning"

tags: [cortex, phase, gather, salience]

inputs:
  prompt:
    type: str
    description: The prompt to gather evidence for.
    required: true

  system:
    type: str
    description: System prompt override.
    default: |
      You are the evidence gathering module of a recursive cognitive system (CORTEX).
      Your role is to plan and execute data collection to answer queries.

      Be efficient - gather only what's needed based on the configured depth level.
      Store important files to memory for use by later phases.

  context:
    type: dict
    description: Context including repo_path, category, etc.
    default: {}

  pipeline_config:
    type: dict
    description: Pipeline configuration from CATEGORIZE.
    default: {}

  synthesis:
    type: list
    description: Accumulated synthesis from previous investigations.
    default: []

  depth:
    type: num
    description: Current recursion depth.
    default: 0

  max_depth:
    type: num
    description: Maximum recursion depth.
    default: 5

  iterations:
    type: num
    description: Current task completion iteration.
    default: 0

  max_iterations:
    type: num
    description: Maximum continuation attempts.
    default: 3

  confidence_threshold:
    type: num
    description: Minimum confidence to skip investigation.
    default: 0.9

  state:
    type: str
    description: Path to SQLite state database.
    default: ""

  models:
    type: dict
    description: Model definitions for CRUD operations.
    default: {}

  parent_id:
    type: str
    description: Parent task ID for task tree.
    default: ""

  profile:
    type: str
    description: LLM profile to use.
    default: "default"

  capabilities:
    type: list
    description: Available capabilities registry.
    default: []

  permissions:
    type: dict
    description: Permission flags.
    default: { "read": true, "write": false, "execute": false }

  # v2 addition: evidence targets from DECIDE → REFINE loop
  evidence_targets:
    type: list
    description: Specific evidence to gather (from DECIDE refine action).
    default: []

  # Embedding profile for semantic search
  embedding_profile:
    type: str
    description: LLM profile for embedding generation.
    default: "embedding"

  # addition: feature toggles
  features:
    type: dict
    description: feature toggles.
    default:
      salience_enabled: false
      semantic_enabled: true  # Enable semantic vector search when salience is enabled
      context_budget:
        max_tokens: 60000 # ~60% of 100k context window
        max_files: 50 # Maximum files to include

blocks:
  # ===========================================================================
  # STEP 1: Register Phase Task
  # ===========================================================================
  - id: register
    type: Sql
    description: Register this phase in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: phase
        name: gather
        metadata: "{{ {'labels': {'phase_order': 2, 'strategy': inputs.pipeline_config.gather.depth | default('focused'), 'version': 'v2'}} }}"
        inputs: "{{ inputs }}"
        status: running
        depth: "{{ inputs.depth }}"

  # ===========================================================================
  # STEP 2: Salience Filtering (when salience_enabled=true)
  # ===========================================================================
  # Runs BM25 keyword search to rank files by relevance to the query.
  # Future: Add semantic search and other ranking signals.
  # RRF fusion combines rankings into a unified priority list.
  # Context budget limits included files to fit within LLM context window.
  # ===========================================================================

  - id: keyword_search
    type: Workflow
    description: "Salience: BM25 keyword search for file ranking."
    depends_on: [register]
    condition: "{{ inputs.features.salience_enabled | default(false) }}"
    inputs:
      workflow: cortex-keyword-search
      inputs:
        query: "{{ inputs.prompt }}"
        base_path: "{{ inputs.context.repo_path | default('.') }}"
        max_results: "{{ inputs.features.context_budget.max_files | default(50) }}"
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        task_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

  - id: semantic_search
    type: Workflow
    description: "Salience: Semantic vector search for embedding-based similarity."
    depends_on: [keyword_search]
    condition: "{{ inputs.features.salience_enabled | default(false) and inputs.features.semantic_enabled | default(true) and blocks.keyword_search.succeeded }}"
    inputs:
      workflow: cortex-semantic-search
      inputs:
        query: "{{ inputs.prompt }}"
        files: "{{ blocks.keyword_search.outputs.ranked_files | map(attribute='path') | list }}"
        profile: "{{ inputs.embedding_profile }}"
        max_results: "{{ inputs.features.context_budget.max_files | default(50) }}"
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

  - id: rrf_fusion
    type: Workflow
    description: "Salience: RRF fusion to combine keyword and semantic rankings."
    depends_on:
      - block: keyword_search
        required: true
      - block: semantic_search
        required: false
    condition: "{{ blocks.keyword_search.succeeded }}"
    inputs:
      workflow: cortex-rrf-fusion
      inputs:
        rankings: >-
          {{
            [
              {'source': 'keyword', 'weight': 1.0, 'files': blocks.keyword_search.outputs.ranked_files | default([])},
              {'source': 'semantic', 'weight': 1.0, 'files': blocks.semantic_search.outputs.rankings | default([])}
            ] if blocks.semantic_search.succeeded
            else [
              {'source': 'keyword', 'weight': 1.0, 'files': blocks.keyword_search.outputs.ranked_files | default([])}
            ]
          }}
        k: 60
        context_budget: "{{ inputs.features.context_budget.max_tokens | default(60000) }}"
        max_results: "{{ inputs.features.context_budget.max_files | default(50) }}"
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        task_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

  # ===========================================================================
  # STEP 3: Build Schema & Recall
  # ===========================================================================

  - id: build_schema
    type: Workflow
    description: Generate response schema with enforced capability enum.
    depends_on: [register]
    inputs:
      workflow: cortex-internal-schema-operations
      inputs:
        capabilities: "{{ inputs.capabilities }}"
        phase: "gather"
        include_spawn: true
        include_completion: false
        include_config_override: true

  - id: recall
    type: Sql
    description: Query existing evidence from memory.
    depends_on: [register]
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        SELECT namespace as evidence_type, key, value, metadata,
               json_extract(metadata, '$.byte_size') as byte_size
        FROM memory
        WHERE namespace IN ('content', 'outline', 'search')
        ORDER BY namespace, key
        LIMIT 200

  # ===========================================================================
  # STEP 4: Plan Evidence Gathering
  # ===========================================================================

  - id: plan
    type: LLMCall
    description: Decide what evidence to gather.
    depends_on:
      - block: register
        required: true
      - block: build_schema
        required: true
      - block: recall
        required: false
      - block: keyword_search
        required: false
      - block: semantic_search
        required: false
      - block: rrf_fusion
        required: false
    inputs:
      profile: "{{ inputs.profile }}"
      timeout: 600
      system_instructions: "{{ inputs.system }}"
      prompt: |
        # Evidence Gathering (CORTEX)

        ## Query
        {{ inputs.prompt }}

        ## Gather Depth: {{ get(inputs.pipeline_config, 'gather', {}).depth | default('focused') | upper }}

        {% if inputs.evidence_targets | length > 0 %}
        ## Specific Evidence Targets (from DECIDE refine)
        These are specific pieces of evidence that were requested:
        {% for target in inputs.evidence_targets %}
        - {{ target }}
        {% endfor %}
        **Focus on gathering these specific items.**
        {% endif %}

        {% if inputs.context %}
        ## Context
        {{ inputs.context | tojson }}
        {% endif %}

        {% if blocks.rrf_fusion.succeeded %}
        ## Salience Prioritized Files (RRF ranked)
        These files have been ranked by relevance to your query. Prioritize reading these:

        **Context Budget**: {{ blocks.rrf_fusion.outputs.tokens_used | default(0) }} / {{ inputs.features.context_budget.max_tokens | default(60000) }} tokens

        {% for file in blocks.rrf_fusion.outputs.fused_ranking[:20] %}
        {{ loop.index }}. `{{ file.path }}` (RRF: {{ file.rrf_score }}, sources: {{ file.sources | join(', ') }})
        {% endfor %}
        {% if blocks.rrf_fusion.outputs.count > 20 %}
        ... and {{ blocks.rrf_fusion.outputs.count - 20 }} more files
        {% endif %}

        **IMPORTANT**: Focus on these prioritized files. They have been ranked by relevance using {{ 'BM25 keyword matching + semantic embeddings (RRF fusion)' if blocks.semantic_search.succeeded else 'BM25 keyword matching' }}.
        {% elif blocks.keyword_search.succeeded %}
        ## Keyword-Matched Files (BM25 ranked)
        {% for file in blocks.keyword_search.outputs.ranked_files[:15] %}
        {{ loop.index }}. `{{ file.path }}` (BM25: {{ file.bm25_score }}, terms: {{ file.terms_matched }})
        {% endfor %}
        {% endif %}

        {% if get(blocks.recall.outputs, 'rows', []) | length > 0 %}
        ## Evidence Already Gathered ({{ get(blocks.recall.outputs, 'rows', []) | length }} items)
        {% for item in get(blocks.recall.outputs, 'rows', []) %}
        - [{{ item.evidence_type }}] {{ item.key }}
        {% endfor %}
        {% endif %}

        {% if inputs.synthesis | length > 0 %}
        ## Previous Investigations
        {% for c in inputs.synthesis %}
        ### Investigation {{ loop.index }}
        - Prompt: {{ c.prompt }}
        - Response: {{ c.response }}
        {% endfor %}
        {% endif %}

        ## Depth Instructions

        {% if get(inputs.pipeline_config, 'gather', {}).depth == 'shallow' %}
        **SHALLOW**: 1-2 targeted searches, existence verification only
        {% elif get(inputs.pipeline_config, 'gather', {}).depth == 'focused' %}
        **FOCUSED**: 3-5 searches in relevant directories, key files only
        {% elif get(inputs.pipeline_config, 'gather', {}).depth == 'broad' %}
        **BROAD**: Multiple search patterns, cover all relevant directories
        {% elif get(inputs.pipeline_config, 'gather', {}).depth == 'deep' %}
        **DEEP**: Full file contents, related code, tests, configs, docs
        {% endif %}

        ## Available Capabilities

        {% if inputs.capabilities | length > 0 %}
        {% for cap in inputs.capabilities %}
        **{{ cap.name }}** - {{ cap.description }}
        {% for key, schema in cap.inputs.items() %}
          - {{ key }} ({{ schema.type }}{% if schema.required %}, required{% endif %}): {{ schema.description }}
        {% endfor %}
        {% endfor %}
        {% else %}
        No capabilities available.
        {% endif %}

        ## Output

        Provide operations to gather evidence and questions for uncertainty.

      response_schema: "{{ blocks.build_schema.outputs.schema }}"

  # ===========================================================================
  # Store LLM call
  # ===========================================================================
  - id: store_llm_call
    type: Sql
    description: Store LLM call details.
    depends_on:
      - block: plan
        required: false
    condition: "{{ inputs.state | trim != '' and blocks.plan.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.llm_call }}"
      op: insert
      data:
        task_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        phase: gather
        system_instructions: "{{ inputs.system | default('') }}"
        prompt: "{{ inputs.prompt }}"
        response: "{{ blocks.plan.outputs.response }}"
        model: "{{ blocks.plan.outputs.metadata.model | default('unknown') }}"
        prompt_tokens: "{{ blocks.plan.outputs.metadata.usage.prompt_tokens | default(0) }}"
        completion_tokens: "{{ blocks.plan.outputs.metadata.usage.completion_tokens | default(0) }}"
        duration_ms: "{{ blocks.plan.metadata.duration_ms | default(0) }}"

  - id: dispatch
    type: Workflow
    description: Execute capability workflows in parallel.
    depends_on: [plan]
    condition: |
      {{ get(blocks.plan.outputs.response.result, 'operations', []) | length > 0 }}
    for_each: "{{ get(blocks.plan.outputs.response.result, 'operations', []) }}"
    for_each_mode: parallel
    max_parallel: 5
    continue_on_error: true
    inputs:
      workflow: "{{ each.value.capability }}"
      inputs:
        state: "{{ inputs.state }}"
        models: "{{ inputs.models }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        task_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        base_path: "{{ get(inputs.context, 'repo_path', '.') }}"
        patterns: "{{ get(each.value.inputs, 'patterns', []) }}"
        pattern: "{{ get(each.value.inputs, 'pattern', '') }}"
        query: "{{ get(each.value.inputs, 'query', '') }}"
        mode: "{{ get(each.value.inputs, 'mode', 'outline') }}"
        path: "{{ get(each.value.inputs, 'path', '') }}"

  - id: questions
    type: Workflow
    description: Spawn child cortex-cell to answer questions.
    depends_on: [dispatch]
    condition: |
      {{ get(blocks.plan.outputs.response.result, 'questions', []) | length > 0
         and get(inputs.pipeline_config.gather, 'spawn', 'complex_only') != 'never'
         and inputs.depth < inputs.max_depth }}
    for_each: "{{ get(blocks.plan.outputs.response.result, 'questions', []) }}"
    for_each_mode: parallel
    max_parallel: 3
    continue_on_error: true
    inputs:
      workflow: cortex-cell
      inputs:
        prompt: "{{ each.value.prompt }}"
        context: "{{ inputs.context | combine({'parent_prompt': inputs.prompt, 'spawn_reason': each.value.reason}) }}"
        depth: "{{ inputs.depth + 1 }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        state: "{{ inputs.state }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        capabilities: "{{ inputs.capabilities }}"
        permissions: "{{ inputs.permissions }}"

  - id: combine_results
    type: Shell
    description: Combine question results for recursion.
    depends_on: [questions]
    condition: |
      {{ blocks.questions.succeeded
         and get(blocks.questions.metadata, 'count', 0) > 0 }}
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        requests = json.loads(os.environ.get('REQUESTS', '[]'))
        outputs = json.loads(os.environ.get('OUTPUTS', '[]'))
        previous = json.loads(os.environ.get('PREVIOUS', '[]'))

        combined = []
        for i, req in enumerate(requests):
            result = outputs[i] if i < len(outputs) else {}
            combined.append({
                'prompt': req.get('prompt', ''),
                'reason': req.get('reason', ''),
                'response': result.get('response', ''),
                'synthesis': result.get('synthesis', {})
            })

        all_synthesis = previous + combined
        print(json.dumps(all_synthesis))
        EOF
      env:
        REQUESTS: "{{ get(blocks.plan.outputs.response.result, 'questions', []) | tojson }}"
        OUTPUTS: "{{ blocks.questions.outputs | default([]) | tojson }}"
        PREVIOUS: "{{ inputs.synthesis | tojson }}"

  - id: recurse
    type: Workflow
    description: Recurse with accumulated syntheses.
    depends_on: [combine_results]
    condition: |
      {{ blocks.combine_results.succeeded
         and inputs.depth < inputs.max_depth }}
    inputs:
      workflow: cortex-phase-gather
      inputs:
        prompt: "{{ inputs.prompt }}"
        system: "{{ inputs.system }}"
        context: "{{ inputs.context }}"
        pipeline_config: "{{ inputs.pipeline_config }}"
        depth: "{{ inputs.depth + 1 }}"
        max_depth: "{{ inputs.max_depth }}"
        iterations: "{{ inputs.iterations }}"
        max_iterations: "{{ inputs.max_iterations }}"
        confidence_threshold: "{{ inputs.confidence_threshold }}"
        state: "{{ inputs.state }}"
        parent_id: "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"
        profile: "{{ inputs.profile }}"
        capabilities: "{{ inputs.capabilities }}"
        permissions: "{{ inputs.permissions }}"
        synthesis: "{{ blocks.combine_results.outputs.stdout | fromjson }}"
        features: "{{ inputs.features }}"

  # ===========================================================================
  # Track Completion
  # ===========================================================================
  - id: track_done
    type: Sql
    description: Mark phase complete.
    depends_on:
      - block: register
        required: true
      - block: dispatch
        required: false
      - block: questions
        required: false
      - block: combine_results
        required: false
      - block: recurse
        required: false
      - block: keyword_search
        required: false
      - block: semantic_search
        required: false
      - block: rrf_fusion
        required: false
    condition: "{{ blocks.register.succeeded }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      sql: |
        UPDATE tasks
        SET status = 'done',
            outputs = ?,
            updated_at = datetime('now')
        WHERE id = ?
      params:
        - >-
          {{ {
            'strategy': blocks.plan.outputs.response.result.strategy | default(''),
            'operations_executed': blocks.dispatch.metadata.count | default(0),
            'operations_failed': blocks.dispatch.metadata.count_failed | default(0),
            'questions_spawned': blocks.questions.metadata.count | default(0),
            'confidence': blocks.plan.outputs.response.confidence | default(0),
            'depth': inputs.depth,
            'salience_enabled': inputs.features.salience_enabled | default(false),
            'files_ranked': blocks.rrf_fusion.outputs.count | default(0) if blocks.rrf_fusion.succeeded else (blocks.keyword_search.outputs.count | default(0) if blocks.keyword_search.succeeded else 0),
            'context_tokens_used': blocks.rrf_fusion.outputs.tokens_used | default(0) if blocks.rrf_fusion.succeeded else 0
          } | tojson }}
        - "{{ (blocks.register.outputs.rows | default([{}]))[0].id | default('') }}"

# =============================================================================
# OUTPUTS
# =============================================================================
outputs:
  result:
    type: dict
    description: "Gathering result."
    value: |
      {{ blocks.recurse.outputs.result if blocks.recurse.succeeded else {
        'strategy': blocks.plan.outputs.response.result.strategy | default(''),
        'operations_executed': blocks.dispatch.metadata.count | default(0),
        'questions_spawned': blocks.questions.metadata.count | default(0),
        'confidence': blocks.plan.outputs.response.confidence | default(0),
        'depth': inputs.depth
      } }}

  config_override:
    type: dict
    description: "Config override for downstream phases."
    value: |
      {{ blocks.recurse.outputs.config_override if blocks.recurse.succeeded
         else (blocks.plan.outputs.response.config_override | default({})) }}

  # Salience filtering outputs
  salience:
    type: dict
    description: "Salience filtering results."
    value: |
      {{ {
        'enabled': inputs.features.salience_enabled | default(false),
        'semantic_enabled': inputs.features.semantic_enabled | default(true),
        'semantic_used': blocks.semantic_search.succeeded | default(false),
        'files_ranked': blocks.rrf_fusion.outputs.count if blocks.rrf_fusion.succeeded else (blocks.keyword_search.outputs.count if blocks.keyword_search.succeeded else 0),
        'files_excluded': blocks.rrf_fusion.outputs.excluded | length if blocks.rrf_fusion.succeeded else 0,
        'tokens_used': blocks.rrf_fusion.outputs.tokens_used if blocks.rrf_fusion.succeeded else 0,
        'tokens_remaining': blocks.rrf_fusion.outputs.tokens_remaining if blocks.rrf_fusion.succeeded else 0,
        'keywords': blocks.keyword_search.outputs.keywords if blocks.keyword_search.succeeded else [],
        'sources_fused': blocks.rrf_fusion.outputs.sources_fused if blocks.rrf_fusion.succeeded else []
      } }}

  prioritized_files:
    type: list
    description: "Top prioritized files from salience filtering."
    value: "{{ blocks.rrf_fusion.outputs.fused_ranking if blocks.rrf_fusion.succeeded else (blocks.keyword_search.outputs.ranked_files if blocks.keyword_search.succeeded else []) }}"

  state:
    type: str
    description: "Path to state database."
    value: "{{ inputs.state }}"
