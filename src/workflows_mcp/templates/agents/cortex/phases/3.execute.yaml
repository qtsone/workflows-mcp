# CORTEX Execute Phase
# Runs commands between GATHER and REASON so the reasoning phase receives
# actual command output (test results, lint errors) rather than just file contents.
#
# Pure execution - no LLM reasoning:
#   1. Infer command from category + prompt keywords
#   2. Execute command with timeout
#   3. Store results in executions table
#   4. Pass structured output to REASON phase

name: cortex-phase-execute
description: "Execute inferred commands and store results for reasoning"

tags: [cortex, phase, execute]

inputs:
  prompt:
    type: str
    description: The original prompt.
    required: true

  context:
    type: dict
    description: Context including repo_path, category, gather_result.
    default: {}

  pipeline_config:
    type: dict
    description: Pipeline configuration from CATEGORIZE.
    default: {}

  state:
    type: str
    description: Path to SQLite state database.
    required: true

  models:
    type: dict
    description: Model definitions for CRUD operations.
    default: {}

  parent_id:
    type: str
    description: Parent task ID for task tree.
    default: ""

  profile:
    type: str
    description: LLM profile to use.
    default: "default"

  capabilities:
    type: list
    description: Available capabilities.
    default: []

  permissions:
    type: dict
    description: Permission flags.
    default: { "read": true, "write": false, "execute": false }

  depth:
    type: num
    description: Task depth in the tree.
    default: 0

blocks:
  - id: register_phase
    type: Sql
    description: Register this phase in the task tree.
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: insert
      data:
        parent_id: "{{ inputs.parent_id if inputs.parent_id else none }}"
        kind: phase
        name: execute
        metadata: "{'labels': {'phase_order': 3}}"
        inputs: "{'prompt': inputs.prompt}"
        status: running
        depth: "{{ inputs.depth }}"

  - id: infer_command
    type: Shell
    description: Infer appropriate command based on category and context.
    depends_on: [register_phase]
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os

        category = os.environ.get('CATEGORY', 'understanding')
        repo_path = os.environ.get('REPO_PATH', '.')
        prompt = os.environ.get('PROMPT', '').lower()
        execute_type = os.environ.get('EXECUTE_TYPE', 'inferred')
        execute_enabled = os.environ.get('EXECUTE_ENABLED', 'false').lower() == 'true'

        if not execute_enabled:
            print(json.dumps({
                'should_execute': False,
                'reason': 'execute_disabled',
                'command': None
            }))
            exit(0)

        # Infer command based on category and prompt keywords
        command = None
        command_type = None

        # Check for explicit command type
        if execute_type == 'lint_and_test':
            # For quality category, run both lint and test
            command = 'uv run ruff check . && uv run pytest -v'
            command_type = 'lint_and_test'
        elif execute_type == 'test':
            command = 'uv run pytest -v'
            command_type = 'test'
        elif execute_type == 'lint':
            command = 'uv run ruff check .'
            command_type = 'lint'
        elif execute_type == 'build':
            command = 'uv run python -m build'
            command_type = 'build'
        elif execute_type == 'inferred':
            # Infer from prompt keywords
            # Priority 1: Specific tool names (unambiguous)
            if any(kw in prompt for kw in ['ruff', 'flake8', 'pylint', 'eslint']):
                command = 'uv run ruff check .'
                command_type = 'lint'
            elif any(kw in prompt for kw in ['pytest', 'unittest', 'jest', 'vitest']):
                command = 'uv run pytest -v --tb=short'
                command_type = 'test'
            elif any(kw in prompt for kw in ['mypy', 'pyright', 'tsc']):
                command = 'uv run mypy .'
                command_type = 'typecheck'
            # Priority 2: Action words (may be ambiguous)
            elif any(kw in prompt for kw in ['lint', 'linter', 'linting', 'check style']):
                command = 'uv run ruff check .'
                command_type = 'lint'
            elif any(kw in prompt for kw in ['test', 'tests', 'spec', 'specs']):
                command = 'uv run pytest -v --tb=short'
                command_type = 'test'
            elif any(kw in prompt for kw in ['typecheck', 'type check', 'type-check', 'typing']):
                command = 'uv run mypy .'
                command_type = 'typecheck'
            elif any(kw in prompt for kw in ['build', 'compile', 'package']):
                command = 'uv run python -m build'
                command_type = 'build'
            elif any(kw in prompt for kw in ['install', 'setup', 'deps', 'dependencies']):
                command = 'uv sync'
                command_type = 'install'
            # Priority 3: Category-based fallbacks
            elif category == 'debugging':
                command = 'uv run pytest -v --tb=long 2>&1 | head -200'
                command_type = 'test'
            elif category == 'quality':
                command = 'uv run ruff check . && uv run pytest -v --tb=short'
                command_type = 'lint_and_test'
            elif category == 'action':
                # No specific command detected, skip execution
                pass

        should_execute = command is not None

        print(json.dumps({
            'should_execute': should_execute,
            'command': command,
            'command_type': command_type,
            'working_dir': repo_path,
            'reason': 'command_inferred' if should_execute else 'no_command_detected'
        }))
        EOF
      env:
        CATEGORY: "{{ inputs.context.category | default('understanding') }}"
        REPO_PATH: "{{ inputs.context.repo_path | default('.') }}"
        PROMPT: "{{ inputs.prompt }}"
        EXECUTE_TYPE: "{{ inputs.pipeline_config.execute.type | default('inferred') }}"
        EXECUTE_ENABLED: "{{ inputs.pipeline_config.execute.enabled | default(false) }}"

  - id: run_command
    type: Shell
    description: Execute the inferred command.
    depends_on: [infer_command]
    condition: "{{ get(blocks.infer_command.outputs.stdout, 'should_execute', false) }}"
    inputs:
      command: |
        cd "{{ get(blocks.infer_command.outputs.stdout, 'working_dir', '.') }}"

        # Capture start time
        START_MS=$(python3 -c "import time; print(int(time.time() * 1000))")

        # Run the command, capturing stdout and stderr
        {{ get(blocks.infer_command.outputs.stdout, 'command', '') }} 2>&1
        EXIT_CODE=$?

        # Capture end time
        END_MS=$(python3 -c "import time; print(int(time.time() * 1000))")
        DURATION_MS=$((END_MS - START_MS))

        # Output exit code and duration as final lines for parsing
        echo "___EXIT_CODE___:$EXIT_CODE"
        echo "___DURATION_MS___:$DURATION_MS"
      timeout: 300 # 5 minute timeout (seconds)

  - id: parse_result
    type: Shell
    description: Parse command output into structured result.
    depends_on:
      - block: infer_command
        required: true
      - block: run_command
        required: false
    inputs:
      command: |
        python3 << 'EOF'
        import json
        import os
        import re

        should_execute = os.environ.get('SHOULD_EXECUTE', 'false').lower() == 'true'

        if not should_execute:
            print(json.dumps({
                'executed': False,
                'reason': os.environ.get('SKIP_REASON', 'execution_disabled'),
                'command': None,
                'exit_code': None,
                'stdout': None,
                'stderr': None,
                'duration_ms': 0
            }))
            exit(0)

        command = os.environ.get('COMMAND', '')
        command_type = os.environ.get('COMMAND_TYPE', 'unknown')
        working_dir = os.environ.get('WORKING_DIR', '.')
        raw_output = os.environ.get('RAW_OUTPUT', '')

        # Parse exit code and duration from output
        lines = raw_output.split('\n')
        exit_code = 1  # default to failure
        duration_ms = 0
        output_lines = []

        for line in lines:
            if line.startswith('___EXIT_CODE___:'):
                try:
                    exit_code = int(line.split(':')[1])
                except:
                    pass
            elif line.startswith('___DURATION_MS___:'):
                try:
                    duration_ms = int(line.split(':')[1])
                except:
                    pass
            else:
                output_lines.append(line)

        stdout = '\n'.join(output_lines)

        # Determine success based on exit code
        success = exit_code == 0

        # Extract key information based on command type
        summary = {}
        if command_type == 'test':
            # Parse pytest output
            passed_match = re.search(r'(\d+) passed', stdout)
            failed_match = re.search(r'(\d+) failed', stdout)
            error_match = re.search(r'(\d+) error', stdout)

            summary = {
                'passed': int(passed_match.group(1)) if passed_match else 0,
                'failed': int(failed_match.group(1)) if failed_match else 0,
                'errors': int(error_match.group(1)) if error_match else 0
            }
        elif command_type in ['lint', 'lint_and_test']:
            # Parse ruff/lint output
            error_count = len(re.findall(r'^\S+:\d+:\d+:', stdout, re.MULTILINE))
            summary = {'issues': error_count}

        result = {
            'executed': True,
            'success': success,
            'command': command,
            'command_type': command_type,
            'working_dir': working_dir,
            'exit_code': exit_code,
            'stdout': stdout[:50000],  # Limit to 50KB
            'duration_ms': duration_ms,
            'summary': summary
        }

        print(json.dumps(result))
        EOF
      env:
        SHOULD_EXECUTE: "{{ get(blocks.infer_command.outputs.stdout, 'should_execute', false) }}"
        SKIP_REASON: "{{ get(blocks.infer_command.outputs.stdout, 'reason', 'unknown') }}"
        COMMAND: "{{ get(blocks.infer_command.outputs.stdout, 'command', '') }}"
        COMMAND_TYPE: "{{ get(blocks.infer_command.outputs.stdout, 'command_type', 'unknown') }}"
        WORKING_DIR: "{{ get(blocks.infer_command.outputs.stdout, 'working_dir', '.') }}"
        RAW_OUTPUT: "{{ blocks.run_command.outputs.stdout | default('') }}"

  - id: store_execution
    type: Sql
    description: Store execution results in executions table.
    depends_on: [parse_result]
    condition: "{{ get(blocks.parse_result.outputs.stdout, 'executed', false) }}"
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.execution }}"
      op: insert
      data:
        task_id: "{{ get(blocks.register_phase.outputs, 'rows.0.id', '') }}"
        command: "{{ get(blocks.parse_result.outputs.stdout, 'command', '') }}"
        working_dir: "{{ get(blocks.parse_result.outputs.stdout, 'working_dir', '.') }}"
        exit_code: "{{ get(blocks.parse_result.outputs.stdout, 'exit_code', 1) }}"
        stdout: "{{ get(blocks.parse_result.outputs.stdout, 'stdout', '') }}"
        stderr: ""
        duration_ms: "{{ get(blocks.parse_result.outputs.stdout, 'duration_ms', 0) }}"
        phase: "execute"
        iteration: 0

  - id: track_done
    type: Sql
    description: Mark phase complete.
    depends_on:
      - block: register_phase
        required: true
      - block: parse_result
        required: true
      - block: store_execution
        required: false
    inputs:
      engine: sqlite
      path: "{{ inputs.state }}"
      model: "{{ inputs.models.task }}"
      op: update
      where:
        id: "{{ get(blocks.register_phase.outputs, 'rows.0.id', '') }}"
      data:
        status: done
        outputs: "{{ blocks.parse_result.outputs.stdout }}"

outputs:
  result:
    type: dict
    description: "Execution result with command, exit_code, stdout, etc."
    value: "{{ blocks.parse_result.outputs.stdout }}"

  executed:
    type: bool
    description: "Whether a command was executed."
    value: "{{ get(blocks.parse_result.outputs.stdout, 'executed', false) }}"

  success:
    type: bool
    description: "Whether the command succeeded (exit_code == 0)."
    value: "{{ get(blocks.parse_result.outputs.stdout, 'success', false) }}"

  state:
    type: str
    description: "Path to state database."
    value: "{{ inputs.state }}"
